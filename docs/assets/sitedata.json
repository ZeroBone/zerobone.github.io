

{
  "pages": [
    {
      
      
      
      "content": "\n",
      "url": "/404.html"
    },{
      
      "title": "Blog",
      "description": "Alexander Mayorov’s blog about computer science.\n",
      "content": "\n",
      "url": "/blog/"
    },{
      
      "title": "Other",
      "description": "This page contains some of my summaries, cheat sheets and other materials that you might also find useful.\n",
      "content": "\n  Integral table.\n  Modular arithmetic\n  Boolean algebra.\n  Regular expressions.\n\n",
      "url": "/other/"
    },{
      
      "title": "Welcome",
      
      "content": "Hi, I am Alexander Mayorov (a.k.a. ZeroBone) and this is my personal website. I am currently a computer science student and a backend developer.\n\nApart from studying I do research. My fields of interests:\n\n\n  Theoretical computer science, algorithms, formal languages &amp; computability\n  Logic, formal verification and automated proof procedures\n  Compiler design and programming languages\n  Group theory &amp; linear algebra\n  Graph theory\n\n\nLatest posts\n\n\n\nLatest projects\n\n\n",
      "url": "/"
    },{
      
      
      
      "content": "\n",
      "url": "/offline.html"
    },{
      
      "title": "Posts",
      
      "content": "\n",
      "url": "/posts/"
    },{
      
      "title": "Projects",
      "description": "Most of my open-source projects are listed on this page. Every project has a link to a GitHub repo with all the details and the source code.\n",
      "content": "\n",
      "url": "/projects/"
    },{
      
      "title": "Resume",
      "description": "Alexander Mayorov’s resume as a software engineer.\n",
      "content": "\n",
      "url": "/resume/"
    },{
      
      "title": "Boolean algebra cheat sheet",
      
      "content": "\n  Operator Precedence\n  Axioms\n  Basic laws\n  Implication properties    \n      Simplification rules\n      Rewrite rules\n      Implication in an operator basis\n    \n  \n  XOR and equivalence properties\n  NAND and NOR\n  Linear clause forms\n\nOperator Precedence\n\nFirst operators in this list bind stronger:\n\n\n  ¬\\neg¬ (Negation)\n  ∧\\wedge∧ (And)\n  ⊕\\oplus⊕ (Xor)\n  ∨\\vee∨ (Or)\n  →\\rightarrow→ (Implication)\n  ↔\\leftrightarrow↔ (Equivalence)\n\n\nFrom this point on we will use multiplication (⋅\\cdot⋅) instead of ∧\\wedge∧ and +++ instead of ∨\\vee∨. Also, instead of writing ¬a\\neg a¬a we will write aˉ\\bar{a}aˉ. Sometimes the equality sign (===) is used to denote syntactic equality, but here we will denote sematic equivalence with ===.\n\nAxioms\n\na+b=b+aa⋅b=b⋅a}commutativitya(b+c)=ab+aca+bc=(a+b)(a+c)}distributivitya+0=aa⋅1=a}neutral elementsa+aˉ=1a⋅aˉ=0}complement\\begin{aligned}\n\\left.\\begin{aligned}\na + b &amp;= b + a \\\\\na \\cdot b &amp;= b \\cdot a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{commutativity} \\\\\n\\left.\\begin{aligned}\na(b + c) &amp;= ab + ac \\\\\na + bc &amp;= (a + b)(a + c)\n\\end{aligned}\\right\\rbrace\n&amp;\\text{distributivity} \\\\\n\\left.\\begin{aligned}\na + 0 &amp;= a \\\\\na \\cdot 1 &amp;= a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{neutral elements} \\\\\n\\left.\\begin{aligned}\na + \\bar{a} &amp;= 1 \\\\\na \\cdot \\bar{a} &amp;= 0\n\\end{aligned}\\right\\rbrace\n&amp;\\text{complement}\n\\end{aligned}a+ba⋅b​=b+a=b⋅a​}a(b+c)a+bc​=ab+ac=(a+b)(a+c)​}a+0a⋅1​=a=a​}a+aˉa⋅aˉ​=1=0​}​commutativitydistributivityneutral elementscomplement​\n\nBasic laws\n\na+a=aa⋅a=a}idempotencya+1=1a⋅0=0}killer elementsa+ab=aa(a+b)=a}absorbtion(a+b)+c=a+(b+c)(a⋅b)⋅c=a⋅(b⋅c)}associativitya+b‾=aˉ⋅bˉa⋅b‾=aˉ+bˉ}De Morganaˉˉ=a}involutionab+bc+aˉc=ab+aˉc(a+b)(b+c)(aˉ+c)=(a+b)(aˉ+c)}consensus\\begin{aligned}\n\\left.\\begin{aligned}\na + a &amp;= a \\\\\na \\cdot a &amp;= a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{idempotency} \\\\\n\\left.\\begin{aligned}\na + 1 &amp;= 1 \\\\\na \\cdot 0 &amp;= 0\n\\end{aligned}\\right\\rbrace\n&amp;\\text{killer elements} \\\\\n\\left.\\begin{aligned}\na + ab &amp;= a \\\\\na(a + b) &amp;= a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{absorbtion} \\\\\n\\left.\\begin{aligned}\n(a + b) + c &amp;= a + (b + c) \\\\\n(a \\cdot b) \\cdot c &amp;= a \\cdot (b \\cdot c)\n\\end{aligned}\\right\\rbrace\n&amp;\\text{associativity} \\\\\n\\left.\\begin{aligned}\n\\overline{a + b} &amp;= \\bar{a} \\cdot \\bar{b} \\\\\n\\overline{a \\cdot b} &amp;= \\bar{a} + \\bar{b}\n\\end{aligned}\\right\\rbrace\n&amp;\\text{De Morgan} \\\\\n\\left.\\begin{aligned}\n\\bar{\\bar{a}} &amp;= a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{involution} \\\\\n\\left.\\begin{aligned}\nab + bc + \\bar{a}c &amp;= ab + \\bar{a}c \\\\\n(a + b)(b + c)(\\bar{a} + c) &amp;= (a + b)(\\bar{a} + c)\n\\end{aligned}\\right\\rbrace\n&amp;\\text{consensus}\n\\end{aligned}a+aa⋅a​=a=a​}a+1a⋅0​=1=0​}a+aba(a+b)​=a=a​}(a+b)+c(a⋅b)⋅c​=a+(b+c)=a⋅(b⋅c)​}a+b​a⋅b​=aˉ⋅bˉ=aˉ+bˉ​}aˉˉ​=a​}ab+bc+aˉc(a+b)(b+c)(aˉ+c)​=ab+aˉc=(a+b)(aˉ+c)​}​idempotencykiller elementsabsorbtionassociativityDe Morganinvolutionconsensus​\n\nImplication properties\n\nSimplification rules\n\n0→a=11→a=aa→1=1a→0=aˉa→aˉ=aˉa→a=1a⋅b→a=1a→a⋅b=a→ba→(a→b)=a→ba→(b→a)=1(a→b)→a=a(a→b)(b→c)→(a→c)=1\\begin{aligned}\n0 \\rightarrow a &amp;= 1 \\\\\n1 \\rightarrow a &amp;= a \\\\\na \\rightarrow 1 &amp;= 1 \\\\\na \\rightarrow 0 &amp;= \\bar{a} \\\\\na \\rightarrow \\bar{a} &amp;= \\bar{a} \\\\\na \\rightarrow a &amp;= 1 \\\\\na \\cdot b \\rightarrow a &amp;= 1 \\\\\na \\rightarrow a \\cdot b &amp;= a \\rightarrow b \\\\\na \\rightarrow (a \\rightarrow b) &amp;= a \\rightarrow b \\\\\na \\rightarrow (b \\rightarrow a) &amp;= 1 \\\\\n(a \\rightarrow b) \\rightarrow a &amp;= a \\\\\n(a \\rightarrow b)(b \\rightarrow c) \\rightarrow (a \\rightarrow c) &amp;= 1\n\\end{aligned}0→a1→aa→1a→0a→aˉa→aa⋅b→aa→a⋅ba→(a→b)a→(b→a)(a→b)→a(a→b)(b→c)→(a→c)​=1=a=1=aˉ=aˉ=1=1=a→b=a→b=1=a=1​\n\nRewrite rules\n\na→b=aˉ+ba→b‾=abˉa→b=bˉ→aˉa→b⋅c=(a→b)(a→c)a→b+c=(a→b)+(a→c)(a+b)→c=(a→c)(b→c)a→(b→c)=a⋅b→c(a→b)→c=(aˉ→c)(b→c)\\begin{aligned}\na \\rightarrow b &amp;= \\bar{a} + b \\\\\n\\overline{a \\rightarrow b} &amp;= a\\bar{b} \\\\\na \\rightarrow b &amp;= \\bar{b} \\rightarrow \\bar{a} \\\\\na \\rightarrow b \\cdot c &amp;= (a \\rightarrow b)(a \\rightarrow c) \\\\\na \\rightarrow b + c &amp;= (a \\rightarrow b) + (a \\rightarrow c) \\\\\n(a + b) \\rightarrow c &amp;= (a \\rightarrow c)(b \\rightarrow c) \\\\\na \\rightarrow (b \\rightarrow c) &amp;= a \\cdot b \\rightarrow c \\\\\n(a \\rightarrow b) \\rightarrow c &amp;= (\\bar{a} \\rightarrow c)(b \\rightarrow c)\n\\end{aligned}a→ba→ba→ba→b⋅ca→b+c(a+b)→ca→(b→c)(a→b)→c​=aˉ+b=abˉ=bˉ→aˉ=(a→b)(a→c)=(a→b)+(a→c)=(a→c)(b→c)=a⋅b→c=(aˉ→c)(b→c)​\n\nImplication in an operator basis\n\naˉ=a→0a⋅b=a→bˉ‾a⋅b=(a→(b→0))→0a+b=(a→b)→b\\begin{aligned}\n\\bar{a} &amp;= a \\rightarrow 0 \\\\\na \\cdot b &amp;= \\overline{a \\rightarrow \\bar{b}} \\\\\na \\cdot b &amp;= (a \\rightarrow (b \\rightarrow 0)) \\rightarrow 0 \\\\\na + b &amp;= (a \\rightarrow b) \\rightarrow b\n\\end{aligned}aˉa⋅ba⋅ba+b​=a→0=a→bˉ=(a→(b→0))→0=(a→b)→b​\n\nXOR and equivalence properties\n\na⊕0=aa⊕1=aˉa⊕a=0a⊕b=abˉ+aˉba⊕b=aˉ⊕bˉa⊕b=aˉ↔b=a↔bˉa⊕b=a↔b‾a⋅(b⊕c)=ab⊕aca+b=ab⊕a⊕ba→b=ab⊕a⊕1a↔b=a⊕b⊕1a↔a=1a↔aˉ=0a↔b=(a→b)(b→a)a↔b=a⋅b+aˉ⋅bˉa↔b=(aˉ+b)(a+bˉ)a↔b=aˉ↔bˉa↔b=aˉ⊕b=a⊕bˉ\\begin{aligned}\na \\oplus 0 &amp;= a \\\\\na \\oplus 1 &amp;= \\bar{a} \\\\\na \\oplus a &amp;= 0 \\\\\na \\oplus b &amp;= a\\bar{b} + \\bar{a}b \\\\\na \\oplus b &amp;= \\bar{a} \\oplus \\bar{b} \\\\\na \\oplus b &amp;= \\bar{a} \\leftrightarrow b = a \\leftrightarrow \\bar{b} \\\\\na \\oplus b &amp;= \\overline{a \\leftrightarrow b} \\\\\na \\cdot (b \\oplus c) &amp;= ab \\oplus ac \\\\\na + b &amp;= ab \\oplus a \\oplus b \\\\\na \\rightarrow b &amp;= ab \\oplus a \\oplus 1 \\\\\na \\leftrightarrow b &amp;= a \\oplus b \\oplus 1 \\\\\na \\leftrightarrow a &amp;= 1 \\\\\na \\leftrightarrow \\bar{a} &amp;= 0 \\\\\na \\leftrightarrow b &amp;= (a \\rightarrow b)(b \\rightarrow a) \\\\\na \\leftrightarrow b &amp;= a \\cdot b + \\bar{a} \\cdot \\bar{b} \\\\\na \\leftrightarrow b &amp;= (\\bar{a} + b)(a + \\bar{b}) \\\\\na \\leftrightarrow b &amp;= \\bar{a} \\leftrightarrow \\bar{b} \\\\\na \\leftrightarrow b &amp;= \\bar{a} \\oplus b = a \\oplus \\bar{b}\n\\end{aligned}a⊕0a⊕1a⊕aa⊕ba⊕ba⊕ba⊕ba⋅(b⊕c)a+ba→ba↔ba↔aa↔aˉa↔ba↔ba↔ba↔ba↔b​=a=aˉ=0=abˉ+aˉb=aˉ⊕bˉ=aˉ↔b=a↔bˉ=a↔b=ab⊕ac=ab⊕a⊕b=ab⊕a⊕1=a⊕b⊕1=1=0=(a→b)(b→a)=a⋅b+aˉ⋅bˉ=(aˉ+b)(a+bˉ)=aˉ↔bˉ=aˉ⊕b=a⊕bˉ​\n\nNAND and NOR\n\naˉ=a⊼aaˉ=a⊻aa⊼b‾=aˉ⊻bˉa⊻b‾=aˉ⊼bˉa⋅b=(a⊼b)⊼(a⊼b)a⋅b=(a⊻a)⊻(b⊻b)a+b=(a⊼a)⊼(b⊼b)a+b=(a⊻b)⊻(a⊻b)a⊕b=(a⊼(a⊼b))⊼((a⊼b)⊼b)a⊕b=(a⊻b)⊻((a⊻a)⊻(b⊻b))a↔b=(a⊼b)⊼((a⊼a)⊼(b⊼b))a↔b=(a⊻(a⊻b))⊻((a⊻b)⊻b)\\begin{aligned}\n\\bar{a} &amp;= a \\barwedge a \\\\\n\\bar{a} &amp;= a \\veebar a \\\\\n\\overline{a \\barwedge b} &amp;= \\bar{a} \\veebar \\bar{b} \\\\\n\\overline{a \\veebar b} &amp;= \\bar{a} \\barwedge \\bar{b} \\\\\na \\cdot b &amp;= (a \\barwedge b) \\barwedge (a \\barwedge b) \\\\\na \\cdot b &amp;= (a \\veebar a) \\veebar (b \\veebar b) \\\\\na + b &amp;= (a \\barwedge a) \\barwedge (b \\barwedge b) \\\\\na + b &amp;= (a \\veebar b) \\veebar (a \\veebar b) \\\\\na \\oplus b &amp;= (a \\barwedge (a \\barwedge b))\\barwedge ((a \\barwedge b) \\barwedge b) \\\\\na \\oplus b &amp;= (a \\veebar b) \\veebar ((a \\veebar a) \\veebar (b \\veebar b)) \\\\\na \\leftrightarrow b &amp;= (a \\barwedge b) \\barwedge ((a \\barwedge a) \\barwedge (b \\barwedge b)) \\\\\na \\leftrightarrow b &amp;= (a \\veebar (a \\veebar b)) \\veebar ((a \\veebar b) \\veebar b)\n\\end{aligned}aˉaˉa⊼b​a⊻b​a⋅ba⋅ba+ba+ba⊕ba⊕ba↔ba↔b​=a⊼a=a⊻a=aˉ⊻bˉ=aˉ⊼bˉ=(a⊼b)⊼(a⊼b)=(a⊻a)⊻(b⊻b)=(a⊼a)⊼(b⊼b)=(a⊻b)⊻(a⊻b)=(a⊼(a⊼b))⊼((a⊼b)⊼b)=(a⊻b)⊻((a⊻a)⊻(b⊻b))=(a⊼b)⊼((a⊼a)⊼(b⊼b))=(a⊻(a⊻b))⊻((a⊻b)⊻b)​\n\nLinear clause forms\n\nThese minimum conjunctive normal forms are often used to construct a linear clause form of some formula. The CNF is then typically passed to a SAT-solver.\n\nx↔aˉ=(xˉ+aˉ)(x+a)x↔a⋅b=(xˉ+a)(xˉ+b)(x+aˉ+bˉ)x↔a+b=(x+aˉ)(x+bˉ)(xˉ+a+b)x↔a⊕b=(x+a+bˉ)(x+aˉ+b)(xˉ+a+b)(xˉ+aˉ+bˉ)x↔a→b=(x+a)(x+bˉ)(xˉ+aˉ+b)x↔a↔b=(xˉ+aˉ+b)(xˉ+a+bˉ)(x+aˉ+bˉ)(x+a+b)\\begin{aligned}\nx \\leftrightarrow \\bar{a} &amp;= (\\bar{x} + \\bar{a})(x + a) \\\\\nx \\leftrightarrow a \\cdot b &amp;= (\\bar{x} + a)(\\bar{x} + b)(x + \\bar{a} + \\bar{b}) \\\\\nx \\leftrightarrow a + b &amp;= (x + \\bar{a})(x + \\bar{b})(\\bar{x} + a + b) \\\\\nx \\leftrightarrow a \\oplus b &amp;= (x + a + \\bar{b})(x + \\bar{a} + b)(\\bar{x} + a + b)(\\bar{x} + \\bar{a} + \\bar{b}) \\\\\nx \\leftrightarrow a \\rightarrow b &amp;= (x + a)(x + \\bar{b})(\\bar{x} + \\bar{a} + b) \\\\\nx \\leftrightarrow a \\leftrightarrow b &amp;= (\\bar{x} + \\bar{a} + b)(\\bar{x} + a + \\bar{b})(x + \\bar{a} + \\bar{b})(x + a + b)\n\\end{aligned}x↔aˉx↔a⋅bx↔a+bx↔a⊕bx↔a→bx↔a↔b​=(xˉ+aˉ)(x+a)=(xˉ+a)(xˉ+b)(x+aˉ+bˉ)=(x+aˉ)(x+bˉ)(xˉ+a+b)=(x+a+bˉ)(x+aˉ+b)(xˉ+a+b)(xˉ+aˉ+bˉ)=(x+a)(x+bˉ)(xˉ+aˉ+b)=(xˉ+aˉ+b)(xˉ+a+bˉ)(x+aˉ+bˉ)(x+a+b)​\n",
      "url": "/other/boolean-algebra/"
    },{
      
      "title": "Integral table",
      
      "content": "\n  Inverse derivatives\n  Standart integrals\n\n\nInverse derivatives\n\n∫dx=x+C\\int \\mathrm{d}x = x + C∫dx=x+C\n\n∫0dx=C\\int 0\\mathrm{d}x = C∫0dx=C\n\n∫xmdx=xm+1m+1+C,m≠−1\\int x^m\\mathrm{d}x = \\frac{x^{m+1}}{m+1} + C, m \\neq -1∫xmdx=m+1xm+1​+C,m​=−1\n\n∫dxx=ln⁡∣x∣+C\\int \\frac{\\mathrm{d}x}{x} = \\ln|x| + C∫xdx​=ln∣x∣+C\n\n∫cos⁡(x)dx=sin⁡(x)+C\\int \\cos(x) \\mathrm{d}x = \\sin(x) + C∫cos(x)dx=sin(x)+C\n\n∫sin⁡(x)dx=−cos⁡(x)+C\\int \\sin(x) \\mathrm{d}x = -\\cos(x) + C∫sin(x)dx=−cos(x)+C\n\n∫dx1+x2=arctg⁡(x)+C=−arcctg⁡(x)+C\\int \\frac{\\mathrm{d}x}{1+x^2} = \\arctg(x) + C = -\\arcctg(x) + C∫1+x2dx​=arctg(x)+C=−arcctg(x)+C\n\n∫dx1−x2=arcsin⁡(x)+C=−arccos⁡(x)+C\\int \\frac{\\mathrm{d}x}{\\sqrt{1-x^2}} = \\arcsin(x) + C = -\\arccos(x) + C∫1−x2​dx​=arcsin(x)+C=−arccos(x)+C\n\n∫axdx=axln⁡a+C\\int a^x \\mathrm{d}x = \\frac{a^x}{\\ln a} + C∫axdx=lnaax​+C\n\n∫exdx=ex+C\\int e^x \\mathrm{d}x = e^x + C∫exdx=ex+C\n\n∫sec⁡(x)2dx=∫dxcos⁡(x)2=tg⁡(x)+C\\int \\sec(x)^2 \\mathrm{d}x = \\int \\frac{dx}{\\cos(x)^2} = \\tg(x) + C∫sec(x)2dx=∫cos(x)2dx​=tg(x)+C\n\n∫cosec⁡(x)2dx=∫dxsin⁡(x)2=−ctg⁡(x)+C\\int \\cosec(x)^2 \\mathrm{d}x = \\int \\frac{dx}{\\sin(x)^2} = -\\ctg(x) + C∫cosec(x)2dx=∫sin(x)2dx​=−ctg(x)+C\n\n∫sh⁡(x)dx=ch⁡(x)+C\\int \\sh(x)\\mathrm{d}x = \\ch(x) + C∫sh(x)dx=ch(x)+C\n\n∫ch⁡(x)dx=sh⁡(x)+C\\int \\ch(x)\\mathrm{d}x = \\sh(x) + C∫ch(x)dx=sh(x)+C\n\n∫dxch⁡(x)2=th⁡(x)+C\\int \\frac{\\mathrm{d}x}{\\ch(x)^2} = \\th(x) + C∫ch(x)2dx​=th(x)+C\n\n∫dxsh⁡(x)2=−cth⁡(x)+C\\int \\frac{\\mathrm{d}x}{\\sh(x)^2} = -\\cth(x) + C∫sh(x)2dx​=−cth(x)+C\n\nStandart integrals\n\n∫dxx2+a2=1aarctg⁡(xa)+C\\int \\frac{\\mathrm{d}x}{x^2 + a^2} = \\frac{1}{a} \\arctg\\Big(\\frac{x}{a}\\Big) + C∫x2+a2dx​=a1​arctg(ax​)+C\n\n∫dxx2−a2=12aln⁡∣x−ax+a∣+C\\int \\frac{\\mathrm{d}x}{x^2 - a^2} = \\frac{1}{2a}\\ln\\Big|\\frac{x-a}{x+a}\\Big| + C∫x2−a2dx​=2a1​ln∣∣∣∣​x+ax−a​∣∣∣∣​+C\n\n∫dxx2±a2=ln⁡∣x+x2±a2∣+C\\int \\frac{\\mathrm{d}x}{\\sqrt{x^2 \\pm a^2}} = \\ln|x + \\sqrt{x^2 \\pm a^2}| + C∫x2±a2​dx​=ln∣x+x2±a2​∣+C\n\n∫dxa2−x2=arcsin⁡(xa)+C\\int \\frac{\\mathrm{d}x}{\\sqrt{a^2 - x^2}} = \\arcsin \\Big(\\frac{x}{a}\\Big) + C∫a2−x2​dx​=arcsin(ax​)+C\n",
      "url": "/other/int-table/"
    },{
      
      "title": "Modular arithmetic",
      
      "content": "\n  Definition\n  Basic properties    \n      Addition and multiplication        \n          Proof\n        \n      \n      Powers        \n          Proof\n        \n      \n      Combination with different moduli\n    \n  \n  General simultaneous congruences    \n      Proof\n    \n  \n\nDefinition\n\na≡bmod  m:⇔∃k∈Z:a−b=k⋅ma \\equiv b \\mod m :\\Leftrightarrow \\exists k \\in \\mathbb{Z} : a - b = k \\cdot ma≡bmodm:⇔∃k∈Z:a−b=k⋅m\n\nBasic properties\n\nAddition and multiplication\n\na≡bmod  mc≡dmod  m}⇒{a+c≡b+dmod  ma−c≡b−dmod  ma⋅c≡b⋅dmod  m\\left.\\begin{aligned}\na &amp;\\equiv b \\mod m \\\\\nc &amp;\\equiv d \\mod m\n\\end{aligned}\\right\\rbrace \\Rightarrow \n\\left\\lbrace\\begin{aligned}\na + c &amp;\\equiv b + d \\mod m \\\\\na - c &amp;\\equiv b - d \\mod m \\\\\na \\cdot c &amp;\\equiv b \\cdot d \\mod m\n\\end{aligned}\\right.ac​≡bmodm≡dmodm​}⇒⎩⎪⎪⎨⎪⎪⎧​a+ca−ca⋅c​≡b+dmodm≡b−dmodm≡b⋅dmodm​\n\nProof\n\nBy definition,\n\na−b=k1⋅mc−d=k2⋅ma - b = k_1 \\cdot m \\\\\nc - d = k_2 \\cdot ma−b=k1​⋅mc−d=k2​⋅m\n\nTherefore:\n\n(1): a−b+c−d=k1m+k2ma - b + c - d = k_1 m + k_2 ma−b+c−d=k1​m+k2​m reordered gives (a+c)−(b+d)=(k1+k2)m(a + c) - (b + d) = (k_1 + k_2)m(a+c)−(b+d)=(k1​+k2​)m.\n\n(2): Follows trivially from (1)\n\n(3): ac−bd=ac−bc+bc−bd=c(a−b)+b(c−d)=ck1m+bk2m=(ck1+bk2)mac - bd = ac - bc + bc - bd = c(a - b) + b(c - d) = c k_1 m + b k_2 m = (c k_1 + b k_2)mac−bd=ac−bc+bc−bd=c(a−b)+b(c−d)=ck1​m+bk2​m=(ck1​+bk2​)m\n\nPowers\n\na≡bmod  m⇒∀k∈N:ak≡bkmod  ma \\equiv b \\mod m \\Rightarrow \\forall k \\in \\mathbb{N} : a^k \\equiv b ^k \\mod ma≡bmodm⇒∀k∈N:ak≡bkmodm\n\nProof\n\na≡bmod  ma≡bmod  m\\begin{aligned}\na \\equiv b \\mod m \\\\\na \\equiv b \\mod m\n\\end{aligned}a≡bmodma≡bmodm​\n\nimplies a2≡b2mod  ma^2 \\equiv b^2 \\mod ma2≡b2modm. For greater exponents the statement is true by induction.\n\nCombination with different moduli\n\nx≡amod  n1n2⇒{x≡amod  n1x≡amod  n2x \\equiv a \\mod n_1 n_2 \\Rightarrow \n\\left\\lbrace\\begin{aligned}\nx &amp;\\equiv a \\mod n_1 \\\\\nx &amp;\\equiv a \\mod n_2\n\\end{aligned}\\right.x≡amodn1​n2​⇒{xx​≡amodn1​≡amodn2​​\n\nIf gcd⁡(n1,n2)=1\\gcd(n_1, n_2) = 1gcd(n1​,n2​)=1, then the converse also holds:\n\nx≡amod  n1x≡amod  n2}⇒x≡amod  n1n2\\left.\\begin{aligned}\nx &amp;\\equiv a \\mod n_1 \\\\\nx &amp;\\equiv a \\mod n_2\n\\end{aligned}\\right\\rbrace \\Rightarrow\nx \\equiv a \\mod n_1 n_2xx​≡amodn1​≡amodn2​​}⇒x≡amodn1​n2​\n\nGeneral simultaneous congruences\n\nIt holds, that\n\nx≡a1mod  n1x≡a2mod  n2L≠∅}⇔{a1≡a2mod  gcd⁡(n1,n2)x1≡x2mod  lcm(n1,n2)∀x1,x2∈L\\left.\\begin{aligned}\nx &amp;\\equiv a_1 \\mod n_1 \\\\\nx &amp;\\equiv a_2 \\mod n_2 \\\\\nL &amp;\\neq \\varnothing\n\\end{aligned}\\right\\rbrace\n\\Leftrightarrow\n\\left\\lbrace\\begin{aligned}\na_1 &amp;\\equiv a_2 \\mod \\gcd(n_1, n_2) \\\\\nx_1 &amp;\\equiv x_2 \\mod \\lcm(n_1, n_2) \\quad\\forall x_1,x_2 \\in L\n\\end{aligned}\\right.xxL​≡a1​modn1​≡a2​modn2​​=∅​⎭⎪⎪⎬⎪⎪⎫​⇔{a1​x1​​≡a2​modgcd(n1​,n2​)≡x2​modlcm(n1​,n2​)∀x1​,x2​∈L​\n\nwhere LLL is the solution set.\n\nProof\n\n(⇒\\Rightarrow⇒): First, we prove that a1a_1a1​ must be equivalent to a2a_2a2​ modulo gcd⁡(n1,n2)\\gcd(n_1, n_2)gcd(n1​,n2​):\n\n{x≡a1mod  n1x≡a2mod  n2⇒{x−a1≡0mod  n1x−a2≡0mod  n2⇒{x−a1≡0mod  gcd⁡(n1,n2)x−a2≡0mod  gcd⁡(n1,n2)⇒x−a1−x+a2≡0mod  gcd⁡(n1,n2)⇒a2−a1≡0mod  gcd⁡(n1,n2)⇒a1≡a2mod  gcd⁡(n1,n2)\\begin{aligned}\n\\left\\lbrace\\begin{aligned}\nx &amp;\\equiv a_1 \\mod n_1 \\\\\nx &amp;\\equiv a_2 \\mod n_2\n\\end{aligned}\\right.\n&amp;\\Rightarrow\n\\left\\lbrace\\begin{aligned}\nx - a_1 &amp;\\equiv 0 \\mod n_1 \\\\\nx - a_2 &amp;\\equiv 0 \\mod n_2\n\\end{aligned}\\right. \\\\\n&amp;\\Rightarrow\n\\left\\lbrace\\begin{aligned}\nx - a_1 &amp;\\equiv 0 \\mod \\gcd(n_1, n_2) \\\\\nx - a_2 &amp;\\equiv 0 \\mod \\gcd(n_1, n_2)\n\\end{aligned}\\right. \\\\\n&amp;\\Rightarrow\nx - a_1 - x + a_2 \\equiv 0 \\mod \\gcd(n_1, n_2) \\\\\n&amp;\\Rightarrow\na_2 - a_1 \\equiv 0 \\mod \\gcd(n_1, n_2) \\\\\n&amp;\\Rightarrow\na_1 \\equiv a_2 \\mod \\gcd(n_1, n_2)\n\\end{aligned}{xx​≡a1​modn1​≡a2​modn2​​​⇒{x−a1​x−a2​​≡0modn1​≡0modn2​​⇒{x−a1​x−a2​​≡0modgcd(n1​,n2​)≡0modgcd(n1​,n2​)​⇒x−a1​−x+a2​≡0modgcd(n1​,n2​)⇒a2​−a1​≡0modgcd(n1​,n2​)⇒a1​≡a2​modgcd(n1​,n2​)​\n\nThe set of solutions LLL is defined modulo lcm(n1,n2)\\lcm(n_1, n_2)lcm(n1​,n2​), because given x1,x2∈L≠∅x_1, x_2 \\in L \\neq \\varnothingx1​,x2​∈L​=∅ it follows, that\n\nx1=a1+k1⋅n1x1=a2+k2⋅n2x2=a1+k3⋅n1x2=a2+k4⋅n2\\begin{aligned}\nx_1 &amp;= a_1 + k_1 \\cdot n_1 \\\\\nx_1 &amp;= a_2 + k_2 \\cdot n_2 \\\\\nx_2 &amp;= a_1 + k_3 \\cdot n_1 \\\\\nx_2 &amp;= a_2 + k_4 \\cdot n_2\n\\end{aligned}x1​x1​x2​x2​​=a1​+k1​⋅n1​=a2​+k2​⋅n2​=a1​+k3​⋅n1​=a2​+k4​⋅n2​​\n\nwhere k1,k2,k3,k4∈Zk_1, k_2, k_3, k_4 \\in \\mathbb{Z}k1​,k2​,k3​,k4​∈Z. By subtracting the third row from the first one and the fourth row from the second one we get:\n\nx1−x2=a1+k1⋅n1−a1−k3⋅n1=(k1−k3)⋅n1x1−x2=a2+k2⋅n2−a2−k4⋅n2=(k2−k4)⋅n2\\begin{aligned}\nx_1 - x_2 &amp;= a_1 + k_1\\cdot n_1 - a_1 - k_3 \\cdot n_1 = (k_1 - k_3) \\cdot n_1 \\\\\nx_1 - x_2 &amp;= a_2 + k_2 \\cdot n_2 - a_2 - k_4 \\cdot n_2 = (k_2 - k_4) \\cdot n_2\n\\end{aligned}x1​−x2​x1​−x2​​=a1​+k1​⋅n1​−a1​−k3​⋅n1​=(k1​−k3​)⋅n1​=a2​+k2​⋅n2​−a2​−k4​⋅n2​=(k2​−k4​)⋅n2​​\n\nSo x1−x2x_1 - x_2x1​−x2​ must be a multiple of both n1n_1n1​ and n2n_2n2​ and therefore:\n\nx1−x2≡0mod  lcm(n1,n2)x_1 - x_2 \\equiv 0 \\mod \\lcm(n_1, n_2)x1​−x2​≡0modlcm(n1​,n2​)\n\n(⇐\\Leftarrow⇐): By assumption, we know that for some k∈Zk \\in \\mathbb{Z}k∈Z\n\na1−a2=k⋅gcd⁡(n1,n2)a_1 - a_2 = k \\cdot \\gcd(n_1, n_2)a1​−a2​=k⋅gcd(n1​,n2​)\n\nAlso, as stated in the Bézout Lemma, we can write the greatest common divisor as a linear combination:\n\ngcd⁡(n1,n2):=u⋅n1+v⋅n2\\gcd(n_1, n_2) := u \\cdot n_1 + v \\cdot n_2gcd(n1​,n2​):=u⋅n1​+v⋅n2​\n\nBy replacing gcd⁡(n1,n2)\\gcd(n_1, n_2)gcd(n1​,n2​) with the linear combination we get:\n\na1−a2=k⋅u⋅n1+k⋅v⋅n2a_1 - a_2 =\nk \\cdot u \\cdot n_1 + k \\cdot v \\cdot n_2a1​−a2​=k⋅u⋅n1​+k⋅v⋅n2​\n\nThus,\n\na1−k⋅u⋅n1⏟≡a1 mod n1=a2+k⋅v⋅n2⏟≡a2 mod n2\\underbrace{a_1 - k \\cdot u \\cdot n_1}_{\\equiv a_1 \\bmod n_1} =\n\\underbrace{a_2 + k \\cdot v \\cdot n_2}_{\\equiv a_2 \\bmod n_2}≡a1​modn1​a1​−k⋅u⋅n1​​​=≡a2​modn2​a2​+k⋅v⋅n2​​​\n\nAssuming the solution is defined modulo lcm(n1,n2)\\lcm(n_1, n_2)lcm(n1​,n2​), it follows that:\n\nx≡a1−k⋅u⋅n1≡a2+k⋅v⋅n2mod  lcm(n1,n2)x \\equiv a_1 - k \\cdot u \\cdot n_1 \\equiv a_2 + k \\cdot v \\cdot n_2 \\mod \\lcm(n_1, n_2)x≡a1​−k⋅u⋅n1​≡a2​+k⋅v⋅n2​modlcm(n1​,n2​)\n",
      "url": "/other/modulo/"
    },{
      
      "title": "Regular expression cheat sheet",
      
      "content": "\n  Denotational semantics\n  The nullable function\n  Brzozowski derivatives\n  Basic laws\n  Other identities    \n      Simplifying Kleene star\n      General elimination rules\n    \n  \n  Arden’s Theorem\n\nDenotational semantics\n\n[ ⁣[∅] ⁣]:=∅[ ⁣[ε] ⁣]:={ε}[ ⁣[a] ⁣]:={a}[ ⁣[a⋅b] ⁣]:=[ ⁣[a] ⁣]⋅[ ⁣[b] ⁣][ ⁣[a+b] ⁣]:=[ ⁣[a] ⁣]∪[ ⁣[b] ⁣][ ⁣[a∗] ⁣]:=[ ⁣[a] ⁣]∗\\begin{aligned}\n[\\![\\varnothing]\\!] &amp;:= \\varnothing \\\\\n[\\![\\varepsilon]\\!] &amp;:= \\{\\varepsilon\\} \\\\\n[\\![a]\\!] &amp;:= \\{a\\} \\\\\n[\\![a \\cdot b]\\!] &amp;:= [\\![a]\\!] \\cdot [\\![b]\\!] \\\\\n[\\![a + b]\\!] &amp;:= [\\![a]\\!] \\cup [\\![b]\\!] \\\\\n[\\![a^*]\\!] &amp;:= [\\![a]\\!]^*\n\\end{aligned}[[∅]][[ε]][[a]][[a⋅b]][[a+b]][[a∗]]​:=∅:={ε}:={a}:=[[a]]⋅[[b]]:=[[a]]∪[[b]]:=[[a]]∗​\n\nwhere the definitions of concatenation of languages and the Kleene stars are as follows:\n\nL1⋅L2:={w1⋅w2:w1∈L1,w2∈L2}L0:={ε}Ln:=L⋅Ln−1L∗:=⋃n∈N0Ln\\begin{aligned}\nL_1 \\cdot L_2 &amp;:= \\{w_1 \\cdot w_2 : w_1 \\in L_1, w_2 \\in L_2\\} \\\\\nL^0 &amp;:= \\{\\varepsilon\\} \\\\\nL^n &amp;:= L \\cdot L^{n-1} \\\\\nL^* &amp;:= \\bigcup_{n \\in \\mathbb{N}_0}{L^n}\n\\end{aligned}L1​⋅L2​L0LnL∗​:={w1​⋅w2​:w1​∈L1​,w2​∈L2​}:={ε}:=L⋅Ln−1:=n∈N0​⋃​Ln​\n\nThe nullable function\n\nn(a)=⊥n(∅)=⊤n(r1⋅r2)=n(r1)∧n(r2)n(r1+r2)=n(r1)∨n(r2)n(r∗)=⊤\\begin{aligned}\nn(a) &amp;= \\bot \\\\\nn(\\varnothing) &amp;= \\top \\\\\nn(r_1 \\cdot r_2) &amp;= n(r_1) \\wedge n(r_2) \\\\\nn(r_1 + r_2) &amp;= n(r_1) \\vee n(r_2) \\\\\nn(r^*) &amp;= \\top\n\\end{aligned}n(a)n(∅)n(r1​⋅r2​)n(r1​+r2​)n(r∗)​=⊥=⊤=n(r1​)∧n(r2​)=n(r1​)∨n(r2​)=⊤​\n\nBrzozowski derivatives\n\nx−1⋅L:={a:x⋅a∈L}x−1⋅a={εa=x∅a≠xx−1⋅ε=∅x−1⋅∅=∅x−1⋅(a⋅b)={(x−1⋅a)⋅b+x−1⋅bn(a)(x−1⋅a)⋅botherwisex−1⋅(a+b)=(x−1⋅a)+(x−1⋅b)x−1⋅a∗=(x−1⋅a)⋅a∗\\begin{aligned}\nx^{-1} \\cdot L &amp;:= \\{a : x \\cdot a \\in L\\} \\\\\nx^{-1}\\cdot a &amp;= \\begin{cases}\n\\varepsilon &amp; a = x \\\\\n\\varnothing &amp; a \\neq x\n\\end{cases} \\\\\nx^{-1}\\cdot \\varepsilon &amp;= \\varnothing \\\\\nx^{-1}\\cdot \\varnothing &amp;= \\varnothing \\\\\nx^{-1}\\cdot (a \\cdot b) &amp;= \\begin{cases}\n(x^{-1} \\cdot a) \\cdot b + x^{-1} \\cdot b &amp; n(a) \\\\\n(x^{-1} \\cdot a) \\cdot b &amp; \\text{otherwise}\n\\end{cases} \\\\\nx^{-1}\\cdot (a + b) &amp;= (x^{-1} \\cdot a) + (x^{-1} \\cdot b) \\\\\nx^{-1} \\cdot a^* &amp;= (x^{-1} \\cdot a) \\cdot a^*\n\\end{aligned}x−1⋅Lx−1⋅ax−1⋅εx−1⋅∅x−1⋅(a⋅b)x−1⋅(a+b)x−1⋅a∗​:={a:x⋅a∈L}={ε∅​a=xa​=x​=∅=∅={(x−1⋅a)⋅b+x−1⋅b(x−1⋅a)⋅b​n(a)otherwise​=(x−1⋅a)+(x−1⋅b)=(x−1⋅a)⋅a∗​\n\nBasic laws\n\na+b=b+aa+a=a∅+a=a∅⋅a=a⋅∅=∅ε⋅a=a⋅ε=aa(b+c)=ab+ac(a+b)c=ac+bca∗a∗=a∗\\begin{aligned}\na + b &amp;= b + a \\\\\na + a &amp;= a \\\\\n\\varnothing + a &amp;= a \\\\\n\\varnothing \\cdot a = a \\cdot \\varnothing &amp;= \\varnothing \\\\\n\\varepsilon \\cdot a = a \\cdot \\varepsilon &amp;= a \\\\\na(b + c) &amp;= ab + ac \\\\\n(a + b)c &amp;= ac + bc \\\\\na^* a^* &amp;= a^*\n\\end{aligned}a+ba+a∅+a∅⋅a=a⋅∅ε⋅a=a⋅εa(b+c)(a+b)ca∗a∗​=b+a=a=a=∅=a=ab+ac=ac+bc=a∗​\n\nOther identities\n\nε∗=ε∅∗=∅aa∗=a∗a(a∗)∗=a∗ε+aa∗=a∗(ab)∗a=a(ba)∗\\begin{aligned}\n\\varepsilon^* &amp;= \\varepsilon \\\\\n\\varnothing^* &amp;= \\varnothing \\\\\na a^* &amp;= a^* a \\\\\n(a^*)^* &amp;= a^* \\\\\n\\varepsilon + aa^* &amp;= a^* \\\\\n(ab)^* a &amp;= a(ba)^*\n\\end{aligned}ε∗∅∗aa∗(a∗)∗ε+aa∗(ab)∗a​=ε=∅=a∗a=a∗=a∗=a(ba)∗​\n\nSimplifying Kleene star\n\n(a+b)∗=(a∗b∗)∗=(a∗+b∗)∗=a∗(ba∗)∗(a + b)^* = (a^* b^*)^* = (a^* + b^*)^* = a^*(ba^*)^*(a+b)∗=(a∗b∗)∗=(a∗+b∗)∗=a∗(ba∗)∗\n\nGeneral elimination rules\n\na+b={aL(b)⊆L(a)bL(a)⊆L(b)a∗b∗={a∗L(b)⊆L(a)b∗L(a)⊆L(b)\\begin{aligned}\na + b &amp;= \\begin{cases}\na &amp; L(b) \\subseteq L(a) \\\\\nb &amp; L(a) \\subseteq L(b)\n\\end{cases} \\\\\na^* b^* &amp;= \\begin{cases}\na^* &amp; L(b) \\subseteq L(a) \\\\\nb^* &amp; L(a) \\subseteq L(b)\n\\end{cases}\n\\end{aligned}a+ba∗b∗​={ab​L(b)⊆L(a)L(a)⊆L(b)​={a∗b∗​L(b)⊆L(a)L(a)⊆L(b)​​\n\nTypical examples of the application of this rule:\n\nab+(a+b)∗=(a+b)∗ε+(a+b)∗=(a+b)∗a∗b∗aba+(a+b+c)∗=(a+b+c)∗(a+b)∗a∗=(a+b)∗\\begin{aligned}\nab + (a + b)^* &amp;= (a + b)^* \\\\\n\\varepsilon + (a + b)^* &amp;= (a + b)^* \\\\\na^* b^* aba + (a + b + c)^* &amp;= (a + b + c)^* \\\\\n(a+b)^* a^* &amp;= (a+b)^*\n\\end{aligned}ab+(a+b)∗ε+(a+b)∗a∗b∗aba+(a+b+c)∗(a+b)∗a∗​=(a+b)∗=(a+b)∗=(a+b+c)∗=(a+b)∗​\n\nArden’s Theorem\n\nr=q+rp⇒r=qp∗r = q + rp \\Rightarrow r = qp^*r=q+rp⇒r=qp∗\n",
      "url": "/other/regexp/"
    }
  ], 
  "documents": [
    {
      "image": "../../assets/img/blog/call-stack-buffer-overflow-tmb.jpg",
      "title": "Call Stack - buffer overflow vulnerability",
      "date": "2019-06-30 00:00:00 +0200",
      
      "content": "Buffer overflows are a kind of call stack vulnerability that occur when buffers are created on the stack, but accessed improperly. Buffer underruns are typically not so dangerous, because writing in the current stack frame or beyond the stack pointer will only affect local variables on that stack frame. On the other side, buffer overruns can allow the attacker to overwrite the return address and thus even modify the program’s behaviour.\n\nBuffer overflow\n\nC programmers often allocate buffers on the stack to handle user input. If the input reading logic is implemented incorrectly and has now buffer length checks, a underflow/overflow can happen. If the user input is long enouph, it will overwrite the saved ebp register of the previous stack frame and, what matters most, the return address.\n\n\n\nExample\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid __attribute__((noinline)) fun(int a, int b, int c) {\n\t\n\tchar buffer[16] = {0};\n\t\n\tint* prevEbp = &amp;a - 2;\n\tint* ret = &amp;a - 1;\n\t\n\tprintf(\"Buffer start: %p Buffer start pointer address: %p\\n\", buffer, &amp;buffer);\n\tprintf(\"Previous EBP: %p Value: %d Value as hex: %x\\n\", prevEbp, *prevEbp, *prevEbp);\n\tprintf(\"Return address: %p Value: %x\\n\", ret, *ret);\n\tprintf(\"Buffer end: %p\\n\", buffer + 16);\n\t\n\tfflush(stdout);\n\t\n}\n\nint main() {\n\tprintf(\"Ptr size: %d bytes\\n\", sizeof(void*));\n\tfun(1, 2, 3)\n\treturn 0;\n}\n\n\nWe can calculate the return address position by taking addresses of the buffer and the function arguments. In this case we only take the pointer to the first argument, because it is added to the stack last. The previous base pointer size as well as the return address size are 4 bytes, so we can just subtract 1 (4 bytes) from the pointer to get the return address and 2 (8 bytes) to get the base pointer.\n\nWe can now compile the program with the -fno-stack-protector flag to disable stack protecting canary that gcc adds by default:\n\n$ gcc main.c -o viewret -fno-stack-protector\n\n\nBy running the program I got:\n\nPtr size: 4 bytes\nBuffer start: 0061FEE8 Buffer start pointer address: 0061FEE8\nPrevious EBP: 0061FF08 Value: 6422312 Value as hex: 61ff28\nReturn address: 0061FF0C Value: 401508\nBuffer end: 0061FEF8\n\n\nWe can easly alter the return address value now:\n\nvoid __attribute__((noinline)) fun(int a, int b, int c) {\n\t\n\tchar buffer[16] = {0};\n\t\n\tint* prevEbp = &amp;a - 2;\n\tint* ret = &amp;a - 1;\n\t\n\tprintf(\"Buffer start: %p Buffer start pointer address: %p\\n\", buffer, &amp;buffer);\n\tprintf(\"Previous EBP: %p Value: %d Value as hex: %x\\n\", prevEbp, *prevEbp, *prevEbp);\n\tprintf(\"Return address: %p Value: %x\\n\", ret, *ret);\n\tprintf(\"Buffer end: %p\\n\", buffer + 16);\n\t\n\tfflush(stdout);\n\t\n\t*ret = 0xcafeefac;\n\t\n}\n\n\nNow, if we run the program we will get a segmentation fault error because the function will try to jump back to the calee using an invalid address.\n\nWe can examine exactly how it works by running the GDB debugger:\n\n$ gdb viewret.exe\n\n\nOf course, we need to set the breakpoint at the fun function:\n\n(gdb) $ br fun\n\n\n[New Thread 3388.0x3368]\n[New Thread 3388.0x1a2c]\nPtr size: 4 bytes\n\nBreakpoint 1, 0x00401416 in fun ()\n\n\nBy using the frame command we can view the saved registers if the current stack frame.\n\n(gdb) $ info frame\n\n\nStack level 0, frame at 0x61ff10:\n eip = 0x401416 in fun; saved eip 0x401508\n called by frame at 0x61ff30\n Arglist at 0x61ff08, args:\n Locals at 0x61ff08, Previous frame's sp is 0x61ff10\n Saved registers:\n  ebp at 0x61ff08, eip at 0x61ff0c\n\n\nThe ebp register of the previous stack frame is at address 0x61ff08, the return address - at 0x61ff0c. The values are the same as generated by the program above.\n\n(gdb) $ c\n\n\nContinuing.\nBuffer start: 0061FEE8 Buffer start pointer address: 0061FEE8\nPrevious EBP: 0061FF08 Value: 6422312 Value as hex: 61ff28\nReturn address: 0061FF0C Value: 401508\nBuffer end: 0061FEF8\n\nProgram received signal SIGSEGV, Segmentation fault.\n0xcafeefac in ?? ()\n\n\nBy stepping over the breakpoint we can see the invalid return address that caused the segmentation fault.\n\nAltering variables\n\nLet’s examine another program that reads data from the standart input stream:\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n\t\n\tvolatile int zero;\n\t\n\tchar buffer[64];\n\t\n\tzero = 0;\n\t\n\tgets(buffer);\n\t\n\tif (zero) {\n\t\tprintf(\"You changed the zero variable to %d (hex: %x)!\", zero, zero);\n\t}\n\telse {\n\t\tputs(\"Variable not changed.\");\n\t}\n\t\n\treturn 0;\n}\n\n\nThe zero variable is marked as volatile to prevent the compiler from optimizing it’s usage, e.g. by caching it’s value in one of the general-purpose registers.\n\nBy dissassembling with GDB we get:\n\n0x00401410 &lt;+0&gt;:     push   ebp ; save the previous ebp register\n0x00401411 &lt;+1&gt;:     mov    ebp,esp ; initializing ebp of the new stack frame\n0x00401413 &lt;+3&gt;:     and    esp,0xfffffff0 ; memory aligning\n0x00401416 &lt;+6&gt;:     sub    esp,0x60 ; memory allocation on the stack\n0x00401419 &lt;+9&gt;:     call   0x401980 &lt;__main&gt;\n0x0040141e &lt;+14&gt;:    mov    DWORD PTR [esp+0x5c],0x0 ; assign to zero\n; eax = esp + 0x1c\n0x00401426 &lt;+22&gt;:    lea    eax,[esp+0x1c]\n; the address calculated with the previous instruction gets saved on the stack\n0x0040142a &lt;+26&gt;:    mov    DWORD PTR [esp],eax\n0x0040142d &lt;+29&gt;:    call   0x403ae8 &lt;gets&gt; ; gets() call\n; load the value from the memory for comparison\n0x00401432 &lt;+34&gt;:    mov    eax,DWORD PTR [esp+0x5c]\n0x00401436 &lt;+38&gt;:    test   eax,eax ; test if it is zero\n0x00401438 &lt;+40&gt;:    je     0x401458 &lt;main+72&gt; \n0x0040143a &lt;+42&gt;:    mov    edx,DWORD PTR [esp+0x5c]\n; commands needed for printf\n0x0040143e &lt;+46&gt;:    mov    eax,DWORD PTR [esp+0x5c]\n0x00401442 &lt;+50&gt;:    mov    DWORD PTR [esp+0x8],edx\n0x00401446 &lt;+54&gt;:    mov    DWORD PTR [esp+0x4],eax\n0x0040144a &lt;+58&gt;:    mov    DWORD PTR [esp],0x405044\n0x00401451 &lt;+65&gt;:    call   0x403ac8 &lt;printf&gt; ; success print\n0x00401456 &lt;+70&gt;:    jmp    0x401464 &lt;main+84&gt; ; jump over the else branch\n0x00401458 &lt;+72&gt;:    mov    DWORD PTR [esp],0x405073\n0x0040145f &lt;+79&gt;:    call   0x403ac0 &lt;puts&gt; ; error print\n; return with exit code 0\n0x00401464 &lt;+84&gt;:    mov    eax,0x0\n0x00401469 &lt;+89&gt;:    leave\n0x0040146a &lt;+90&gt;:    ret\n0x0040146b &lt;+91&gt;:    nop\n0x0040146c &lt;+92&gt;:    xchg   ax,ax\n0x0040146e &lt;+94&gt;:    xchg   ax,ax\n\n\nWe can set 2 breakpoints - before and after the gets() call.\n\n(gdb) $ br *0x0040142d\n(gdb) $ br *0x00401432\n\n\nWith gdb we can define what commands to run when these breakpoints are reached:\n\n(gdb) $ define hook-stop\n&gt;info registers\n&gt;x/24wx $esp\n&gt;x/2i $eip\n&gt;end\n\n\nWith the commands above we will see the register state, 24 machine words on the stack and two next instuctions after the instruction pointer:\n\neax            0x61fedc 6422236\necx            0x4018f0 4200688\nedx            0x50000018       1342177304\nebx            0x2d2000 2957312\nesp            0x61fec0 0x61fec0\nebp            0x61ff28 0x61ff28\nesi            0x4012d0 4199120\nedi            0x4012d0 4199120\neip            0x40142d 0x40142d &lt;main+29&gt;\neflags         0x202    [ IF ]\ncs             0x23     35\nss             0x2b     43\nds             0x2b     43\nes             0x2b     43\nfs             0x53     83\ngs             0x2b     43\n0x61fec0:       0x0061fedc      0x00000008      0x772c8023      0x772c801a\n0x61fed0:       0xb3b6879d      0x004012d0      0x004012d0      0x00000000\n0x61fee0:       0x004018f0      0x0061fed0      0x0061ff08      0x0061ffcc\n0x61fef0:       0x772cdd70      0xc4e6dd59      0xfffffffe      0x772c801a\n0x61ff00:       0x772c810d      0x004018f0      0x0061ff50      0x0040195b\n0x61ff10:       0x004018f0      0x00000000      0x002d2000      0x00000000\n=&gt; 0x40142d &lt;main+29&gt;:  call   0x403ae8 &lt;gets&gt;\n   0x401432 &lt;main+34&gt;:  mov    eax,DWORD PTR [esp+0x5c]\n\nBreakpoint 1, 0x0040142d in main ()\n\n\nNow we can examine how the input affects the stack:\n\n(gdb) $ c\nContinuing.\n0000000000000000000000000000000000000000000\n\n\neax            0x61fedc 6422236\necx            0x772eb098       1999548568\nedx            0xa      10\nebx            0x2d2000 2957312\nesp            0x61fec0 0x61fec0\nebp            0x61ff28 0x61ff28\nesi            0x4012d0 4199120\nedi            0x4012d0 4199120\neip            0x401432 0x401432 &lt;main+34&gt;\neflags         0x216    [ PF AF IF ]\ncs             0x23     35\nss             0x2b     43\nds             0x2b     43\nes             0x2b     43\nfs             0x53     83\ngs             0x2b     43\n0x61fec0:       0x0061fedc      0x00000008      0x772c8023      0x772c801a\n0x61fed0:       0xb3b6879d      0x004012d0      0x004012d0      0x30303030\n0x61fee0:       0x30303030      0x30303030      0x30303030      0x30303030\n0x61fef0:       0x30303030      0x30303030      0x30303030      0x30303030\n0x61ff00:       0x30303030      0x00303030      0x0061ff50      0x0040195b\n0x61ff10:       0x004018f0      0x00000000      0x002d2000      0x00000000\n=&gt; 0x401432 &lt;main+34&gt;:  mov    eax,DWORD PTR [esp+0x5c]\n   0x401436 &lt;main+38&gt;:  test   eax,eax\n\nBreakpoint 2, 0x00401432 in main ()\n\n\nAs we see, 43 zero-characters (ascii code 0x30) was not enouph to get to the zero value that we want ti akter. In order to get to the value, we need 64 bytes (because the buffer size is 64). For demonstration purpuses we will use the following string as the input:\n\n000011111111111111112222222222222222333333333333333344444444444456\n\n\nThis string contains 66 characters, so the 2 last characters 5 and 6 should overwrite the 2 least significant bytes (because memory endianness is little-endian) of the variable.\n\n(gdb) $ c\nContinuing.\n000011111111111111112222222222222222333333333333333344444444444456\n\n\neax            0x61fedc 6422236\necx            0x772eb098       1999548568\nedx            0xa      10\nebx            0x3f9000 4165632\nesp            0x61fec0 0x61fec0\nebp            0x61ff28 0x61ff28\nesi            0x4012d0 4199120\nedi            0x4012d0 4199120\neip            0x401432 0x401432 &lt;main+34&gt;\neflags         0x216    [ PF AF IF ]\ncs             0x23     35\nss             0x2b     43\nds             0x2b     43\nes             0x2b     43\nfs             0x53     83\ngs             0x2b     43\n0x61fec0:       0x0061fedc      0x00000008      0x772c8023      0x772c801a\n0x61fed0:       0xe53b01b1      0x004012d0      0x004012d0      0x30303030\n0x61fee0:       0x31313131      0x31313131      0x31313131      0x31313131\n0x61fef0:       0x32323232      0x32323232      0x32323232      0x32323232\n0x61ff00:       0x33333333      0x33333333      0x33333333      0x33333333\n0x61ff10:       0x34343434      0x34343434      0x34343434      0x00003635\n=&gt; 0x401432 &lt;main+34&gt;:  mov    eax,DWORD PTR [esp+0x5c]\n   0x401436 &lt;main+38&gt;:  test   eax,eax\n\nBreakpoint 2, 0x00401432 in main ()\n\n\nBy continuing we see that the variable now contains 0x3635 or 13877 in decimal.\n\n(gdb) $ c\n\n\nContinuing.\nYou changed the zero variable to 13877 (hex: 3635)![Inferior 1 (process 4848) exited normally]\nError while running hook_stop:\nThe program has no registers now.\n\n\nIn order to alter the zero variable we need to represent the number in the little endian form and write the corresponding bytes to the 65, 66, 67 and 68 offsets in in buffer.\n\nProtection against buffer overflows\n\nCompilers and operating systems have some techniques to prevent such stack exploits. In gcc, for example, if the function allocates a buffer on the stack, an additional so-called stack canary is added. A stack canary is just a random integer generated when the function is called. Before returning the function makes sure that the canary has the same value. If the canary has been altered, the program is terminated with a fatal Stack smashing detected error.\n\nAnother technique used by operating systems is restricting code evaluation on the stack. When the stack overflow is exploited, hackers will try to overwrite the return address so that it points at the buffer location with the malicious code injected. Even if the exact address is not known, it is possible to construct a NO-OP-instruction slide in the stack buffer so that a jump at any address within this slide will lead to malicious code execution. Exact stack addresses are typically different after every program run because operating systems push environmental variables onto it.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/call-stack-buffer-overflow/"
    },{
      "image": "../../assets/img/blog/extended-euklidian-code.jpg",
      "title": "Extended Euclidean algorithm without stack or recursion",
      "date": "2020-02-21 00:00:00 +0100",
      
      "content": "Typical implementation of the extended Euclidean algorithm on the internet will just iteratively calculate modulo until 0 is reached. However, sometimes you also need to calculate the linear combination coefficients for the greatest common divisor.\n\nExtended Euclidean algorithm\n\nThe extended Euclidean algorithm allows us not only to calculate the gcd (greatest common divisor) of 2 numbers, but gives us also a representation of the result in a form of a linear combination:\n\ngcd⁡(a,b)=u⋅a+v⋅bu,v∈Z\\gcd(a, b) = u \\cdot a + v \\cdot b \\quad u,v \\in \\mathbb{Z}gcd(a,b)=u⋅a+v⋅bu,v∈Z\n\ngcd of more than 2 numbers can always be done by iteratively calculating the gcd of 2 numbers.\n\nFor example, let’s calculate gcd⁡(14,5)\\gcd(14, 5)gcd(14,5):\n\n14=5⋅2+45=4⋅1+14=1⋅4+0\\begin{aligned}\n14 &amp;= 5 \\cdot 2 + 4 \\\\\n5 &amp;= 4 \\cdot 1 + 1 \\\\\n4 &amp;= 1 \\cdot 4 + 0\n\\end{aligned}1454​=5⋅2+4=4⋅1+1=1⋅4+0​\n\nSo the greatest common divisor of 141414 and 555 is 111.\n\nWe can find the linear combination coefficients by writing 111 in terms of 141414 and 555:\n\n1=5−4⋅1=5−(14−5⋅2)⋅1=5−14+5⋅2=3⋅5+(−1)⋅14\\begin{aligned}\n1 &amp;= 5 - 4 \\cdot 1 \\\\\n&amp;= 5 - (14 - 5 \\cdot 2) \\cdot 1 \\\\\n&amp;= 5 - 14 + 5 \\cdot 2 \\\\\n&amp;= 3 \\cdot 5 + (-1) \\cdot 14\n\\end{aligned}1​=5−4⋅1=5−(14−5⋅2)⋅1=5−14+5⋅2=3⋅5+(−1)⋅14​\n\nSo in this case u=3u = 3u=3 and v=−1v = -1v=−1:\n\ngcd⁡(14,5)=(−1)⋅14+3⋅5=1\\gcd(14, 5) = (-1) \\cdot 14 + 3 \\cdot 5 = 1gcd(14,5)=(−1)⋅14+3⋅5=1\n\nWe can calculate the linear combination coefficients by doing back substitution. But it is not so easy to implement this without recursion, because the back substitution is done when we are climbing out of the recursive calls. We will implement the algorithm recursively first.\n\nRecursive implementation\n\nThe formula\n\ngcd⁡(a,b)={b,if a=0gcd⁡(b mod a,a),otherwise\\gcd(a, b) =\n\t\\begin{cases}\n\tb, &amp; \\text{if}\\ a = 0 \\\\\n\t\\gcd(b \\bmod a, a), &amp; \\text{otherwise}\n\t\\end{cases}gcd(a,b)={b,gcd(bmoda,a),​if a=0otherwise​\n\nallows us to describe the algorithm in a functional way:\n\n\n  If a=0a = 0a=0, then the greatest common divisor is bbb. Coefficients u=0u = 0u=0 and v=0v = 0v=0.\n  Else, we make the problem simpler by calculating gcd⁡(b mod a,a)\\gcd(b \\bmod a, a)gcd(bmoda,a). We can calculate the new coefficients based on the coefficients of the simpler problem.\n\n\nSo, how can we calculate uuu and vvv so that\n\ngcd⁡(a,b)=u⋅a+v⋅b\\gcd(a, b) = u \\cdot a + v \\cdot bgcd(a,b)=u⋅a+v⋅b\n\nby knowing u′u&#x27;u′ and v′v&#x27;v′ with:\n\ngcd⁡(b mod a,a)=u′⋅(b mod a)+v′⋅a\\gcd(b \\bmod a, a) = u&#x27; \\cdot (b \\bmod a) + v&#x27; \\cdot agcd(bmoda,a)=u′⋅(bmoda)+v′⋅a\n\nIn order to do that we can write b mod ab \\bmod abmoda in terms of initial aaa and bbb:\n\ngcd⁡(b mod a,a)=u′⋅(b mod a)+v′⋅a=u′⋅(b−⌊ba⌋⋅a)+v′⋅a=u′⋅b−u′⋅⌊ba⌋⋅a+v′⋅a=(v′−u′⋅⌊ba⌋)⋅a+u′⋅b\\begin{aligned}\n\t\\gcd(b \\bmod a, a)\n    \t&amp;= u&#x27; \\cdot (b \\bmod a) + v&#x27; \\cdot a \\\\\n    \t&amp;= u&#x27; \\cdot (b - \\left\\lfloor \\frac{b}{a} \\right\\rfloor \\cdot a) + v&#x27; \\cdot a \\\\\n    \t&amp;= u&#x27; \\cdot b - u&#x27; \\cdot \\left\\lfloor \\frac{b}{a} \\right\\rfloor \\cdot a + v&#x27; \\cdot a \\\\\n    \t&amp;= (v&#x27; - u&#x27; \\cdot \\left\\lfloor \\frac{b}{a} \\right\\rfloor) \\cdot a + u&#x27; \\cdot b\n\\end{aligned}gcd(bmoda,a)​=u′⋅(bmoda)+v′⋅a=u′⋅(b−⌊ab​⌋⋅a)+v′⋅a=u′⋅b−u′⋅⌊ab​⌋⋅a+v′⋅a=(v′−u′⋅⌊ab​⌋)⋅a+u′⋅b​\n\nSo the new linear combination coefficients are:\n\nu=v′−u′⋅⌊ba⌋v=u′\\begin{aligned}\n    u &amp;= v&#x27; - u&#x27; \\cdot \\left\\lfloor \\frac{b}{a} \\right\\rfloor \\\\\n    v &amp;= u&#x27;\n\\end{aligned}uv​=v′−u′⋅⌊ab​⌋=u′​\n\nWith this formula we are now ready to implement the algorithm:\n\ndef extended_gcd(a: int, b: int) -&gt; (int, int, int):\n    if a == 0:\n        return b, 0, 1\n    (g, u, v) = extended_gcd(b % a, a)\n    new_u = v - (b // a) * u\n    new_v = u\n    return g, new_u, new_v\n\n\nNon-recursive implementation\n\nThe recursion in the algorithm above cannot be easily eliminated because the function is not tail-recursive.\n\nIn order to implement the algorithm with a loop we need to define a sequence of division remainders and then update the coefficients as we calculate the remainders. Formally, we can define a finite sequence rnr_nrn​:\n\nr1=ar2=brn+2=rn mod rn+1\\begin{aligned}\nr_1 &amp;= a \\\\\nr_2 &amp;= b \\\\\nr_{n+2} &amp;= r_n \\bmod r_{n+1}\n\\end{aligned}r1​r2​rn+2​​=a=b=rn​modrn+1​​\n\nIf rn+1=0r_{n+1} = 0rn+1​=0, rn+2r_{n+2}rn+2​ is not defined. We can write each rnr_nrn​ as a linear combination of uuu and vvv. Now we are interested in how uuu and vvv change as we calculate remainders. To do this formally, we will need to define two new finite sequences unu_nun​ and vnv_nvn​ which will represent the linear combination coefficients:\n\nrn=un⋅a+vn⋅br_n = u_n \\cdot a + v_n \\cdot brn​=un​⋅a+vn​⋅b\n\nBy definition, r1=ar_1  = ar1​=a and r2=br_2 = br2​=b, so we can directly write the linear combination coefficients for r1r_1r1​ and r2r_2r2​:\n\nu1=1v1=0u2=0v2=1\\begin{aligned}\n    u_1 &amp;= 1 \\\\\n    v_1 &amp;= 0 \\\\\n    u_2 &amp;= 0 \\\\\n    v_2 &amp;= 1\n\\end{aligned}u1​v1​u2​v2​​=1=0=0=1​\n\nLet qnq_nqn​ be the finite sequence of integer divisions in rnr_nrn​:\n\nrn=rn+1⋅qn+2+rn+2r_n = r_{n+1} \\cdot q_{n+2} + r_{n+2}rn​=rn+1​⋅qn+2​+rn+2​\n\nNow we can write unu_nun​ and vnv_nvn​ in terms of qnq_nqn​:\n\nrn+2=rn−rn+1⋅qn+2=un⋅a+vn⋅b−rn+1⋅qn+2=un⋅a+vn⋅b−(un+1⋅a+vn+1⋅b)⋅qn+2=un⋅a+vn⋅b−un+1⋅a⋅qn+2−vn+1⋅b⋅qn+2=(un−un+1⋅qn+2)⋅a+(vn−vn+1⋅qn+2)⋅b\\begin{aligned} \n    r_{n+2} &amp;= r_n - r_{n+1} \\cdot q_{n+2} \\\\\n    &amp;= u_n \\cdot a + v_n \\cdot b - r_{n+1} \\cdot q_{n+2} \\\\\n    &amp;= u_n \\cdot a + v_n \\cdot b - (u_{n+1} \\cdot a + v_{n+1} \\cdot b) \\cdot q_{n+2} \\\\\n    &amp;= u_n \\cdot a + v_n \\cdot b - u_{n+1} \\cdot a \\cdot q_{n+2} - v_{n+1} \\cdot b \\cdot q_{n+2} \\\\\n    &amp;= (u_n - u_{n+1} \\cdot q_{n+2}) \\cdot a + (v_n - v_{n+1} \\cdot q_{n+2}) \\cdot b\n\\end{aligned}rn+2​​=rn​−rn+1​⋅qn+2​=un​⋅a+vn​⋅b−rn+1​⋅qn+2​=un​⋅a+vn​⋅b−(un+1​⋅a+vn+1​⋅b)⋅qn+2​=un​⋅a+vn​⋅b−un+1​⋅a⋅qn+2​−vn+1​⋅b⋅qn+2​=(un​−un+1​⋅qn+2​)⋅a+(vn​−vn+1​⋅qn+2​)⋅b​\n\nTo get the formula for unu_nun​ and vnv_nvn​ we can just substitute nnn instead of n+2n + 2n+2:\n\nun=un−2−qn⋅un−1vn=vn−2−qn⋅vn−1\\begin{aligned}\n    u_n &amp;= u_{n-2} - q_n \\cdot u_{n-1} \\\\\n    v_n &amp;= v_{n-2} - q_n \\cdot v_{n-1}\n\\end{aligned}un​vn​​=un−2​−qn​⋅un−1​=vn−2​−qn​⋅vn−1​​\n\nWith this formula and the initial values of the unu_nun​ and vnv_nvn​ sequences we can now implement the extended Euclidean algorithm without recursion:\n\ndef extended_gcd(a: int, b: int) -&gt; (int, int, int):\n    if a == 0:\n        # The algorithm will work correctly without this check\n        # But it will take one iteration of the inner loop\n        return b, 0, 1\n\n    un_prev = 1\n    vn_prev = 0\n    un_cur = 0\n    vn_cur = 1\n\n    while b != 0:\n        # Calculate new element of the qn sequence\n        qn = a // b\n\n        # Calculate new element of the rn sequence\n        new_r = a % b\n        a = b\n        b = new_r\n\n        # Calculate new coefficients with the formula above\n        un_new = un_prev - qn * un_cur\n        vn_new = vn_prev - qn * vn_cur\n\n        # Shift coefficients\n        un_prev = un_cur\n        vn_prev = vn_cur\n        un_cur = un_new\n        vn_cur = vn_new\n\n    return a, un_prev, vn_prev\n\n\nExample\n\nWe can visualize the finite sequences we defined and see how the algorithm works with a table. We will calculate gcd⁡(104,47)\\gcd(104, 47)gcd(104,47) and it’s linear combination coefficients:\n\ngcd⁡(104,47)=u⋅104+v⋅47\\gcd(104, 47) = u \\cdot 104 + v \\cdot 47gcd(104,47)=u⋅104+v⋅47\n\n\n  \n    \n      rnr_nrn​\n      qnq_nqn​\n      unu_nun​\n      vnv_nvn​\n    \n  \n  \n    \n      104\n      -\n      1\n      0\n    \n    \n      47\n      -\n      0\n      1\n    \n    \n      10\n      2\n      1\n      -2\n    \n    \n      7\n      4\n      -4\n      9\n    \n    \n      3\n      1\n      5\n      -11\n    \n    \n      1\n      2\n      -14\n      31\n    \n    \n      0\n      3\n      33\n      20\n    \n  \n\n\nAt each step we first calculate the next element from the qnq_nqn​ sequence and then use it to calculate new linear combination coefficients unu_nun​ and vnv_nvn​.\n\nThe result of the algorithm:\n\ngcd⁡(104,47)=−14⋅104+31⋅47=1\\gcd(104, 47) = -14 \\cdot 104 + 31 \\cdot 47 = 1gcd(104,47)=−14⋅104+31⋅47=1\n\nImprovement of the non-recusive solution\n\nAs we see in the example above, we don’t need to calculate the last row of the table because we aren’t interested in the linear combination that forms zero. We can terminate the algorithm directly after calculating the new element of the rnr_nrn​ sequence:\n\ndef extended_gcd(a: int, b: int) -&gt; (int, int, int):\n    if a == 0: # Optional check\n        return b, 0, 1\n\n    if b == 0: # Without this check the first iteration will divide by zero\n        return a, 1, 0\n\n    un_prev = 1\n    vn_prev = 0\n    un_cur = 0\n    vn_cur = 1\n\n    while True:\n        qn = a // b\n        new_r = a % b\n        a = b\n        b = new_r\n\n        if b == 0:\n            return a, un_cur, vn_cur\n\n        # Update coefficients\n        un_new = un_prev - qn * un_cur\n        vn_new = vn_prev - qn * vn_cur\n\n        # Shift coefficients\n        un_prev = un_cur\n        vn_prev = vn_cur\n        un_cur = un_new\n        vn_cur = vn_new\n\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/non-recursive-extended-euklidean-algorithm/"
    },{
      "image": "../../assets/img/blog/numbers-are-regular-languages-tmb.jpg",
      "title": "Numbers in congruence classes are regular languages",
      "date": "2020-08-29 00:00:00 +0200",
      
      "content": "In this post we will consider natural Radix-b numbers in positional number systems. The congruence class of any such arbitrary natural number can be determined by a finite automata, and thus, intuitively speaking, the language of all Radix-b numbers that satisfy some fixed properties modulo b is regular.\n\nRadix-b numbers divisible by m\n\nThe key idea behind the automation construction is that after reading a digit, the new congruence class depends only on the current read digit and on the previous congruence class. There are only m&lt;∞m &lt; \\inftym&lt;∞ congruence classes so we can construct a finite automata that accepts all numbers divisible by m:\n\nA=(Z,Σ,δ,z0,E)Z:={0,…,m−1}Σ:={0,…,b−1}E:={0}z0:=0δ(r,d):=r⋅b+d mod m\\begin{aligned}\nA &amp;= (Z,\\Sigma,\\delta, z_0, E) \\\\\nZ &amp;:= \\{0,\\dots, m-1\\} \\\\\n\\Sigma &amp;:= \\{0,\\dots, b-1\\} \\\\\nE &amp;:= \\{0\\} \\\\\nz_0 &amp;:= 0 \\\\\n\\delta(r,d) &amp;:= r \\cdot b + d \\bmod{m}\n\\end{aligned}AZΣEz0​δ(r,d)​=(Z,Σ,δ,z0​,E):={0,…,m−1}:={0,…,b−1}:={0}:=0:=r⋅b+dmodm​\n\nThe correctness of the transition function can be seen as follows: when the automation is in state rrr, the number that has been read is in the congruence class rrr modulo mmm. So we can write it as mk+rmk+rmk+r. By multiplying with bbb (i.e. shift the number by 1 digit to the left) and adding ddd we get the new congruence class:\n\nδ(r,d)=(mk+r)⋅b+d mod m=mkb+rb+d mod m=rb+d mod m\\begin{aligned}\n\\delta(r,d) &amp;= (mk + r)\\cdot b + d \\bmod{m} \\\\\n&amp;= mkb + rb + d \\bmod{m} \\\\\n&amp;= rb + d \\bmod{m}\n\\end{aligned}δ(r,d)​=(mk+r)⋅b+dmodm=mkb+rb+dmodm=rb+dmodm​\n\nOf course, by altering the set of final states EEE we can accept not only multiples of mmm, but any such number nnn with n mod m∈En \\bmod{m} \\in Enmodm∈E for any E⊆ZE \\subseteq ZE⊆Z.\n\nBinary numbers modulo 4\n\nFor example, we can construct a finite automation that accepts all binary (b=2b = 2b=2) numbers divisible by 4 (m=4m = 4m=4):\n\n\n\nThe transitions are obtained as follows:\n\n0⋅2 mod 4=0  ⟹  δ(0,0)=0,δ(0,1)=11⋅2 mod 4=2  ⟹  δ(1,0)=2,δ(1,1)=32⋅2 mod 4=0  ⟹  δ(2,0)=0,δ(2,1)=13⋅2 mod 4=2  ⟹  δ(3,0)=2,δ(3,1)=3\\begin{aligned}\n0 \\cdot 2 \\bmod{4} = 0 &amp;\\implies \\delta(0, 0) = 0, \\delta(0, 1) = 1 \\\\\n1 \\cdot 2 \\bmod{4} = 2 &amp;\\implies \\delta(1, 0) = 2, \\delta(1, 1) = 3 \\\\\n2 \\cdot 2 \\bmod{4} = 0 &amp;\\implies \\delta(2, 0) = 0, \\delta(2, 1) = 1 \\\\\n3 \\cdot 2 \\bmod{4} = 2 &amp;\\implies \\delta(3, 0) = 2, \\delta(3, 1) = 3\n\\end{aligned}0⋅2mod4=01⋅2mod4=22⋅2mod4=03⋅2mod4=2​⟹δ(0,0)=0,δ(0,1)=1⟹δ(1,0)=2,δ(1,1)=3⟹δ(2,0)=0,δ(2,1)=1⟹δ(3,0)=2,δ(3,1)=3​\n\nIn this particular case the constructed automation is not minimal. States s1 and s3 are equivalent (this can be formally proven with the Myhill-Nerode theorem) and can be replaced by one state:\n\n\n\nAs languages accepted by DFA’s are exactly the regular languages, we can transform any DFA in a regular expression to see the exact structure of numbers divisible, for example, by 4. Unfortunately such regular expressions are very long when constructed with the Kleene or with the Arden method by a computer. These regular expressions must be massively simplified in order to be readable. With the Kleene construction I’ve implemented in this project we get the following regular expression:\n\nε|0|(ε|0)0*(ε|0)|(1|(ε|0)0*1)(1|0*1)*0*(ε|0)|(1|(ε|0)0*1)(1|0*1)*0((1|00*1)(1|0*1)*0)*(0|00*(ε|0)|(1|00*1)(1|0*1)*0*(ε|0))\n\n\nIt can be simplified by computer heuristics down to:\n\nε|0|1(1|01)*00|(0|ε|1(1|01)*00)(0|1(1|01)*00)*(0|ε|1(1|01)*00)\n\n\nStill, the regular expression is not so readable. By transforming it further by hand, we can prove that it is equivalent to:\n\nε|0|1(1|01)*00|(0|ε|1(1|01)*00)(0|1(1|01)*00)*(0|ε|1(1|01)*00) =\nε|0|1(1|01)*00|(ε|0|1(1|01)*00)(0|1(1|01)*00)*(ε|0|1(1|01)*00) =\nε|0|1(1|01)*00|(0|1(1|01)*00)*(ε|0|1(1|01)*00) =\nε|0|1(1|01)*00|(0|1(1|01)*00)* =\nε|0|(0|1(1|01)*00)* =\nε|0|(0|1(1|0)*00)* =\nε|0|0*(1(1|0)*000*)* =\nε|0|0*(1(1|0)*0*00)* =\nε|0|0*(1(1|0)*00)* =\nε|0|0*(1(0|1)*00)* =\nε|0|0*(1(0|1)*00|ε) =\nε|0|0*|0*1(0|1)*00 =\nε|0|0*|0*(0|1)*00 =\nε|0|0*(0|1)*00 =\nε|0|(0|1)*00\n\n\nSo, any binary number divisible by 4 must be zero or end with 00.\n\nOther examples\n\nDecimal numbers divisible by 20\n\n\n\nDecimal numbers divisible by 75\n\n\n\nHexadecimal numbers divisible by 24\n\n\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/numbers-are-regular-languages/"
    },{
      "image": "../../assets/img/blog/compressing-congruence-automata-tmb.jpg",
      "title": "Efficient compression of congruence class automations",
      "date": "2020-09-01 00:00:00 +0200",
      
      "content": "As already discussed in the previous post, any radix-b number nnn with n mod m∈E⊆Zn \\bmod{m} \\in E \\subseteq Znmodm∈E⊆Z can be accepted by finite automata in a digit-by-digit manner. However, the construction is not always optimal. The amount of states required is not always the amount of congruence classes. In this post we will examine when exactly the finite automata can be simplified by combining states. Reducing the amount of states will also help produce a much simpler regular expression, for example, with the Kleene construction.\n\nGeneral way to optimize an automation\n\nOf course, we can optimize the DFA with the well-known algorithm based on the Myhill-Nerode theorem. Unfortunately, even an optimized implementation is not efficient enough to do the optimization for large mmm and bbb, because the complexity of the algorithm expressed in these terms is O(b⋅m2)O(b \\cdot m^2)O(b⋅m2), as the algorithm needs to iterate over all m2m^2m2 pairs of states and consider all the outgoing transitions (bbb in each state). With this tool we can run the algorithm and get the following results for b=10b = 10b=10 and 1≤m≤201 \\le m \\le 201≤m≤20:\n\nm= 1 |Z|= 1  Z = [[0]]\nm= 2 |Z|= 2  Z = [[0], [1]]\nm= 3 |Z|= 3  Z = [[0], [1], [2]]\nm= 4 |Z|= 3  Z = [[0], [3, 1], [2]]\nm= 5 |Z|= 2  Z = [[0], [2, 1, 3, 4]]\nm= 6 |Z|= 4  Z = [[0], [4, 1], [5, 2], [3]]\nm= 7 |Z|= 7  Z = [[0], [1], [2], [3], [4], [5], [6]]\nm= 8 |Z|= 5  Z = [[0], [5, 1], [6, 2], [7, 3], [4]]\nm= 9 |Z|= 9  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8]]\nm=10 |Z|= 2  Z = [[0], [5, 4, 3, 2, 7, 6, 9, 8, 1]]\nm=11 |Z|=11  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\nm=12 |Z|= 7  Z = [[0], [7, 1], [8, 2], [9, 3], [10, 4], [11, 5], [6]]\nm=13 |Z|=13  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]]\nm=14 |Z|= 8  Z = [[0], [8, 1], [9, 2], [10, 3], [11, 4], [12, 5], [13, 6], [7]]\nm=15 |Z|= 4  Z = [[0], [13, 10, 7, 4, 1], [5, 2, 8, 11, 14], [9, 6, 12, 3]]\nm=16 |Z|= 9  Z = [[0], [9, 1], [10, 2], [11, 3], [12, 4], [13, 5], [14, 6], [15, 7], [8]]\nm=17 |Z|=17  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16]]\nm=18 |Z|=10  Z = [[0], [10, 1], [11, 2], [12, 3], [13, 4], [14, 5], [15, 6], [16, 7], [17, 8], [9]]\nm=19 |Z|=19  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18]]\nm=20 |Z|= 3  Z = [[0], [13, 11, 15, 17, 5, 3, 7, 9, 19, 1], [12, 10, 14, 16, 4, 2, 6, 8, 18]]\n\n\nHere, ZZZ is the set of states in the new, minimal DFA. ZZZ is a set of equivalence classes that represent which states can be combined into one state. These equivalence classes have been computed for E:={0}E := \\{0\\}E:={0}. That is why, for example, for m=8m = 8m=8 states 0 and 4 aren’t in one equivalence class. They can’t be equivalent, because state 4 is intermediate while state 0 is final.\n\nComputing equivalence classes ahead-of-time\n\nAs you can see, there is clearly a pattern in the equivalence states above. For example, we can see that no states are equivalent when mmm is prime. By further examining the data above we can even conclude that the DFA probably can’t be simplified if gcd⁡(b,m)=1\\gcd(b, m) = 1gcd(b,m)=1 i.e. if bbb and mmm are coprime.\n\nAfter formally examining when the states can be combined I came up with the following result:\n\nTheorem\n\nIf m≤bm \\le bm≤b and E≠∅E \\neq \\varnothingE​=∅, then the minimum finite automation can be constructed as follows:\n\nZ=⋃A∈Z′⋅{A∩E,A\\E}\\{∅}Z = \\overset{\\cdot}{\\bigcup_{A\\in Z&#x27;}} \\{A \\cap E, A \\backslash E\\}\\backslash\\{\\varnothing\\}Z=A∈Z′⋃​⋅​{A∩E,A&lt;/span&gt;E}&lt;/span&gt;{∅}\n\nwhere Z′Z&#x27;Z′ is the set of equivalence classes of the following equivalence relation:\n\nx∼y⇔x≡ymod  mgcd⁡(b,m)x \\sim y \\Leftrightarrow x \\equiv y \\mod{\\frac{m}{\\gcd(b, m)}}x∼y⇔x≡ymodgcd(b,m)m​\n\nProof\n\nBy definition of δ\\deltaδ, we know that:\n\nδ(r,d)=k  ⟹  δ(r,d+1 mod b)=k+1 mod m\\delta(r, d) = k \\implies \\delta(r, d + 1 \\bmod b) = k + 1 \\bmod mδ(r,d)=k⟹δ(r,d+1modb)=k+1modm\n\nBecause m≤bm \\le bm≤b, it follows that the mapping δ(x,Σ)\\delta(x, \\Sigma)δ(x,Σ) is surjective. In other words, m≤bm \\le bm≤b implies that every state has at least one transition to any other state. Any change to this mapping will shift it. As E≠∅E \\neq \\varnothingE​=∅, any shifted mapping δ′\\delta&#x27;δ′ will lead to at least 1 digit ddd such that δ(x,d)∈E\\delta(x, d) \\in Eδ(x,d)∈E and δ′(x,d)∉E\\delta&#x27;(x, d) \\notin Eδ′(x,d)∈/​E.\n\nThus, 2 states x,y∈Zx,y \\in Zx,y∈Z are equivalent if and only if they have equivalent mappings which is equivalent to:\n\nδ(x,Σ)=δ(y,Σ)⇔δ(x,d)=δ(y,d)∀d∈Σ⇔xb+d mod m=yb+d mod m⇔xb+d≡yb+dmod  m⇔xb≡ybmod  m⇔xb−yb≡0mod  m⇔(x−y)⋅b≡0mod  m⇔x−y≡0mod  mgcd⁡(b,m)⇔x≡ymod  mgcd⁡(b,m)\\begin{aligned}\n\\delta(x, \\Sigma) = \\delta(y, \\Sigma) &amp;\\Leftrightarrow \\delta(x, d) = \\delta(y, d) \\quad \\forall d \\in \\Sigma \\\\\n&amp;\\Leftrightarrow xb + d \\bmod m = yb + d \\bmod m \\\\\n&amp;\\Leftrightarrow xb + d \\equiv yb + d \\mod{m} \\\\\n&amp;\\Leftrightarrow xb \\equiv yb \\mod{m} \\\\\n&amp;\\Leftrightarrow xb - yb \\equiv 0 \\mod{m} \\\\\n&amp;\\Leftrightarrow (x-y) \\cdot b \\equiv 0 \\mod{m} \\\\\n&amp;\\Leftrightarrow x-y \\equiv 0 \\mod{\\frac{m}{\\gcd(b, m)}} \\\\\n&amp;\\Leftrightarrow x \\equiv y \\mod{\\frac{m}{\\gcd(b, m)}}\n\\end{aligned}δ(x,Σ)=δ(y,Σ)​⇔δ(x,d)=δ(y,d)∀d∈Σ⇔xb+dmodm=yb+dmodm⇔xb+d≡yb+dmodm⇔xb≡ybmodm⇔xb−yb≡0modm⇔(x−y)⋅b≡0modm⇔x−y≡0modgcd(b,m)m​⇔x≡ymodgcd(b,m)m​​\n\nFirst corollary\n\nIt follows, that if m≤bm \\le bm≤b, E≠∅E \\neq \\varnothingE​=∅ and gcd⁡(b,m)=1\\gcd(b, m) = 1gcd(b,m)=1, then no states can be combined and the minimal automation has mmm states.\n\nSecond corollary\n\nIf m≤bm \\le bm≤b and E≠∅E \\neq \\varnothingE​=∅, then the amount of states Z′Z&#x27;Z′ (before splitting each equivalence class in final states and non final ones as shown in the theorem) is equal to the following set of orbits:\n\nZ′=(Z/m)/⟨m/gcd⁡(b,m)‾⟩Z&#x27; = (\\mathbb{Z}/m)/\\langle \\overline{m / \\gcd(b, m)} \\rangleZ′=(Z/m)/⟨m/gcd(b,m)​⟩\n\nExample implementation\n\nThis Java method uses the results above to compute the equivalence classes efficiently:\n\npublic static List&lt;List&lt;Integer&gt;&gt; computeEquivalenceClasses(int b, int m, HashSet&lt;Integer&gt; finalStates) {\n    assert m &lt;= b;\n    assert !finalStates.isEmpty();\n\n    // Union-Find datastructure, preferrably with union-by-rank and path compression\n    UnionFind equivalenceClasses = new UnionFind(m);\n\n    int bmgcd = Euklidian.gcd(b, m);\n\n    if (bmgcd == 1) {\n        return equivalenceClasses.getDisjointSets();\n    }\n\n    int optimizedM = m / bmgcd;\n\n    // loop through every set in the new set of equivalence classes\n    for (int anchor = 0; anchor &lt; optimizedM; anchor++) {\n\n        int current = anchor; // [anchor] is the equivalence class\n        // oppositeAnchor is the element that is final if anchor is intermediate\n        // and intermediate if anchor is final\n        // if oppositeAnchor == anchor, then it is considered as not yet found\n        int oppositeAnchor = anchor;\n\n        boolean anchorFinal = finalStates.contains(anchor);\n\n        for (;;) {\n            current = (current + optimizedM) % m;\n\n            if (current == anchor) {\n                break;\n            }\n\n            boolean currentFinal = finalStates.contains(current);\n\n            if (currentFinal == anchorFinal) {\n                equivalenceClasses.union(current, anchor);\n            }\n            else if (oppositeAnchor == anchor) {\n                // \"initialize\" oppositeAnchor\n                oppositeAnchor = current;\n            }\n            else {\n                equivalenceClasses.union(current, oppositeAnchor);\n            }\n        }\n\n    }\n\n    return equivalenceClasses.getDisjointSets();\n}\n\n\nIf we use a union-find datastructure with union in O(1)O(1)O(1) (e.g. union-by-rank with path compression), then the complexity of the algorithm is O(m)O(m)O(m) which is much better than O(b⋅m2)O(b \\cdot m^2)O(b⋅m2).\n\nCase when m &gt; b\n\nIn case m&gt;bm &gt; bm&gt;b the equivalence relation depends very much on the set of final states EEE. With the Myhill Nerode theorem we get:\n\nx∼y⇔(xc∈L⇔yc∈L∀c∈Σ∗)⇔(x⋅b∣c∣+c mod m∈E⇔y⋅b∣c∣+c mod m∈E)\\begin{aligned}\nx \\sim y &amp;\\Leftrightarrow (xc \\in L \\Leftrightarrow yc \\in L \\quad \\forall c \\in \\Sigma^*) \\\\\n&amp;\\Leftrightarrow (x \\cdot b^{|c|} + c \\bmod m \\in E \\Leftrightarrow y \\cdot b^{|c|} + c \\bmod m \\in E)\n\\end{aligned}x∼y​⇔(xc∈L⇔yc∈L∀c∈Σ∗)⇔(x⋅b∣c∣+cmodm∈E⇔y⋅b∣c∣+cmodm∈E)​\n\nConclusion\n\nWith the above theorem it is easy to implement a much faster algorithm that computes equivalent states. It also gives an intuition, why for example it is harder to test whether a decimal number is divisible by 9 than to test whether a decimal number is divisible by 5, if we measure “hardness” by the amount of states in the minimal DFA. It is the case because gcd⁡(5,10)=5\\gcd(5, 10) = 5gcd(5,10)=5 and gcd⁡(9,10)=1\\gcd(9, 10) = 1gcd(9,10)=1.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/compressing-congruence-automata/"
    },{
      "image": "../../assets/img/blog/kadane-algorithm-tmb.jpg",
      "title": "Kadane's algorithm and the idea behind it",
      "date": "2020-09-19 00:00:00 +0200",
      
      "content": "People often refer to Kadane’s algorithm only in the context of the maximum subarray problem. However, the idea behind this algorithm allows one to use it for solving a variety of problems that have something to do with finding a continuous subarray with a given property. The algorithm can also be used in 2d or multidimensional arrays but in this post we will only consider regular one-dimensional arrays.\n\nMotivation &amp; idea\n\nIf we are searching for a subarray with some property, the easiest solution would be to try all possible intervals. A naive algorithm can do that in O(n3)O(n^3)O(n3) time, but we can improve that by computing solutions for intervals of increasing length and reusing already computed solutions (aka dynamic programming). In this case the complexity of the algorithm is O(n2)O(n^2)O(n2). Of course, this will work not for all problems. It will work, for example, for the computation of any binary associative operation for all intervals in the array. Another example for such an algorithm would be finding all palindromes in a given string:\n\n\n  Mark all letters as palindromes of length 1\n  Mark all adjacent equal letters as palindromes of length 2\n  For every fixed length kkk, starting with k=3k = 3k=3 traverse all substrings of length kkk in the string and mark the current substring as a palindrome if the letters at the start and at the end match and if the middle part is a palindrome.\n\n\nO(n2)O(n^2)O(n2) is the optimal worst-case complexity if the problem cannot be solved without traversing the entire search space. For example, this is the case for the problem of computing all palindromes of a given string (a worst-case example is an∈Σ∗a^n \\in \\Sigma^*an∈Σ∗ where the amount of palindromes is ∑i=1ni=n⋅(n+1)2∈Ω(n2)\\sum_{i=1}^{n}i =\\frac{n \\cdot (n + 1)}{2} \\in \\Omega(n^2)∑i=1n​i=2n⋅(n+1)​∈Ω(n2)).\n\nThe key question is: How can we traverse only some subset of the search space and still benefit from dynamic programming?\n\nWell, we can identify a problem with only one side of the interval in the array. We will choose the right side of the interval because that is what is commonly used in real algorithms. That way it is possible to construct a linear time algorithm the following way:\n\n\n  Create a recursive function f(k)f(k)f(k) that computes the trivial solution if k=1k = 1k=1 and computes the new solution based on the already computed result, e.g. f(k−1)f(k-1)f(k−1).\n  Find the best solution among {f(k):0≤k&lt;n}\\{f(k) : 0 \\le k &lt; n\\}{f(k):0≤k&lt;n}.\n\n\nBoth parts take linear (O(n)O(n)O(n)) time if we cache and reuse the result of f(k)f(k)f(k) or if we use dynamic programming which is better most of the times.\n\nMaximum sum subarray\n\nA good example for a construction of such an algorithm is the maximum sum subarray problem - given an array AAA of integers the algorithm should find such indices a,b∈{1,…,n−1}a,b \\in \\{1, \\dots, n-1\\}a,b∈{1,…,n−1} with a≤ba \\le ba≤b such that ∑i=abA[i]\\sum_{i=a}^b {A[i]}∑i=ab​A[i] is maximal.\n\nWe can define the problem only in terms of the right border kkk - “what is the maximum subarray ending at kkk”?\n\nClearly, the maximum subarray of the whole array is the maximum of subarrays ending at kkk for all 0≤k&lt;n0 \\le k &lt; n0≤k&lt;n. Also, the maximum sum subarray ending at k is either A[k]A[k]A[k] itself or the maximum sum subarray ending at k−1k - 1k−1 combined with A[k]A[k]A[k]. So we can now write the algorithm formally with pseudocode:\n\nmaxSumSubarrayEndingAt(A, k) {\n    if (k == 0) {\n        return (A[0], 1);\n    }\n    (sum, left) = maxSumSubarrayEndingAt(k - 1);\n    if (sum &gt;= 0) { /* sum + A[k] &gt;= A[k] */\n        /* the max subarray (ending at k) is the previous subarray together with this element */\n        return (sum + A[k], left);\n    }\n    else {\n        /* the max subarray (ending at k) is the current element */\n        return (A[k], k);\n    }\n}\n\n\nAnd the maximum subarray sum and indices can be computed with a simple maximim-search algorithm:\n\nmaxSumSubarray(A) {\n    n = size(A);\n    maxSum = A[0];\n    maxLeft = 0;\n    maxRight = 0;\n    for (r = 1; r &lt; n; r++) {\n        (s, l) = maxSumSubarrayEndingAt(A, r);\n        if (s &gt; maxSum) {\n            maxSum = s;\n            maxLeft = l;\n            maxRight = r;\n        }\n    }\n    return (maxSum, maxLeft, maxRight);\n}\n\n\nThis intermediate pseudocode solution doesn’t cache maxSumSubarrayEndingAt results and is therefore inefficient “as-is”. But we can remove recursion and rewrite the same solution with dynamic programming (Java):\n\npublic static void maxSumSubarray(int[] a) {\n    int maxSum = a[0];\n    int maxLeft = 0;\n    int maxRight = 0;\n\n    int currentSum = a[0];\n    int currentLeft = 0;\n\n    for (int r = 1; r &lt; a.length; r++) {\n\n        if (currentSum &gt;= 0) {\n            currentSum += a[r];\n        }\n        else {\n            currentSum = a[r];\n            currentLeft = r;\n        }\n\n        if (currentSum &gt; maxSum) {\n            maxSum = currentSum;\n            maxLeft = currentLeft;\n            maxRight = r;\n        }\n    }\n\n    // the sum between A[maxLeft] and A[maxRight] (inclusive) is equal maxSum and maximal\n    System.out.printf(\"Max sum: %5d Indices from %5d to %5d.\\n\", maxSum, maxLeft, maxRight);\n}\n\n\nThis is the efficient Θ(n)\\Theta(n)Θ(n) time and Θ(1)\\Theta(1)Θ(1) space solution that uses the idea of Kadane’s algorithm to compute the maximum sum subarray.\n\nExample\n\nConsider the array {-2, 1, -3, 4, -1, 2, 1, -5, -2, 5}. By running the algorithm we get the following maximum subarrays ending at a specific index:\n\n\n\nThe maximum subarray is marked red.\n\nOther problems that can be solved analogously\n\n\n  Smallest sum subarray problem\n  Largest product subarray problem\n  Smallest product subarray problem\n  Maximum circular sum\n\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/kadane-algorithm/"
    },{
      "image": "../../assets/img/blog/lowest-common-ancestor-tmb.jpg",
      "title": "Computing the lowest common ancestor in a full binary tree",
      "date": "2020-09-23 00:00:00 +0200",
      
      "content": "The lowest common ancestor (LCA) problem is important in many applications of binary trees. For example, by knowing the lowest common ancestor we can easily compute the shortest path between any two vertices in a tree. The most common way to compute the lca of vertices uuu and vvv is to iteratively go up until we get to the root of the subtree containing both uuu and vvv. This method works only if uuu and vvv are on the same level. If not, we can first measure the difference of heights ddd between uuu and vvv and then find the lowest common ancestor of the ddd-th parent of the lowest vertex and the higher vertex.\n\nLCA in a full binary tree\n\nFull binary trees are often represented implicitly in memory. That means, parent-child relations are determined by looking at the positions of nodes in the array. For example, in this tree the nodes are numbered the way they will be positioned in the array:\n\n\n\nIn a full binary tree the parent of node xxx is p(x):=⌊x−12⌋p(x):=\\left\\lfloor\\frac{x-1}{2}\\right\\rfloorp(x):=⌊2x−1​⌋, and children are 2⋅x+12 \\cdot x + 12⋅x+1 and 2⋅x+22 \\cdot x + 22⋅x+2. Formally speaking, for nodes aaa and bbb on the same level we would like to find the value of pn(a)p^n(a)pn(a) such that pn(a)=pn(b)p^n(a) = p^n(b)pn(a)=pn(b) for n minimal. In binary representation, dividing by 2 and discarding the remainder is equivalent to shifting the number by 1 bit to the right. In order to use this we can just add 1 to each node index:\n\n\n\nWith this change we can implement the lca algorithm:\n\nuint32_t lca_sameLevel(uint32_t a, uint32_t b) {\n    while (a != b) {\n        a &gt;&gt;= 1u;\n        b &gt;&gt;= 1u;\n    }\n    return a;\n}\n\n\nThis algorithm works because the lowest common ancestor is the longest common prefix of the binary representation of both nodes.\n\nIf we address nodes starting with one, then any nodewith level lll will be greater than or equal to 2l2^l2l. Therefore, the amount of leading zeroes in the binary representation of nodes in the same level is equal. Moreover, the difference of amounts of leading zeroes is equal to the difference of levels. With this idea we can now implement the algorithm that correctly finds the lowest common ancestor for any pair of nodes.\n\nuint32_t lca(uint32_t a, uint32_t b) {\n    \n    uint32_t aLeadingZeroes = __builtin_clz(a);\n    uint32_t bLeadingZeroes = __builtin_clz(b);\n    \n    while (aLeadingZeroes &gt; bLeadingZeroes) {\n        b &gt;&gt;= 1u;\n        bLeadingZeroes++;\n    }\n    \n    while (bLeadingZeroes &gt; aLeadingZeroes) {\n        a &gt;&gt;= 1u;\n        aLeadingZeroes++;\n    }\n    \n    while (a != b) {\n        a &gt;&gt;= 1u;\n        b &gt;&gt;= 1u;\n    }\n    \n    return a;\n}\n\n\nIt is also easy to modify the algorithm slightly so that it works for elements indexed starting from zero.\n\n#include &lt;stdint.h&gt;\n\nuint32_t lca(uint32_t a, uint32_t b) {\n    a++;\n    b++;\n    \n    uint32_t aLeadingZeroes = __builtin_clz(a);\n    uint32_t bLeadingZeroes = __builtin_clz(b);\n    \n    while (aLeadingZeroes &gt; bLeadingZeroes) {\n        b &gt;&gt;= 1u;\n        bLeadingZeroes++;\n    }\n    \n    while (bLeadingZeroes &gt; aLeadingZeroes) {\n        a &gt;&gt;= 1u;\n        aLeadingZeroes++;\n    }\n    \n    while (a != b) {\n        a &gt;&gt;= 1u;\n        b &gt;&gt;= 1u;\n    }\n    \n    return a - 1u;\n}\n\n\nComplexity: O(log⁡(n))O(\\log(n))O(log(n)) where nnn is the amount if nodes in the tree.\n\nExample\n\nIf the we want to find the lowest common ancestor of 8 and 10 (indexed from zero), then we add 1 to both of them and find the longest common prefix. In this case it is the longest common prefix of 1001 and 1011 which is 10 = 2 in decimal. 2 is the lowest common ancestor in the tree with increased indices, so in the original tree the lowest common ancestor is 2 - 1 = 1.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/lowest-common-ancestor/"
    },{
      "image": "../../assets/img/blog/regular-language-size-np-hard-tmb.jpg",
      "title": "Measuring the size of a regular language is NP-Hard",
      "date": "2020-10-15 00:00:00 +0200",
      
      "content": "In this post we will examine an interesting connection between the NP\\mathcal{NP}NP-complete CNF-SAT problem and the problem of computing the amount of words generated by some efficient representation of a formal language. Here, I call a way of representing a formal language efficient, if it has a polynomially-long encoding but can possibly describe an exponentially-large formal language. Examples of such efficient representations are nondeteministic finite automations and regular expressions.\n\nFormal definition of the problem\n\nWe can formally define the problem of determining the language size the following way:\n\nName: NFA Language Size\n\nInput: A nondeterministic finite automata (NFA) M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) and a number m∈Nm \\in \\mathbb{N}m∈N.\n\nQuestion: Is ∣L(M)∣≤m\\st L(M) \\st \\le m∣L(M)∣≤m?\n\nProof of NP-hardness\n\nWe will construct a reduction from the NP\\mathcal{NP}NP-complete CNF-SAT problem. Consider a formula φ\\varphiφ with kkk clauses and nnn variables:\n\n\n  Create an NFA M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) with the alphabet Σ:={0,1}\\Sigma := \\{0, 1\\}Σ:={0,1}.\n  For every variable 0≤v≤n0 \\le v \\le n0≤v≤n and clause 1≤c≤k1 \\le c \\le k1≤c≤k create a state z(v,c)∈Zz(v, c) \\in Zz(v,c)∈Z. Intuitively, if the NFA is in the state z(v,c)z(v, c)z(v,c), it means that it has already read vvv symbols of some variable assignment where clause ccc is zero.\n  For each clause 1≤c≤k1 \\le c \\le k1≤c≤k of the formula, construct a boolean cube ψ:V→{0,1,∗}\\psi : V \\to \\{0, 1, *\\}ψ:V→{0,1,∗} with ψ−1(1)\\psi^{-1}(1)ψ−1(1) containing variables that are negated in the clause and ψ−1(0)\\psi^{-1}(0)ψ−1(0) containing positive variables (other variables are mapped to ∗*∗). By doing this, we essentially construct a cube C(ψ):=(⋀ψ(x)=1x)∧(⋀ψ(x)=0xˉ)C(\\psi) := (\\bigwedge_{\\psi(x) = 1}{x}) \\wedge (\\bigwedge_{\\psi(x) = 0}{\\bar{x}})C(ψ):=(⋀ψ(x)=1​x)∧(⋀ψ(x)=0​xˉ) that is the negation of the clause. In other words, all full variable assignments ψ′:V→{0,1}\\psi&#x27; : V \\to \\{0, 1\\}ψ′:V→{0,1} such that ψ(x)≠∗⇒ψ′(x)=ψ(x)∀x∈V\\psi(x) \\neq * \\Rightarrow \\psi&#x27;(x) = \\psi(x) \\quad\\forall x \\in Vψ(x)​=∗⇒ψ′(x)=ψ(x)∀x∈V will make the clause (and the whole formula φ\\varphiφ) false. For all 0≤v&lt;n0 \\le v &lt; n0≤v&lt;n create the following transitions:\n    \n      Set δ(z(v,c),0):={z(v+1,c)}\\delta(z(v,c), 0) := \\{z(v + 1,c)\\}δ(z(v,c),0):={z(v+1,c)} if ψ(v+1)=0\\psi(v + 1) = 0ψ(v+1)=0.\n      Set δ(z(v,c),1):={z(v+1,c)}\\delta(z(v,c), 1) := \\{z(v + 1,c)\\}δ(z(v,c),1):={z(v+1,c)} if ψ(v+1)=1\\psi(v + 1) = 1ψ(v+1)=1.\n      Set δ(z(v,c),0):={z(v+1,c)}\\delta(z(v,c), 0) := \\{z(v + 1,c)\\}δ(z(v,c),0):={z(v+1,c)} and δ(z(v,c),1):={z(v+1,c)}\\delta(z(v,c), 1) := \\{z(v + 1,c)\\}δ(z(v,c),1):={z(v+1,c)} if ψ(v+1)=∗\\psi(v + 1) = *ψ(v+1)=∗.\n    \n  \n  Mark all states with v=0v = 0v=0 as initial: S:={z(0,c):1≤c≤k}S := \\{z(0, c) : 1 \\le c \\le k\\}S:={z(0,c):1≤c≤k}.\n  Mark all states with v=nv = nv=n as final: E:={z(n,c):1≤c≤k}E := \\{z(n, c) : 1 \\le c \\le k\\}E:={z(n,c):1≤c≤k}.\n\n\nBy construction, MMM will accept any variable assignment a1,…,an∈Σna_1,\\dots,a_n \\in \\Sigma^na1​,…,an​∈Σn where f(a1,…,an)=0f(a_1, \\dots, a_n) = 0f(a1​,…,an​)=0. As φ\\varphiφ has exactly 2n2^n2n variable assignments, it is satisfiable if and only if the set of accepted variable assignments L(M)L(M)L(M) has at most m:=2n−1m := 2^n - 1m:=2n−1 elements.\n\nThis reduction can clearly be done in polynomial time.\n\nIntuition &amp; Example\n\nConsider the following function:\n\nφ:=(cˉ+d)(aˉ+c+dˉ)(aˉ+bˉ+d)\\varphi := (\\bar{c} + d)(\\bar{a} + c + \\bar{d})(\\bar{a} + \\bar{b} + d)φ:=(cˉ+d)(aˉ+c+dˉ)(aˉ+bˉ+d)\n\nThe cubes that describe variable assignments where φ\\varphiφ is false are **10, 1*01 and 11*0 (variable order: a,b,c,da, b, c, da,b,c,d). These three cubes make the first, second and the third clauses false, respectively.\n\nWe can visualize all the possible variable assignments with a 4-dimensional cube (as φ\\varphiφ has 4 variables):\n\n\n\nIn this cube, every edge connects assignments that differ in exactly one bit (i.e. hamming distance = 1). Any lower dimensional cubes that are subgraphs of this cube are implicants if the vertices in the subgraph cover only assignments where the function is true. If a lower dimensional cube isn’t a part of some higher dimensional cube that still covers only assignments where the function is true, such a cube is a prime implicant. In this case, if we think of CNF-clauses as implicants of the negated function, then we can visualize them the following way:\n\n\n\nThe idea of the reduction is that if we can count the amount of vertices covered by these cubes, then we can compare this amount to the total number of vertices and if it is less than 2n2^n2n where nnn is the number of variables, then the function is satisfiable, otherwise not. The problem is that implicants aren’t always disjoint. So, the satisfiability problem is essentially the problem of comparing 2n2^n2n with the size of the union of variable assignments described by cubes.\n\nThese variable assignments that are parts of some cube(s) can be accepted with a nondeterministic finite automata (NFA) with nnn states. We can create such an NFA for each clause and then union them by marking multiple states as initial. In this example, we get the following NFA:\n\n\n\nThe top row of the NFA accepts variable assignments generated by the 11*0 cube, the middle row corresponds to the 1*01 cube and the bottom one - to **10. φ\\varphiφ is satisfiable if and only if there is at least one word of length 4, such that this NFA doesn’t accept it.\n\nConverting the NFA to a BDD\n\nThe idea of this reduction can be used to compute all the satisfying assignments of φ\\varphiφ. Consider the example above - we can apply the Rabin-Scott algorithm to convert the NFA to a DFA:\n\n\n\nThe computed DFA will always be acyclic (without the ∅\\varnothing∅-node), because the initial NFA had no cycles in it. The satisfying assignments are the ones that are not accepted by the NFA. Therefore, they are exactly the paths of length nnn, leading to ∅\\varnothing∅. A program can easily output all satisfying assignments with a DFS or BFS-search in this tree.\n\nIf we replace ∅\\varnothing∅ with the positive leaf and all final states with a negative leaf (the transitions after them can be removed), then the graph will become an ordered binary decision diagram (OBDD) of φ\\varphiφ:\n\n\n\nOf course, the nodes should be renamed to variable names:\n\n\n\nThe Rabin-Scott algorithm doesn’t always output minimal deterministic automations and therefore in most cases the OBDD will also not be minimal, like in this case where the redundant checks are clearly seen. In order to get a ROBDD we will still need to apply the elimination and the isomorphism rules (remove redundant checks and reuse subtrees):\n\n\n\nNP-Complete special case\n\nWe can tweak the problem defined above to make it belong to NP\\mathcal{NP}NP:\n\nName: NFA Rejected m-string\n\nInput: A nondeterministic finite automata (NFA) M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) and a unary-encoded number m∈Nm \\in \\mathbb{N}m∈N.\n\nQuestion: Exists such a string α∈Σ∗\\alpha \\in \\Sigma^*α∈Σ∗ of length mmm such that α∉L(M)\\alpha \\notin L(M)α∈/​L(M)?\n\nThis problem is in NP\\mathcal{NP}NP, because a string of length mmm can be used as a certificate. Then, by using the idea of the Rabin-Scott theorem, we can test whether the given string is rejected or not in polynomial time. The NP\\mathcal{NP}NP-hardness can be shown with a reduction from CNF-SAT as follows:\n\n\n  Consider a formula φ\\varphiφ with kkk clauses and nnn variables.\n  Construct an NFA M=(Z,Σ,δ,S,E)M = (Z, \\Sigma, \\delta, S, E)M=(Z,Σ,δ,S,E) by following the same steps as in the proof of NP\\mathcal{NP}NP-hardness of NFA Language Size.\n  Set m:=nm := nm:=n.\n\n\nφ\\varphiφ is satisfiable if and only if there is a rejected string of length mmm in L(M)L(M)L(M) which is exactly some satisfying assignment for φ\\varphiφ. Clearly, this is a polynomial-time reduction.\n\nFinding any rejected string is NP-Complete\n\nWe can also define another version of the problem and proof it’s NP\\mathcal{NP}NP-completeness:\n\nName: NFA Rejected String\n\nInput: A nondeterministic finite automata (NFA) M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) and a unary-encoded number m∈Nm \\in \\mathbb{N}m∈N.\n\nQuestion: Exists such a string α∈Σ∗\\alpha \\in \\Sigma^*α∈Σ∗ with ∣α∣≤m\\st \\alpha \\st \\le m∣α∣≤m such that α∉L(M)\\alpha \\notin L(M)α∈/​L(M)?\n\nThis problem is in NP\\mathcal{NP}NP, because a string of length mmm can be used as a certificate, like in the NFA Rejected m-string problem.\n\nWe will prove NP\\mathcal{NP}NP-hardness by reducing NFA Rejected m-string to this problem. Consider an NFA M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) and a number m′∈Nm&#x27; \\in \\mathbb{N}m′∈N:\n\n\n  Create mmm new nodes q1,…,qm∈Zq_1, \\dots, q_m \\in Zq1​,…,qm​∈Z.\n  For all 1≤s&lt;m1 \\le s &lt; m1≤s&lt;m and a∈Σa \\in \\Sigmaa∈Σ, set δ(qs,a):={qs+1}\\delta(q_s, a) := \\{q_{s+1}\\}δ(qs​,a):={qs+1​}.\n  Mark q1q_1q1​ as initial: S:=S∪{q1}S := S \\cup \\{q_1\\}S:=S∪{q1​}.\n  Mark q1,…,qmq_1, \\dots, q_mq1​,…,qm​ as final: E:=E∪{qi:1≤i≤m}E := E \\cup \\{q_i : 1 \\le i \\le m\\}E:=E∪{qi​:1≤i≤m}.\n  Set m:=m′m := m&#x27;m:=m′.\n\n\nBy construction, the new NFA will accept all strings of length m−1m - 1m−1 or less. Thus, there is a rejected string of length mmm if and only if there is a rejected string of length at most mmm in the new NFA. Obviously, this is a polynomial-time reduction.\n\nA few words about complexity\n\nThese problems illustrate an interesting connection between NP\\mathcal{NP}NP-complete problems, that can be solved in polynomial time by nondeterministic turing machines and the problem of just counting the language size described by an NFA. By the Rabin-Scott theorem, it is possible to convert any NFA to an equivalent DFA, but the worst-case complexity of the algorithm is Θ(2n)\\Theta(2^n)Θ(2n), because a set with nnn elements has 2n2^n2n subsets and all of these subsets will be reachable in the worst-case. Would the complexity of the subset-construction be polynomial, then it would mean that P=NP\\mathcal{P}  = \\mathcal{NP}P=NP as we can just search for all paths in the DFA that lead to ∅\\varnothing∅ after exactly nnn transitions (these paths are exactly the variable assignments that satisfy φ\\varphiφ).\n\nThe reduction discussed above can also be done in reverse. Any set of cubes can be transformed to a boolean function in conjunctive normal form. So, if P=NP\\mathcal{P} = \\mathcal{NP}P=NP and the CNF-SAT problem is solvable in polynomial time, then it is also possible to compare 2n2^n2n with the size of the union of some arbitrary boolean cubes. The cube union size problem is essentially a special case of the inclusion–exclusion principle, where the formula to compute the size of a union of nnn sets is also exponentially long, because the amount of possible intersections grows exponentially.\n\nIt seems impossible to compute the size of the union of some arbitrary cubes in polynomial time, because the variable assignments that some cube describes is exponential in the length of the encoding of the cube. And the amount of ways to intersect some cubes is also exponential. Any polynomial encoding for a cube will allow us to distinguish between an exponential amount of cubes. However, the amount of ways to intersect some kkk nnn-variable-cubes is in Ω(3(3n⋅(k−1)))\\Omega(3^{(3^{n \\cdot (k - 1)})})Ω(3(3n⋅(k−1))), so it is impossible to precisely encode the union of these kkk cubes with a polynomially long encoding. However, it could be possible to divide the search space so that it is still possible to compare 2n2^n2n with the size of the union of the cubes in polynomial time. Anyway, these thoughts aren’t even close to a formal proof that P≠NP\\mathcal{P} \\neq \\mathcal{NP}P​=NP.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/regular-language-size-np-hard/"
    },{
      "image": "../../assets/img/blog/palu-tmb.jpg",
      "title": "Using the PA=LU factorization to solve linear systems of equations for many right-hand sides efficiently",
      "date": "2020-11-28 00:00:00 +0100",
      
      "content": "Linear systems of equations come up in almost any technical discipline. The PA=LUPA=LUPA=LU factorization method is a well-known numerical method for solving those types of systems of equations against multiple input vectors. It is also possible to preserve numerical stability by implementing some pivot strategy. In this post we will consider performance and numerical stability issues and try to cope with them by using the PA=LUPA = LUPA=LU factorization.\n\nIdea behind the algorithm\n\nSuppose we have a matrix AAA and some set of vectors {b1,…,bn}\\{b_1, \\dots, b_n\\}{b1​,…,bn​} and we need to solve Ax=biAx = b_iAx=bi​ for all 1≤i≤n1 \\le i \\le n1≤i≤n. Of course, we can just run the Gaussian elimination algorithm for each vector bib_ibi​ and compute the upper-triangular form of the matrix AAA and then backward-substitute the vector bib_ibi​. However, if the set of vectors is large or the matrix itself is big, this is inefficient because we have to convert the matrix to upper-triangular form nnn times.\n\nWhile doing Gaussian elimination, all row operations are applied both to the matrix and the vector. But when we choose what operation should be applied in some step, we never consider the entries of the vector. The key idea behind the PA=LUPA = LUPA=LU factorization algorithm is to compute the upper-triangular form of the matrix once and save all steps that have been performed so that we can perform the same steps on the current vector bib_ibi​ without recomputing the upper-triangular form of AAA for backward substitution. All Gaussian elimination steps can be encoded in a matrix L−1L^{-1}L−1, such that if we multiply it by some vector bib_ibi​, L−1L^{-1}L−1 will apply all the operations performed while computing the upper-triangular form of AAA to bib_ibi​. After that we can just do backward substitution to find xxx.\n\nDuring Gaussian elimination it is possible that we are forced to swap some rows, for example if the pivot on the diagonal is zero. As we will see, it is also a good idea to swap rows not only when we are forced to do so. Anyway, in order for the algorithm to work with any matrix AAA, we need to store the permutations of the rows of AAA in some way. For simplicity, we will store the permutation in a permutation matrix PPP that essentially just swaps rows.\n\nSo, formally speaking, we are interested in finding such matrices PPP, LLL and UUU, such that PA=LUPA = LUPA=LU where PPP is the permutation matrix, LLL is a lower-triangular matrix and UUU is an upper triangular matrix. We need LLL and UUU to be triangular matrices in order to be able to quickly solve linear systems of equations with them.\n\nForward and backward substitution\n\nAs already mentioned above, we need to implement forward and backward substitution in order to proceed with the PA=LUPA = LUPA=LU factorization algorithm:\n\n# Solves a linear system of equations Lx = b where the L matrix is lower-triangular\ndef forward_substitution(L, b):\n    \n    m = len(b)\n    x = np.empty(m)\n    \n    for v in range(m):\n        if L[v][v] == 0:\n            # the value on the diagonal is zero\n            x[v] = 0\n            # the current variable's value is irrelevant\n            continue\n        # calculate v-th variable value\n        value = b[v]\n        # subtract linear combination of the already known variables\n        # in the bottom left corner of the matrix\n        for i in range(v):\n            value -= L[v][i] * x[i]\n        # divide by the coefficient by the v-th variable to get it's value\n        value /= L[v][v]\n        # store the value in the resulting vector\n        x[v] = value\n        \n    return x\n\n\nAnalogously, we can implement backward substitution:\n\n# Solves a linear system of equations Ux = b where the U matrix is upper-triangular\ndef backward_substituion(U, b):\n\n    m = len(b)\n    x = np.empty(m)\n    \n    for v in range(m - 1, -1, -1):\n        if U[v][v] == 0:\n            # the value on the diagonal is zero\n            x[v] = 0\n            continue\n        # calculate v-th variable value\n        value = b[v]\n        # subtract linear combination of the already known variables\n        # in the top right corner of the matrix\n        for i in range(v + 1, m, 1):\n            value -= U[v][i] * x[i]\n        # divide by the coefficient before the i-th variable to get it's value\n        value /= U[v][v]\n        # store the value in the resulting vector\n        x[v] = value\n        \n    return x\n\n\nSolutions for non-homogeneous systems of equations are affine subspaces. If the input matrix is full-rank, i.e. doesn’t reduce the dimension of the vector space during a linear transformation, then there will be only one solution vector and the algorithms will calculate it correctly. For the sake of simplicity these algorithms just set the corresponding variables to zero if there is a zero on the diagonal, so in case the matrix is not full-rank, the algorithms will return only one vector from a set of vectors. In order to get the whole solution affine subspace it is necessary to give back a parametrized set of vectors.\n\nPA=LU factorization\n\nThe factorization algorithm consists of the following steps:\n\n\n  Initialize UUU with the initial matrix AAA and let PPP and LLL be the identity matrices. We will transform UUU to upper-triangular form.\n  Find some non-zero pivot element in the current column of the matrix.\n  If the found pivot element is not on the diagonal, swap the corresponding rows in the UUU and PPP matrices. Also, apply the swap to the part of the LLL matrix under the main diagonal.\n  Subtract the row with the pivot on the diagonal from all rows underneath so that all element under the pivot element become zero. After U[i][j]U[i][j]U[i][j] has been eliminated by subtracting α⋅U[j]\\alpha \\cdot U[j]α⋅U[j] from row iii, set the coefficient L[i][j]:=αL[i][j] := \\alphaL[i][j]:=α such that, in other words, α=U[i][j]U[j][j]\\alpha = \\frac{U[i][j]}{U[j][j]}α=U[j][j]U[i][j]​.\n  Continue from step 2 with the next column.\n\n\nThe idea behind the adjustment of the LLL matrix in step 3 is to add α⋅Pivot row\\alpha \\cdot \\text{Pivot row}α⋅Pivot row to the row to be eliminated and thus reverse this step. We only need to calculate the elements under the diagonal of the LLL matrix because the row to be eliminated is always under the pivot row.\n\nExample\n\nConsider the following matrix:\n\nA:=(−116−4−8621623)A := \\left(\n\\begin{array}{ccc}\n-1  &amp; 1 &amp; 6 \\\\\n-4 &amp; -8 &amp; 6 \\\\\n2 &amp; 16 &amp; 23\n\\end{array}\n\\right)A:=⎝⎜⎛​−1−42​1−816​6623​⎠⎟⎞​\n\nWe can start computing the PA=LUPA = LUPA=LU factorization by writing the initial decomposition as follows:\n\n(100010001)⋅A=(100010001)⋅(−116−4−8621623)\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-1  &amp; 1 &amp; 6 \\\\\n-4 &amp; -8 &amp; 6 \\\\\n2 &amp; 16 &amp; 23\n\\end{array}\n\\right)⎝⎜⎛​100​010​001​⎠⎟⎞​⋅A=⎝⎜⎛​100​010​001​⎠⎟⎞​⋅⎝⎜⎛​−1−42​1−816​6623​⎠⎟⎞​\n\nFor the first column, let’s choose the pivot −4-4−4 in the second row. It is not on the diagonal, so we must swap the first two rows:\n\n(010100001)⋅A=(100010001)⋅(−4−86−11621623)\\left(\n\\begin{array}{ccc}\n    0 &amp; 1 &amp; 0 \\\\\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-4 &amp; -8 &amp; 6 \\\\\n-1  &amp; 1 &amp; 6 \\\\\n2 &amp; 16 &amp; 23\n\\end{array}\n\\right)⎝⎜⎛​010​100​001​⎠⎟⎞​⋅A=⎝⎜⎛​100​010​001​⎠⎟⎞​⋅⎝⎜⎛​−4−12​−8116​6623​⎠⎟⎞​\n\nNow the pivot is on the diagonal and it is time to subtract −1−4=14\\frac{-1}{-4} = \\frac{1}{4}−4−1​=41​ of the first row from the second row and 2−4=−12\\frac{2}{-4} = -\\frac{1}{2}−42​=−21​ of the first row from the third row. We write these coefficients to the first column of the LLL matrix, accordingly:\n\n(010100001)⋅A=(1001410−1201)⋅(−4−86034.501226)\\left(\n\\begin{array}{ccc}\n    0 &amp; 1 &amp; 0 \\\\\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    \\frac{1}{4} &amp; 1 &amp; 0 \\\\\n    -\\frac{1}{2} &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-4 &amp; -8 &amp; 6 \\\\\n0  &amp; 3 &amp; 4.5 \\\\\n0 &amp; 12 &amp; 26\n\\end{array}\n\\right)⎝⎜⎛​010​100​001​⎠⎟⎞​⋅A=⎝⎜⎛​141​−21​​010​001​⎠⎟⎞​⋅⎝⎜⎛​−400​−8312​64.526​⎠⎟⎞​\n\nNow we proceed with the second column. Assume we choose 121212 as a pivot. 121212 is not on the diagonal, so we must swap the second row with the third one. This time we also need to swap the corresponding rows in the LLL matrix:\n\n(010001100)⋅A=(100−12101401)⋅(−4−8601226034.5)\\left(\n\\begin{array}{ccc}\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1 \\\\\n    1 &amp; 0 &amp; 0\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    -\\frac{1}{2} &amp; 1 &amp; 0 \\\\\n    \\frac{1}{4} &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-4 &amp; -8 &amp; 6 \\\\\n0 &amp; 12 &amp; 26 \\\\\n0  &amp; 3 &amp; 4.5\n\\end{array}\n\\right)⎝⎜⎛​001​100​010​⎠⎟⎞​⋅A=⎝⎜⎛​1−21​41​​010​001​⎠⎟⎞​⋅⎝⎜⎛​−400​−8123​6264.5​⎠⎟⎞​\n\nWe are now ready to eliminate 333 from the third row by subtracting 312=14\\frac{3}{12} = \\frac{1}{4}123​=41​ of the second row from it:\n\n(010001100)⋅A=(100−121014141)⋅(−4−860122600−2)\\left(\n\\begin{array}{ccc}\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1 \\\\\n    1 &amp; 0 &amp; 0\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    -\\frac{1}{2} &amp; 1 &amp; 0 \\\\\n    \\frac{1}{4} &amp; \\frac{1}{4} &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-4 &amp; -8 &amp; 6 \\\\\n0 &amp; 12 &amp; 26 \\\\\n0  &amp; 0 &amp; -2\n\\end{array}\n\\right)⎝⎜⎛​001​100​010​⎠⎟⎞​⋅A=⎝⎜⎛​1−21​41​​0141​​001​⎠⎟⎞​⋅⎝⎜⎛​−400​−8120​626−2​⎠⎟⎞​\n\nThis is the PA=LUPA = LUPA=LU decomposition of AAA.\n\nNon-stable implementation\n\nWe can implement the decomposition algorithm described above in Python the following way:\n\n# Compute the PA = LU decomposition of the matrix A\ndef plu(A):\n    m = len(A)\n    \n    P = np.identity(m) # create an identity matrix of size m\n    L = np.identity(m)\n    \n    for x in range(m):\n        \n        pivotRow = x\n        \n        if A[pivotRow][x] == 0:\n            # search for a non-zero pivot\n            for y in range(x + 1, m, 1):\n                if A[y][x] != 0:\n                    pivotRow = y\n                    break\n                            \n        if A[pivotRow][x] == 0:\n            # we didn't find any row with a non-zero leading coefficient\n            # that means that the matrix has all zeroes in this column\n            # so we don't need to search for pivots after all for the current column x\n            continue\n            \n        # did we just use a pivot that is not on the diagonal?\n        if pivotRow != x:\n            # so we need to swap the part of the pivot row after x including x\n            # with the same right part of the x row where the pivot was expected\n            for i in range(x, m, 1):\n                # swap the two values columnwise\n                (A[x][i], A[pivotRow][i]) = (A[pivotRow][i], A[x][i])\n            \n            # we must save the fact that we did this swap in the permutation matrix\n            # swap the pivot row with row x\n            P[[x, pivotRow]] = P[[pivotRow, x]]\n            \n            # we also need to swap the rows in the L matrix\n            # however, the relevant part of the L matrix is only the bottom-left corner\n            # and before x\n            for i in range(x):\n                (L[x][i], L[pivotRow][i]) = (L[pivotRow][i], L[x][i])\n            \n        # now the pivot row is x\n        # search for rows where the leading coefficient must be eliminated\n        for y in range(x + 1, m, 1):\n            currentValue = A[y][x]\n            if currentValue == 0:\n                # variable already eliminated, nothing to do\n                continue\n            \n            pivot = A[x][x]\n            assert pivot != 0 # just in case, we already made sure the pivot is not zero\n            \n            pivotFactor = currentValue / pivot\n            \n            # subtract the pivot row from the current row\n            A[y][x] = 0\n            for i in range(x + 1, m, 1):\n                A[y][i] -= pivotFactor * A[x][i]\n            \n            # write the pivot factor to the L matrix\n            L[y][x] = pivotFactor\n        \n        # the pivot must anyway be at the correct position if we found at least one\n        # non-zero leading coefficient in the current column x\n        assert A[x][x] != 0\n        for y in range(x + 1, m, 1):\n            assert A[y][x] == 0\n    \n    return (P, L, A)\n\n\nWe are now interested in solving Ax=bAx = bAx=b with the computed decomposition of AAA into PA=LUPA = LUPA=LU. It follows that LUx=PbLUx = PbLUx=Pb:\n\nPA=LU⇒PAx=LUx⇒LUx=PAx⇒LUx=Pb\\begin{aligned}\nPA = LU &amp;\\Rightarrow PAx = LUx \\\\\n&amp;\\Rightarrow LUx = PAx \\\\\n&amp;\\Rightarrow LUx = Pb\n\\end{aligned}PA=LU​⇒PAx=LUx⇒LUx=PAx⇒LUx=Pb​\n\nWe can reorder components of the vector bbb by multiplying it with PPP (let z:=Pbz := Pbz:=Pb) and then solve Ly=zLy = zLy=z for yyy using forward substitution (as LLL is lower-triangular). Intuitively, y:=Uxy := Uxy:=Ux is the vector with all the Gaussian elimination steps applied to it. After that, we can compute xxx by solving Ux=yUx = yUx=y using backward substitution (as UUU is upper-triangular).\n\nWe can implement these steps as follows:\n\ndef plu_solve(A, b):\n    (P, L, U) = plu(A)\n    b = np.matmul(P, b) # multiply matrix P with vector b\n    y = forward_substitution(L, b)\n    x = backward_substituion(U, y)\n    return x\n\n\nHowever, there it still a problem with the above implementation of plu. We can illustrate it by approximating the function f(x)=sin⁡(12⋅x)x2+1f(x) = \\frac{\\sin(12 \\cdot x)}{x^2 + 1}f(x)=x2+1sin(12⋅x)​ on the interval x∈[−1,1]x \\in [-1, 1]x∈[−1,1] with the PA=LUPA = LUPA=LU decomposition applied to a Vandermonde matrix:\n\n\n\nAs you can see, the approximation isn’t entirely right. The low-degree terms of the polynomial approximating f(x)f(x)f(x) are getting lost due to rounding errors in floating point arithmetic. These low-degree terms are exactly the ones that are responsible for approximating the right part of the graph and that’s why we see rounding errors there.\n\nNumerical stability\n\nThe issue that causes such precision problems lies in choosing the wrong pivot element. For example, consider the following matrix\n\nA:=(ε111)A := \\left(\n\\begin{array}{cc}\n\\varepsilon &amp; 1 \\\\\n1 &amp; 1\n\\end{array}\n\\right)A:=(ε1​11​)\n\nwhere ε&gt;0\\varepsilon &gt; 0ε&gt;0 is some very small number.\n\nIf we choose ε\\varepsilonε as a pivot, then we need to subtract the second row multiplied with 1ε\\frac{1}{\\varepsilon}ε1​ to perform an elimination step. As ε\\varepsilonε is very small, 1ε\\frac{1}{\\varepsilon}ε1​ is very big. So by choosing such a pivot, we not only preserve ε\\varepsilonε in the matrix but also add a very large number to it, like 1−1ε1 - \\frac{1}{\\varepsilon}1−ε1​ in this case.\n\nThere are many advanced strategies to cope with this problem, but one of the most easy and effective ways to improve numerical stability is to just choose the element with the greatest absolute value as the pivot.\n\nThus, we can improve the implementation by adjusting the code responsible for choosing the pivot:\n\ndef palu(A):\n    m = len(A)\n    \n    P = np.identity(m) # create an identity matrix of size m\n    L = np.identity(m)\n    \n    for x in range(m):\n        \n        pivotRow = x\n        \n        # search for the best pivot\n        for y in range(x + 1, m, 1):\n            if abs(A[y][x]) &gt; abs(A[pivotRow][x]):\n                pivotRow = y\n                            \n        if A[pivotRow][x] == 0:\n            # we didn't find any row with a non-zero leading coefficient\n            # that means that the matrix has all zeroes in this column\n            # so we don't need to search for pivots after all for the current column x\n            continue\n            \n        # did we just use a pivot that is not on the diagonal?\n        if pivotRow != x:\n            # so we need to swap the part of the pivot row after x including x\n            # with the same right part of the x row where the pivot was expected\n            for i in range(x, m, 1):\n                # swap the two values columnwise\n                (A[x][i], A[pivotRow][i]) = (A[pivotRow][i], A[x][i])\n            \n            # we must save the fact that we did this swap in the permutation matrix\n            # swap the pivot row with row x\n            P[[x, pivotRow]] = P[[pivotRow, x]]\n            \n            # we also need to swap the rows in the L matrix\n            # however, the relevant part of the L matrix is only the bottom-left corner\n            # and before x\n            for i in range(x):\n                (L[x][i], L[pivotRow][i]) = (L[pivotRow][i], L[x][i])\n            \n        # now the pivot row is x\n        # search for rows where the leading coefficient must be eliminated\n        for y in range(x + 1, m, 1):\n            currentValue = A[y][x]\n            if currentValue == 0:\n                # variable already eliminated, nothing to do\n                continue\n            \n            pivot = A[x][x]\n            assert pivot != 0 # just in case, we already made sure the pivot is not zero\n            \n            pivotFactor = currentValue / pivot\n            \n            # subtract the pivot row from the current row\n            A[y][x] = 0\n            for i in range(x + 1, m, 1):\n                A[y][i] -= pivotFactor * A[x][i]\n            \n            # write the pivot factor to the L matrix\n            L[y][x] = pivotFactor\n        \n        # the pivot must anyway be at the correct position if we found at least one\n        # non-zero leading coefficient in the current column x\n        assert A[x][x] != 0\n        for y in range(x + 1, m, 1):\n            assert A[y][x] == 0\n    \n    return (P, L, A)\n\n\nWith this implementation we get a much better approximation of f(x)f(x)f(x):\n\n\n\nUsing only one matrix to store the decomposition\n\nAs LLL is a lower-triangular matrix and UUU is upper-triangular, we can actually create only one matrix (apart from PPP) to store the decomposition. It also allows us to simplify the way we do swap operations:\n\ndef forward_substitution(L, b):\n    m = len(b)\n    x = np.empty(m)\n    \n    for v in range(m):\n        # calculate v-th variable value\n        value = b[v]\n        # subtract linear combination of the already known variables\n        # in the bottom left corner of the matrix\n        for i in range(v):\n            value -= L[v][i] * x[i]\n        # no need to divide by L[v][v] as it is implicitly 1, by construction\n        # (this 1 is not stored in the matrix)\n        # store the value in the resulting vector\n        x[v] = value\n        \n    return x\n\ndef plu(A):\n    m = len(A)\n    \n    P = np.identity(m)\n    \n    for x in range(m):\n        pivotRow = x\n        \n        # search for the best pivot\n        for y in range(x + 1, m, 1):\n            if abs(A[y][x]) &gt; abs(A[pivotRow][x]):\n                pivotRow = y\n                    \n        if A[pivotRow][x] == 0:\n            # we didn't find any row with a non-zero leading coefficient\n            # that means that the matrix has all zeroes in this column\n            # so we don't need to search for pivots after all for the current column x\n            continue\n            \n        # did we just use a pivot that is not on the diagonal?\n        if pivotRow != x:\n            # swap the pivot row with the current row in both A and L matrices\n            A[[x, pivotRow]] = A[[pivotRow, x]]\n            \n            # we must save the fact that we did this swap in the permutation matrix\n            # swap the pivot row with row x\n            P[[x, pivotRow]] = P[[pivotRow, x]]\n                        \n        # now the pivot row is x\n        # search for rows where the leading coefficient must be eliminated\n        for y in range(x + 1, m, 1):\n            currentValue = A[y][x]\n            if currentValue == 0:\n                # variable already eliminated, nothing to do\n                continue\n            \n            pivot = A[x][x]\n            assert pivot != 0 # just in case, we already made sure the pivot is not zero\n            \n            pivotFactor = currentValue / pivot\n            \n            # subtract the pivot row from the current row\n            for i in range(x + 1, m, 1):\n                A[y][i] -= pivotFactor * A[x][i]\n            \n            # write the pivot factor to the L matrix\n            A[y][x] = pivotFactor\n    \n    return (P, A)\n\ndef plu_solve(P, LU, b):\n    b = np.matmul(P, b)\n    y = forward_substitution(LU, b)\n    x = backward_substituion(LU, y)\n    return x\n\n\nIt is possible to further optimize the algorithms by removing the permutation matrix PPP and replacing it with a simple array that permutes it’s indices (or some other permutation datastructure).\n\nAs already mentioned in the beginning of the post, the key advantage of the PA=LUPA = LUPA=LU decomposition is that it allows us to solve linear systems of equations for many input vectors from B:={b1,…,bn}B := \\{b_1, \\dots, b_n\\}B:={b1​,…,bn​} by doing Gaussian elimination only once\n\n(P, LU) = plu(A)\nfor b in B:\n    solution = plu_solve(P, LU, b)\n\n\ninstead of doing full Gaussian elimination nnn times.\n\nLet mmm be the size of the vector and AAA be an m×mm \\times mm×m matrix. Then the complexity of computing the PA=LUPA = LUPA=LU factorization is O(m3)O(m^3)O(m3). If we optimize the permutation matrix so that permuting elements takes time in O(m2)O(m^2)O(m2), then the solving algorithm’s complexity is O(m2)O(m^2)O(m2).\n\nTherefore, the overall complexity of the algorithm that takes AAA and a set of nnn vectors BBB and solves Ax=bAx = bAx=b for every b∈Bb \\in Bb∈B (as shown in the code snippet above) is O(m3+n⋅m2)O(m^3 + n \\cdot m^2)O(m3+n⋅m2), which is better than O(n⋅m3)O(n \\cdot m^3)O(n⋅m3) if we apply Gaussian elimination nnn times.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/pa-lu-factorization/"
    },{
      "image": "../../assets/img/blog/linear-transformation-3d-tmb.jpg",
      "title": "Visualizing 3D linear transformations and Gaussian elimination with Python and Manim",
      "date": "2020-12-15 00:00:00 +0100",
      
      "content": "Matrices are omnipresent in linear algebra. Column of a matrix describe where the corresponding basis vectors land relative to the initial basis. All transformed vectors are linear combinations of transformed basis vectors which are the columns of the matrix, this is also called linearity. Algorithms that operate on matrices essentially just alter the way vectors get transformed, preserving some properties. Unfortunately, many algorithms are typically presented to students in a numerical fashion, without describing the whole graphical meaning. In this post we will first visualize simple linear transformations and then we will visualize Gaussian Elimination (with row swaps) steps as a sequence of linear transformations. To do this, we will use Python together with a popular Open-Source library manim.\n\nVisualizing 3D transformations\n\nIn manim, there is a special ApplyMatrix animation that allows us to natively apply a matrix to every 3D vertex of the object.\n\nfrom manimlib.imports import *\n\nclass LinearTransformation3D(ThreeDScene):\n\n    CONFIG = {\n        \"x_axis_label\": \"$x$\",\n        \"y_axis_label\": \"$y$\",\n        \"basis_i_color\": GREEN,\n        \"basis_j_color\": RED,\n        \"basis_k_color\": GOLD\n    }\n\n    def create_matrix(self, np_matrix):\n\n        m = Matrix(np_matrix)\n\n        m.scale(0.5)\n        m.set_column_colors(self.basis_i_color, self.basis_j_color, self.basis_k_color)\n\n        m.to_corner(UP + LEFT)\n\n        return m\n\n    def construct(self):\n\n        M = np.array([\n            [2, 2, -1],\n            [-2, 1, 2],\n            [3, 1, -0]\n        ])\n\n        axes = ThreeDAxes()\n        axes.set_color(GRAY)\n        axes.add(axes.get_axis_labels())\n\n        self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES)\n\n        # basis vectors i,j,k\n        basis_vector_helper = TextMobject(\"$i$\", \",\", \"$j$\", \",\", \"$k$\")\n        basis_vector_helper[0].set_color(self.basis_i_color)\n        basis_vector_helper[2].set_color(self.basis_j_color)\n        basis_vector_helper[4].set_color(self.basis_k_color)\n\n        basis_vector_helper.to_corner(UP + RIGHT)\n\n        self.add_fixed_in_frame_mobjects(basis_vector_helper)\n\n        # matrix\n        matrix = self.create_matrix(M)\n\n        self.add_fixed_in_frame_mobjects(matrix)\n\n        # axes &amp; camera\n        self.add(axes)\n\n        self.begin_ambient_camera_rotation(rate=0.2)\n\n        cube = Cube(side_length=1, fill_color=BLUE, stroke_width=2, fill_opacity=0.1)\n        cube.set_stroke(BLUE_E)\n\n        i_vec = Vector(np.array([1, 0, 0]), color=self.basis_i_color)\n        j_vec = Vector(np.array([0, 1, 0]), color=self.basis_j_color)\n        k_vec = Vector(np.array([0, 0, 1]), color=self.basis_k_color)\n\n        i_vec_new = Vector(M @ np.array([1, 0, 0]), color=self.basis_i_color)\n        j_vec_new = Vector(M @ np.array([0, 1, 0]), color=self.basis_j_color)\n        k_vec_new = Vector(M @ np.array([0, 0, 1]), color=self.basis_k_color)\n\n        self.play(\n            ShowCreation(cube),\n            GrowArrow(i_vec),\n            GrowArrow(j_vec),\n            GrowArrow(k_vec),\n            Write(basis_vector_helper)\n        )\n\n        self.wait()\n\n        matrix_anim = ApplyMatrix(M, cube)\n\n        self.play(\n            matrix_anim,\n            Transform(i_vec, i_vec_new, rate_func=matrix_anim.get_rate_func(),\n                      run_time=matrix_anim.get_run_time()),\n            Transform(j_vec, j_vec_new, rate_func=matrix_anim.get_rate_func(),\n                      run_time=matrix_anim.get_run_time()),\n            Transform(k_vec, k_vec_new, rate_func=matrix_anim.get_rate_func(),\n                      run_time=matrix_anim.get_run_time())\n        )\n\n        self.wait()\n\n        self.wait(7)\n\n\nWith this implementation we can visualize, for example, the following matrix presented in the gif:\n\n\n\nWe can also visualize what it means for 2 columns to be linear dependent:\n\n\n\nVisualizing Gaussian elimination\n\nIn the previous post we implemented an advanced version of the Gaussian elimination algorithm with pivoting (the PA=LUPA = LUPA=LU decomposition). We can analogously implement the pure Gaussian Elimination algorithm with pivoting the following way:\n\ndef gauss(a):\n    m = a.shape[0]\n\n    for x in range(m):\n        pivotRow = x\n\n        # search for the best pivot\n        for y in range(x + 1, m, 1):\n            if abs(a[y][x]) &gt; abs(a[pivotRow][x]):\n                pivotRow = y\n\n        if a[pivotRow][x] == 0:\n            # we didn't find any row with a non-zero leading coefficient\n            # that means that the matrix has all zeroes in this column\n            # so we don't need to search for pivots after all for the current column x\n            continue\n\n        # did we just use a pivot that is not on the diagonal?\n        if pivotRow != x:\n            # swap the pivot row with the current row in both A and L matrices\n            a[[x, pivotRow]] = a[[pivotRow, x]]\n            yield a\n\n        # now the pivot row is x\n        # search for rows where the leading coefficient must be eliminated\n        for y in range(x + 1, m, 1):\n            currentValue = a[y][x]\n            if currentValue == 0:\n                # variable already eliminated, nothing to do\n                continue\n\n            pivot = a[x][x]\n            assert pivot != 0  # just in case, we already made sure the pivot is not zero\n\n            pivotFactor = currentValue / pivot\n\n            # subtract the pivot row from the current row\n\n            a[y][x] = 0\n\n            for i in range(x + 1, m, 1):\n                a[y][i] -= pivotFactor * a[x][i]\n\n            yield a\n\n\nNotice that after every matrix operation we want to visualize I’ve added a yield statement. Generators in Python are perfect for visualizing algorithms, because they allow us to save the current state, yield the value and then continue the execution when the caller handled the received value. In this case, we yield every version of the matrix as the algorithm runs.\n\nNow we can adjust and extend the code used to render linear transformations to do this for every step of the gaussian elimination:\n\nclass Gauss3D(ThreeDScene):\n\n    CONFIG = {\n        \"x_axis_label\": \"$x$\",\n        \"y_axis_label\": \"$y$\",\n        \"basis_i_color\": GREEN,\n        \"basis_j_color\": RED,\n        \"basis_k_color\": GOLD\n    }\n\n    def create_matrix(self, np_matrix):\n\n        m = Matrix(np_matrix)\n\n        m.scale(0.5)\n        m.set_column_colors(self.basis_i_color, self.basis_j_color, self.basis_k_color)\n\n        m.to_corner(UP + LEFT)\n\n        return m\n\n    def construct(self):\n\n        M = np.array([\n            [-1.0, 1.0, -2.0],\n            [-4.0, -2.0, 1.0],\n            [-2.0, 2.0, 3.0]\n        ])\n\n        # axes\n        axes = ThreeDAxes()\n        axes.set_color(GRAY)\n        axes.add(axes.get_axis_labels())\n\n        self.set_camera_orientation(phi=55 * DEGREES, theta=-45 * DEGREES)\n\n        # basis vectors i,j,k\n        basis_vector_helper = TextMobject(\"$i$\", \",\", \"$j$\", \",\", \"$k$\")\n        basis_vector_helper[0].set_color(self.basis_i_color)\n        basis_vector_helper[2].set_color(self.basis_j_color)\n        basis_vector_helper[4].set_color(self.basis_k_color)\n\n        basis_vector_helper.to_corner(UP + RIGHT)\n\n        self.add_fixed_in_frame_mobjects(basis_vector_helper)\n\n        # matrix\n        matrix = self.create_matrix(M)\n\n        self.add_fixed_in_frame_mobjects(matrix)\n\n        # axes &amp; camera\n        self.add(axes)\n\n        self.begin_ambient_camera_rotation(rate=0.15)\n\n        cube = Cube(side_length=1, fill_color=BLUE, stroke_width=2, fill_opacity=0.1)\n        cube.set_stroke(BLUE_E)  # cube.set_stroke(TEAL_E)\n\n        i_vec = Vector(np.array([1, 0, 0]), color=self.basis_i_color)\n        j_vec = Vector(np.array([0, 1, 0]), color=self.basis_j_color)\n        k_vec = Vector(np.array([0, 0, 1]), color=self.basis_k_color)\n\n        i_vec_new = Vector(M @ np.array([1, 0, 0]), color=self.basis_i_color)\n        j_vec_new = Vector(M @ np.array([0, 1, 0]), color=self.basis_j_color)\n        k_vec_new = Vector(M @ np.array([0, 0, 1]), color=self.basis_k_color)\n\n        self.play(\n            ShowCreation(cube),\n            GrowArrow(i_vec),\n            GrowArrow(j_vec),\n            GrowArrow(k_vec),\n            Write(basis_vector_helper)\n        )\n\n        self.wait()\n\n        matrix_anim = ApplyMatrix(M, cube)\n\n        self.play(\n            matrix_anim,\n            ReplacementTransform(i_vec, i_vec_new, rate_func=matrix_anim.get_rate_func(),\n                                 run_time=matrix_anim.get_run_time()),\n            ReplacementTransform(j_vec, j_vec_new, rate_func=matrix_anim.get_rate_func(),\n                                 run_time=matrix_anim.get_run_time()),\n            ReplacementTransform(k_vec, k_vec_new, rate_func=matrix_anim.get_rate_func(),\n                                 run_time=matrix_anim.get_run_time())\n        )\n\n        self.wait()\n\n        i_vec, j_vec, k_vec = i_vec_new, j_vec_new, k_vec_new\n\n        self.wait(2)\n\n        for a in gauss(M):\n\n            a_rounded = np.round(a.copy(), 2)\n\n            self.remove(matrix)\n\n            matrix = self.create_matrix(a_rounded)\n\n            self.add_fixed_in_frame_mobjects(matrix)\n\n            # transformed cube\n            new_cube = Cube(side_length=1, fill_color=BLUE, stroke_width=2, fill_opacity=0.1)\n            new_cube.set_stroke(BLUE_E)\n\n            new_cube.apply_matrix(a)\n\n            # vectors\n            i_vec_new = Vector(a @ np.array([1, 0, 0]), color=self.basis_i_color)\n            j_vec_new = Vector(a @ np.array([0, 1, 0]), color=self.basis_j_color)\n            k_vec_new = Vector(a @ np.array([0, 0, 1]), color=self.basis_k_color)\n\n            # prepare and run animation\n            cube_anim = ReplacementTransform(cube, new_cube)\n\n            self.play(\n                cube_anim,\n                ReplacementTransform(i_vec, i_vec_new, rate_func=cube_anim.get_rate_func(),\n                                     run_time=cube_anim.get_run_time()),\n                ReplacementTransform(j_vec, j_vec_new, rate_func=cube_anim.get_rate_func(),\n                                     run_time=cube_anim.get_run_time()),\n                ReplacementTransform(k_vec, k_vec_new, rate_func=cube_anim.get_rate_func(),\n                                     run_time=cube_anim.get_run_time())\n            )\n\n            self.wait()\n\n            cube = new_cube\n            i_vec, j_vec, k_vec = i_vec_new, j_vec_new, k_vec_new\n\n            self.wait(1)\n\n        self.wait(1)\n\n\nWe can now see graphically how the column vectors get aligned as we bring the matrix to upper triangular form:\n\n\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/linear-transformations-3d-manim/"
    },{
      "image": "../../assets/img/blog/kmp-tmb.jpg",
      "title": "Knuth-Morris-Pratt (KMP) algorithm explained",
      "date": "2020-12-20 00:00:00 +0100",
      
      "content": "Almost every program has to somehow process and transform strings. Very often we want to identify some fixed pattern (substring) in the string, or, formally speaking, find an occurrence of β:=b0b1…bn−1\\beta := b_0b_1 \\dots b_{n-1}β:=b0​b1​…bn−1​ in α:=a0a1…am−1≠ε\\alpha := a_0a_1 \\dots a_{m-1} \\neq \\varepsilonα:=a0​a1​…am−1​​=ε. A naive algorithm would just traverse α\\alphaα and check if β\\betaβ starts at the current character of α\\alphaα. Unfortunately, this approach leads us to an algorithm with a complexity of O(n⋅m)O(n \\cdot m)O(n⋅m). In this post we will discuss a more efficient algorithm solving this problem - the Knuth-Morris-Pratt (KMP) algorithm.\n\nThe Knuth-Morris-Pratt algorithm\n\nObviously, the substring search algorithm has to somehow compare both strings character-after-character. Suppose we are scanning the string from left to right and we found b0b_0b0​. Then we can start comparing further characters of β\\betaβ with the next characters of α\\alphaα. If the remaining characters match, we’ve found an occurrence of β\\betaβ in α\\alphaα. So the key question is what the algorithm should do if there is a mismatch during this comparison process, as shown in this example:\n\n\n\nIf the characters we are currently comparing don’t match, then we need to somehow shift the green, “matching” area of β\\betaβ. This green area that we expand if the characters match and shift otherwise is often referred to as the window. If we found a mismatch, it is possible that some suffix of the window together with the current character form a prefix of β\\betaβ, so we must shift the window so that this prefix of β\\betaβ ends at the current position. In the example above, we must shift the window by 5 characters to the right so that the arrow denoting the current position points at the first character b of the window.\n\nOr, for example, in this situation\n\n\n\nwe should shift the window by 2 characters to the right and get:\n\n\n\nThe current elements (above the arrow) match so we can proceed by comparing further characters. By the way, it is important to shift only by 2 characters and not by 4, because otherwise it is possible that we skip too many characters and don’t detect a valid occurrence of β\\betaβ in α\\alphaα.\n\nTo generalize and formalize how we shift the window when the current characters don’t match, we will define the so-called failure function:\n\nLet f(j)f(j)f(j) be the length of the longest proper prefix of b0b1…bjb_0b_1 \\dots b_jb0​b1​…bj​, such that this prefix is also a suffix of b0b1…bjb_0b_1 \\dots b_jb0​b1​…bj​. In this context, proper means that it is not the string itself so the prefix is not trivial.\n\nThe idea behind the failure function is that when we find mismatching characters while comparing the window with α\\alphaα, we should shift the window such that the f(i)f(i)f(i)-th character of the window is at the position of the last character iii of the window (green area). In other words, the window should be shifted by w−f(i)w - f(i)w−f(i) characters to the right, where www is the size of the window. If, after shifting the window, there is still a mismatch, then it is possible that there is a shorter suffix that matches some prefix of b0…bib_0 \\dots b_ib0​…bi​ so we need to repeat the process again.\n\nWith this idea and assuming the failure function is computed and stored in the f array, we can implement the Knuth-Morris-Pratt algorithm the following way:\n\n# returns the index of the first occurrence of b in a\ndef kmp(a: str, b: str, f: list) -&gt; int:\n    \n    m = len(a)\n    assert m &gt;= 1\n    n = len(b)\n    p = 0\n    \n    for i in range(m):\n        \n        while p != 0 and a[i] != b[p]:\n            p = f[p - 1]\n        \n        if a[i] == b[p]:\n            p += 1\n\n        # invariant formally defined below\n        assert a[i + 1 - p:i + 1] == b[0:p]\n\n        if p == n:\n            return i + 1 - n\n    \n    return -1 # no match found\n\n\nProof of correctness: To formally prove the correctness of the above program, we will define the following invariant which is also verified with the assert statement in code: After every iteration of the for loop, before the last if statement, it holds that ai+1−p…ai=b0…bp−1a_{i+1-p} \\dots a_i = b_0 \\dots b_{p-1}ai+1−p​…ai​=b0​…bp−1​. Intuitively, it means that the window is correct:\n\n\n\nInduction base: We excluded the case α=ε\\alpha = \\varepsilonα=ε in the formal definition of the problem, so the loop will be entered. On the first iteration, the window is empty (p=0p = 0p=0) and the while loop will not be entered because we don’t need to shift the empty window. If the first characters match, then ppp will be incremented establishing the invariant. Otherwise, the window will remain empty.\n\nInduction step: By induction hypothesis, we assume that the invariant holds for some iii and ppp. If p=np = np=n, it means that ai+1−n…ai=b0…bn−1=βa_{i+1-n} \\dots a_i = b_0 \\dots b_{n-1} = \\betaai+1−n​…ai​=b0​…bn−1​=β, so we found an occurrence of β\\betaβ in α\\alphaα at the position i+1−ni + 1 - ni+1−n, and the algorithm terminates. If this is not the case and there is a further character in α\\alphaα, we enter a new iteration of the loop and shift the window by p−f(p−1)p - f(p - 1)p−f(p−1) characters on the right by assigning p to point to the f(p−1)f(p - 1)f(p−1) character of the string. Intuitively by doing this we shift the window so that the new window is the greatest possible proper prefix of the old window, that is also it’s suffix. For this reason it is guaranteed that the window remains valid and we only need to compare the current elements: If a[i] != b[i], then it means that the prefix we took is probably too large and we should search for a smaller one by repeating the window shifting step, until we either find a match or ppp becomes zero meaning that the window is empty. The body of the if statement after the loop expands the window in case the current characters match, establishing the invariant.\n\nIt is also important to note, that the window should be shifted by p−f(p−1)p - f(p - 1)p−f(p−1) characters to the right, because if we take some shorter prefix that is also a suffix of b0…bp−1b_0 \\dots b_{p - 1}b0​…bp−1​, it is possible that we don’t detect an occurrence of β\\betaβ in α\\alphaα. This completes the proof.\n\nIn the example above, the window gets shifted by p−f(p−1)=5−f(4)=5−3=2p - f(p - 1) = 5 - f(4) = 5 - 3 = 2p−f(p−1)=5−f(4)=5−3=2 characters to the right, iii gets incremented and the window gets expanded (by incrementing ppp) because a[i] == b[p]:\n\n\n\nComputing the failure function\n\nAn important part of the KMP algorithm is, of course, the computation of the failure function we defined above. We can come up with an efficient algorithm computing it by approaching the problem with dynamic programming. Suppose we want to compute f(n−1)f(n - 1)f(n−1) and we already computed f(i)f(i)f(i) for all 0≤i≤n−20 \\le i \\le n - 20≤i≤n−2. The value of f(n−2)f(n - 2)f(n−2) gives us the length of the longest suffix ending at bn−2b_{n-2}bn−2​ such that there is a corresponding prefix of this length (equal parts of the string are visualized with braces):\n\nβ=b0…bf(n−2)−1⏟f(n−2)bf(n−2)…bn−2−f(n−2)bn−1−f(n−2)…bn−2⏟f(n−2)bn−1\\beta = \\underbrace{b_0 \\dots b_{f(n-2)-1}}_{f(n-2)}\nb_{f(n-2)} \\dots b_{n-2-f(n-2)}\n\\underbrace{b_{n-1-f(n-2)} \\dots b_{n-2}}_{f(n-2)}\nb_{n-1}β=f(n−2)b0​…bf(n−2)−1​​​bf(n−2)​…bn−2−f(n−2)​f(n−2)bn−1−f(n−2)​…bn−2​​​bn−1​\n\nBy the way, is is possible that the parts of the formula marked with braces intersect, because there is no guarantee that f(n−2)−1&lt;n−1−f(n−2)⇔f(n−2)&lt;n2f(n-2) - 1 &lt; n - 1 - f(n-2) \\Leftrightarrow f(n-2) &lt; \\frac{n}{2}f(n−2)−1&lt;n−1−f(n−2)⇔f(n−2)&lt;2n​.\n\nIf the characters after the equal parts in braces are also the same, or, formally, if bf(n−2)=bn−1b_{f(n-2)} = b_{n-1}bf(n−2)​=bn−1​, then we can conclude that f(n−1)=f(n−2)+1f(n-1) = f(n-2) + 1f(n−1)=f(n−2)+1.\n\nIf bf(n−2)≠bn−1b_{f(n-2)} \\neq b_{n-1}bf(n−2)​​=bn−1​, then the longest prefix of β\\betaβ that is also it’s suffix must have length k&lt;f(n−2)k &lt; f(n-2)k&lt;f(n−2). Suppose we found such maximum k&lt;f(n−2)k &lt; f(n-2)k&lt;f(n−2) such that:\n\nb0…bk−1=bn−1−k…bn−2b0…bk−1⏞kbk…bf(n−2)−1⏟f(n−2)=bn−1−f(n−2)…bn−2−kbn−1−k…bn−2⏞k⏟f(n−2)\\begin{aligned}\nb_0 \\dots b_{k-1} &amp;= b_{n-1-k} \\dots b_{n-2} \\\\\n\\underbrace{\\overbrace{b_0 \\dots b_{k-1}}^{k} b_{k} \\dots b_{f(n-2)-1}}_{f(n-2)} &amp;=\n\\underbrace{b_{n-1-f(n-2)} \\dots b_{n-2-k} \\overbrace{b_{n-1-k} \\dots b_{n-2}}^{k}}_{f(n-2)}\n\\end{aligned}b0​…bk−1​f(n−2)b0​…bk−1​​k​bk​…bf(n−2)−1​​​​=bn−1−k​…bn−2​=f(n−2)bn−1−f(n−2)​…bn−2−k​bn−1−k​…bn−2​​k​​​​\n\nThen, by combining these equations, it follows that there must be a suffix of length kkk equal to the prefix of\n\nb0…bk−1bk…bf(n−2)−1b_0 \\dots b_{k-1} b_{k} \\dots b_{f(n-2)-1}b0​…bk−1​bk​…bf(n−2)−1​\n\nSo, the maximum possible suffix that is also a prefix has length k:=f(n−2)k := f(n-2)k:=f(n−2), if bk=bn−1b_k = b_{n-1}bk​=bn−1​. We can apply this important fact we derived multiple times - if bk≠bn−1b_k \\neq b_{n-1}bk​​=bn−1​, then the maximum suffix/prefix we are searching for has length l:=f(k−1)l := f(k - 1)l:=f(k−1), if bl=bn−1b_l = b_{n-1}bl​=bn−1​, and so on.\n\nThe base case for the dynamic programming approach is simple: f(0)=0f(0) = 0f(0)=0, because the only proper prefix of b0b_0b0​ is the empty string ε\\varepsilonε.\n\nWe are now ready to implement the computation of the failure function:\n\ndef compute_f(b):\n\n    n = len(b)\n    f = [0]\n\n    for i in range(1, n):\n        \n        k = f[i - 1]\n        \n        while k != 0 and b[i] != b[k]:\n            k = f[k - 1]\n\n        if b[i] == b[k]:\n            k += 1\n\n        f.append(k)  # f[i] = k\n\n    return f\n\n\nIn the case of a mismatch the while-loop searches for potential shorter suffix, that is also a prefix of β\\betaβ. If it was found, then the loop terminates with kkk representing the first character after the prefix. If the loop terminated because of k=0k = 0k=0, then it means that there is no shorter prefix that is also a suffix (it is empty), so the value of the failure function is either 0 or 1, depending on whether the current character is equal to the first one.\n\nOf course, we can run the functions above and check the result:\n\na = \"baabbbaabbaabbbabaabbbaabaabababba\"\nb = \"baababa\"\nprint(\"a:\", a)\nprint(\"b:\", b)\nf = compute_f(b)\nprint(\"Failure function:\")\nprint(\"i   :\", [i for i in range(len(b))])\nprint(\"f(i):\", f)  # [0, 0, 0, 1, 2, 1, 2]\nfound = kmp(a, b, f)\nprint(\"Result:\", found)  # 24\n\n\nFinding all matches\n\nThe Knuth-Morris-Pratt algorithm can be easily extended to detect all occurrences of β\\betaβ in α\\alphaα:\n\ndef kmp(a: str, b: str, f: list) -&gt; list:\n\n    m = len(a)\n    assert m &gt;= 1\n    n = len(b)\n    p = 0\n\n    matches = []\n\n    for i in range(m):\n        \n        while p != 0 and a[i] != b[p]:\n            p = f[p - 1]\n        \n        if a[i] == b[p]:\n            p += 1\n\n        if p == n:\n            matches.append(i + 1 - n)\n            p = f[p - 1]\n    \n    return matches\n\n\nIf the window has length nnn and we thus found an occurrence of β\\betaβ in α\\alphaα, all we need to do is to shift the window by n−f(n−1)n - f(n - 1)n−f(n−1) characters to the right. We need to do it in the body of the if p == n: statement, because the while loop expects that the window is a proper prefix of β\\betaβ and thus ppp is a valid index.\n\nComplexity analysis\n\nIt may seem that both the computation of the failure function as well as the matching itself have a complexity of Θ(n⋅m)\\Theta(n \\cdot m)Θ(n⋅m) because of the 2 nested loops in both algorithms. However, this is not the case. We can first show, that the failure function can be computed in linear time:\n\nFailure function computation complexity\n\nWe’ve already argued above that f(i)≤f(i−1)+1∀i∈{1,…,n−1}f(i) \\le f(i - 1) + 1 \\quad\\forall i \\in \\{1, \\dots, n - 1\\}f(i)≤f(i−1)+1∀i∈{1,…,n−1} and f(0)=0f(0) = 0f(0)=0. It follows, that f(i)≤if(i) \\le if(i)≤i also holds for all 0≤i&lt;n0 \\le i &lt; n0≤i&lt;n. Because of that, during a single iteration of the for loop, the while loop makes at most kkk iterations where kkk is the initial value of the k variable.\n\nWe will now prove that the body of of the while loop (in particular, the k = func[k - 1] statement) gets executed at most nnn times during the entire algorithm: the maximum value of kkk is n−1n - 1n−1, so if the while loop is entered, then it will make at most n−1n - 1n−1 iterations and because f(i)≤f(i−1)+1f(i) \\le f(i - 1) + 1f(i)≤f(i−1)+1, the value of kkk will decrease by at least the amount of iteration of the while loop.\n\nTherefore, the overall running time of the algorithm is in O(n)O(n)O(n).\n\nMatching algorithm complexity\n\nAssuming the failure function is computed and stored in an array so that the access time is constant, the complexity of the Knuth-Morris-Pratt algorithm is O(m)O(m)O(m). The proof of this fact is analogous to the previous proof.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/knuth-morris-pratt/"
    },{
      
      "title": "Computer Science",
      "date": "2020-12-21 23:22:58 +0100",
      "description": "This category contains all posts that have something to do with theoretical or practical computer science.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "featured_categories",
      "url": "/blog/cs/"
    },{
      "image": "/assets/img/projects/crproxy.jpg",
      "title": "Clash Royale Proxy",
      "date": "2018-07-01 00:00:00 +0200",
      "description": "CrProxy is a NodeJs implementation of a Clash Royale proxy server. It decrypts traffic between Supercell servers and the game.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/crproxy/"
    },{
      "image": "/assets/img/projects/zeropackerjs.svg",
      "title": "ZeroPackerJs",
      "date": "2018-11-20 00:00:00 +0100",
      "description": "ZeroPackerJs is serializing / deserializing library for javascript. Data is represented in binary format.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/zeropacker/"
    },{
      "image": "/assets/img/projects/apowbmodc.svg",
      "title": "ApowBmodC",
      "date": "2018-12-07 00:00:00 +0100",
      "description": "ApowBmodC is my small implementation of the efficient a ^ b mod c calculation algorithm.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/apowbmodc/"
    },{
      "image": "/assets/img/projects/sdlgrapher.jpg",
      "title": "SDL Grapher",
      "date": "2019-02-04 00:00:00 +0100",
      
      "content": "SdlGrapher allows you to plot graphs for mathematical functions with SDL 2.0 in C++.\n\nFeatures\n\n\n  Horizontal / vertical scrolling.\n  Scaling with mouse wheel.\n  No rendering if the math function returns NaN or Infinity.\n  Movable axises. Screen =&gt; Math, Math =&gt; Screen unit converters.\n  Automatically calculate scale and axis position based on interval of the math function.\n  Pixel perfect rendering.\n\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/sdlgrapher/"
    },{
      "image": "/assets/img/projects/bst.jpg",
      "title": "BST",
      "date": "2019-06-08 00:00:00 +0200",
      "description": "Optimized binary search tree implementation in C++.\n",
      "content": "Note: Binary search trees are not balanced by definition. This implementation does not guarantee O(log n) complexity when searching or deleting.\n\nFeatures\n\n  Pretty good performance compared to many other implementations on the internet.\n  No recursion except for debugging purposes.\n  Node-based on-delete balancing is supported. If you don’t need it, you can easily disable it.\n  Unique / non-unique element insertion.\n  2 deletion methods are supported:\n    \n      With payload copying (efficient, when it’s size is less than 2 * sizeof(void*)).\n      With pointer rearrangement.\n    \n  \n  Debug tree printing with indentation.\n  Lightweight, only one header file.\n\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/bst/"
    },{
      "image": "/assets/img/projects/chainhashmap.svg",
      "title": "ChainHashMap",
      "date": "2019-06-21 00:00:00 +0200",
      "description": "This is an implementation of a closed-addressed hash map in C++ without STL.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/chainhashmap/"
    },{
      "image": "/assets/img/projects/lzz.jpg",
      "title": "LZZ",
      "date": "2019-08-26 00:00:00 +0200",
      "description": "LZZ is a URL shortener that allows changing target URL’s after they have been created.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/lzz/"
    },{
      "image": "/assets/img/projects/cstring.jpg",
      "title": "CString",
      "date": "2019-08-27 00:00:00 +0200",
      "description": "Header-only, expandable and descriptor-caching string implementation in C99.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/cstring/"
    },{
      "image": "/assets/img/projects/zerorobo/gameplay01.png",
      "title": "ZeroRobo",
      "date": "2019-10-25 00:00:00 +0200",
      "description": "This is my RoboCode robot implemented in Java. It moves around special anchor points that allow it to avoid enemy bullets. Only 1vs1 mode is supported.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/zerorobo/"
    },{
      "image": "/assets/img/projects/vkantispam.jpg",
      "title": "VkAntiSpam",
      "date": "2019-11-01 00:00:00 +0100",
      "description": "Intelligent, integrated and self-learning antispam system for filtering spam in VK groups.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/vkantispam/"
    },{
      "image": "/assets/img/projects/chrem.jpg",
      "title": "chrem",
      "date": "2020-01-18 00:00:00 +0100",
      "description": "Algorithm to solve a linear system of congruences using the Chinese remainder theorem. Works also for non-coprime divisors.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/chrem/"
    },{
      "image": "/assets/img/projects/pollardrsacracker.jpg",
      "title": "PollardRsaCracker",
      "date": "2020-02-21 00:00:00 +0100",
      "description": "RSA cracking algorithm based on Pollard factorization.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/pollardrsacracker/"
    },{
      "image": "/assets/img/projects/knife.jpg",
      "title": "Knife",
      "date": "2020-03-31 00:00:00 +0200",
      "description": "Knife is a tool that reads input grammar in BNF format and converts it to a few Java classes that can parse the given grammar through a simple interface.\n",
      "content": "Knife doesn’t require any external libraries or dependencies. All generation is done ahead-of-time. After generating the parsing classes you can just copy them into your project.\n\nAlso, as other good parser generation tools, knife uses itself to read the input grammar.\n\nFeatures\n\n\n  No runtime dependencies, knife generates pure Java code that can easily be ported to other JVM-based languages.\n  Parsing is done using push-down automata without recursion.\n  Knife uses an explicit API for accepting the token stream. It allows you to easily use knife with any (including your own) lexer. You can pause and resume parsing at any point. Parsing multiple token streams simultaneously is also possible.\n  No complete parse-trees are being built during parsing. Reduction of the tree is done on-the-fly for performance. Optimized AST’s can be built during parsing with minimal overhead.\n  If your grammar is left-recursive without A =&gt;* A derivations (aka without cycles), knife will generate an equivalent grammar without left recursion for you.\n  Syntax error recovery using panic mode approach without any additional performance overhead.\n\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/knife/"
    },{
      "image": "/assets/img/projects/grammax.jpg",
      "title": "Grammax",
      "date": "2020-05-21 00:00:00 +0200",
      "description": "Grammax takes a grammar in BNF format as an input and converts it to a Java-class that recognizes the language generated by the grammar. Formally speaking, this tool creates a left-to-right, rightmost derivation (LR) parser for a given grammar. That means that grammax parses the given string by constructing a reversed rightmost derivation of it.\n",
      "content": "Grammax doesn’t require any external libraries or dependencies. All generation is done ahead-of-time. After generating the parsing classes you can just copy them into your project.\n\nAlso, as other good parser generation tools, grammax uses itself to read the input grammar.\n\nFeatures\n\n\n  No runtime dependencies, only pure Java code is generated.\n  Parsing is done using a push-down automation without recursion.\n  Grammax uses an explicit API for accepting the token stream. It allows you to easily use the tool with any (including your own) lexer. You can pause and resume parsing at any point. Parsing multiple token streams simultaneously is also possible.\n  Grammax supports simple lr and canonical lr parsing algorithms.\n  Automatic warnings about possible right-recursion cycles that cause a lot of parsing stack memory consumption.\n  Types can be assigned to terminals and non-terminals. The corresponding expressions are casted automatically.\n  The %top statement allows inserting package and import automatically. Therefore grammax can be used in an automated pipeline.\n\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/grammax/"
    },{
      "image": "/assets/img/projects/garbageset.jpg",
      "title": "GarbageSet",
      "date": "2020-05-22 00:00:00 +0200",
      "description": "Set data structure with all operation is O(1), including initialization!\n",
      "content": "This is a set data structure implementation in C. It can be initialized in constant time what makes it different compared to typical set implementation.\n\nComplexity\n\nThe following table gives an overview of what the datastructure is capable of. All the complexities are calculated assuming memory allocation is performed in O(1).\n\n\n  \n    \n      Operation\n      Description\n      Complexity\n    \n  \n  \n    \n      garbageset_init\n      Initializes the data structure with the specified capacity.\n      O(1)\n    \n    \n      garbageset_isset\n      Checks if there is an element at a specified index.\n      O(1)\n    \n    \n      garbageset_get\n      Retrieves the element at a specified indexor returns null, if there is no element at that index.\n      O(1)\n    \n    \n      garbageset_write\n      Writes a new element at the specified index or overwrites an old one if it was defined.\n      O(1)\n    \n  \n\n\nSpace complexity\n\nThe space complexity of the data structure is in O(n). However, apart from storing the payload array itself the data structure requires additional space for redundancy checking purposes. This additional space is about 2*n*sizeof(index_t) bytes where n is the capacity and index_t is the type used for indexes. With this user-defined you can easily reduce the memory overhead.\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/garbageset/"
    },{
      "image": "/assets/img/projects/numpat.jpg",
      "title": "NumPat",
      "date": "2020-08-30 00:00:00 +0200",
      "description": "Research tool for examining integer digit structure in different remainder classes.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/numpat/"
    },{
      "image": "/assets/img/projects/f0verifier.jpg",
      "title": "F0Verifier",
      "date": "2020-12-03 00:00:00 +0100",
      "description": "A program that verifies propositional logic proofs in the F0 proof system\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/f0verifier/"
    },{
      "image": "/assets/img/projects/flai.jpg",
      "title": "FLAI",
      "date": "2020-12-17 00:00:00 +0100",
      "description": "AI that analyzes flat/apartment offers and predicts prices, neural network implemented in pure python &amp; numpy.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/flai/"
    }
  ]
}

