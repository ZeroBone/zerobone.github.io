


{
  "pages": [
    {
      
      
      
      "content": "\n",
      "url": "/404.html"
    },{
      
      "title": "CV",
      "description": "Alexander Mayorov’s CV as a software engineer.\n",
      "content": "\n",
      "url": "/cv/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/"
    },{
      
      "title": "Other",
      "description": "This page contains some of my summaries, cheat sheets and other materials that you might also find useful.\n",
      "content": "\n  Integral table.\n  Modular arithmetic\n  Boolean algebra.\n  Regular expressions.\n\n",
      "url": "/other/"
    },{
      
      "title": "Legal",
      "description": "Links to pages containing legal information can be found here.\n",
      "content": "\n  Cookies Policy.\n\n",
      "url": "/legal/"
    },{
      
      "title": "Welcome!",
      
      "content": "Hi, I am Alexander Mayorov (a.k.a. ZeroBone) and this is my personal website. I am currently a computer science student and a backend developer.\n\nApart from studying I do research. My fields of interests:\n\n\n  Theoretical computer science: algorithms, theory of computation, formal languages, etc.\n  Logic, formal verification and automated proof procedures\n  Compiler design and programming languages\n  Group theory &amp; linear algebra\n  Graph theory\n\n\nLatest posts\n\n\n\nLatest projects\n\n\n",
      "url": "/"
    },{
      
      
      
      "content": "\n",
      "url": "/offline.html"
    },{
      
      "title": "Posts",
      
      "content": "\n",
      "url": "/posts/"
    },{
      
      "title": "Projects",
      "description": "Most of my open-source projects are listed on this page. Every project has a link to a GitHub repo with all the details and the source code.\n",
      "content": "\n",
      "url": "/projects/"
    },{
      
      "title": "Boolean algebra cheat sheet",
      
      "content": "Boolean algebra cheat sheet\n\n\n  Boolean algebra cheat sheet    \n      Operator Precedence\n      Axioms\n      Basic laws\n      Implication properties        \n          Simplification rules\n          Rewrite rules\n          Implication in an operator basis\n        \n      \n      XOR and equivalence properties\n      NAND and NOR\n      Linear clause forms\n    \n  \n\nOperator Precedence\n\nFirst operators in this list bind stronger:\n\n\n  ¬\\neg¬ (Negation)\n  ∧\\wedge∧ (And)\n  ⊕\\oplus⊕ (Xor)\n  ∨\\vee∨ (Or)\n  →\\rightarrow→ (Implication)\n  ↔\\leftrightarrow↔ (Equivalence)\n\n\nFrom this point on we will use multiplication (⋅\\cdot⋅) instead of ∧\\wedge∧ and +++ instead of ∨\\vee∨. Also, instead of writing ¬a\\neg a¬a we will write aˉ\\bar{a}aˉ. Sometimes the equality sign (===) is used to denote syntactic equality, but here we will denote sematic equivalence with ===.\n\nAxioms\n\na+b=b+aa⋅b=b⋅a}commutativitya(b+c)=ab+aca+bc=(a+b)(a+c)}distributivitya+0=aa⋅1=a}neutral elementsa+aˉ=1a⋅aˉ=0}complement\\begin{aligned}\n\\left.\\begin{aligned}\na + b &amp;= b + a \\\\\na \\cdot b &amp;= b \\cdot a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{commutativity} \\\\\n\\left.\\begin{aligned}\na(b + c) &amp;= ab + ac \\\\\na + bc &amp;= (a + b)(a + c)\n\\end{aligned}\\right\\rbrace\n&amp;\\text{distributivity} \\\\\n\\left.\\begin{aligned}\na + 0 &amp;= a \\\\\na \\cdot 1 &amp;= a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{neutral elements} \\\\\n\\left.\\begin{aligned}\na + \\bar{a} &amp;= 1 \\\\\na \\cdot \\bar{a} &amp;= 0\n\\end{aligned}\\right\\rbrace\n&amp;\\text{complement}\n\\end{aligned}a+ba⋅b​=b+a=b⋅a​}a(b+c)a+bc​=ab+ac=(a+b)(a+c)​}a+0a⋅1​=a=a​}a+aˉa⋅aˉ​=1=0​}​commutativitydistributivityneutral elementscomplement​\n\nBasic laws\n\na+a=aa⋅a=a}idempotencya+1=1a⋅0=0}killer elementsa+ab=aa(a+b)=a}absorbtion(a+b)+c=a+(b+c)(a⋅b)⋅c=a⋅(b⋅c)}associativitya+b‾=aˉ⋅bˉa⋅b‾=aˉ+bˉ}De Morganaˉˉ=a}involutionab+bc+aˉc=ab+aˉc(a+b)(b+c)(aˉ+c)=(a+b)(aˉ+c)}consensus\\begin{aligned}\n\\left.\\begin{aligned}\na + a &amp;= a \\\\\na \\cdot a &amp;= a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{idempotency} \\\\\n\\left.\\begin{aligned}\na + 1 &amp;= 1 \\\\\na \\cdot 0 &amp;= 0\n\\end{aligned}\\right\\rbrace\n&amp;\\text{killer elements} \\\\\n\\left.\\begin{aligned}\na + ab &amp;= a \\\\\na(a + b) &amp;= a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{absorbtion} \\\\\n\\left.\\begin{aligned}\n(a + b) + c &amp;= a + (b + c) \\\\\n(a \\cdot b) \\cdot c &amp;= a \\cdot (b \\cdot c)\n\\end{aligned}\\right\\rbrace\n&amp;\\text{associativity} \\\\\n\\left.\\begin{aligned}\n\\overline{a + b} &amp;= \\bar{a} \\cdot \\bar{b} \\\\\n\\overline{a \\cdot b} &amp;= \\bar{a} + \\bar{b}\n\\end{aligned}\\right\\rbrace\n&amp;\\text{De Morgan} \\\\\n\\left.\\begin{aligned}\n\\bar{\\bar{a}} &amp;= a\n\\end{aligned}\\right\\rbrace\n&amp;\\text{involution} \\\\\n\\left.\\begin{aligned}\nab + bc + \\bar{a}c &amp;= ab + \\bar{a}c \\\\\n(a + b)(b + c)(\\bar{a} + c) &amp;= (a + b)(\\bar{a} + c)\n\\end{aligned}\\right\\rbrace\n&amp;\\text{consensus}\n\\end{aligned}a+aa⋅a​=a=a​}a+1a⋅0​=1=0​}a+aba(a+b)​=a=a​}(a+b)+c(a⋅b)⋅c​=a+(b+c)=a⋅(b⋅c)​}a+b​a⋅b​=aˉ⋅bˉ=aˉ+bˉ​}aˉˉ​=a​}ab+bc+aˉc(a+b)(b+c)(aˉ+c)​=ab+aˉc=(a+b)(aˉ+c)​}​idempotencykiller elementsabsorbtionassociativityDe Morganinvolutionconsensus​\n\nImplication properties\n\nSimplification rules\n\n0→a=11→a=aa→1=1a→0=aˉa→aˉ=aˉa→a=1a⋅b→a=1a→a⋅b=a→ba→(a→b)=a→ba→(b→a)=1(a→b)→a=a(a→b)(b→c)→(a→c)=1\\begin{aligned}\n0 \\rightarrow a &amp;= 1 \\\\\n1 \\rightarrow a &amp;= a \\\\\na \\rightarrow 1 &amp;= 1 \\\\\na \\rightarrow 0 &amp;= \\bar{a} \\\\\na \\rightarrow \\bar{a} &amp;= \\bar{a} \\\\\na \\rightarrow a &amp;= 1 \\\\\na \\cdot b \\rightarrow a &amp;= 1 \\\\\na \\rightarrow a \\cdot b &amp;= a \\rightarrow b \\\\\na \\rightarrow (a \\rightarrow b) &amp;= a \\rightarrow b \\\\\na \\rightarrow (b \\rightarrow a) &amp;= 1 \\\\\n(a \\rightarrow b) \\rightarrow a &amp;= a \\\\\n(a \\rightarrow b)(b \\rightarrow c) \\rightarrow (a \\rightarrow c) &amp;= 1\n\\end{aligned}0→a1→aa→1a→0a→aˉa→aa⋅b→aa→a⋅ba→(a→b)a→(b→a)(a→b)→a(a→b)(b→c)→(a→c)​=1=a=1=aˉ=aˉ=1=1=a→b=a→b=1=a=1​\n\nRewrite rules\n\na→b=aˉ+ba→b‾=abˉa→b=bˉ→aˉa→b⋅c=(a→b)(a→c)a→b+c=(a→b)+(a→c)(a+b)→c=(a→c)(b→c)a→(b→c)=a⋅b→c(a→b)→c=(aˉ→c)(b→c)\\begin{aligned}\na \\rightarrow b &amp;= \\bar{a} + b \\\\\n\\overline{a \\rightarrow b} &amp;= a\\bar{b} \\\\\na \\rightarrow b &amp;= \\bar{b} \\rightarrow \\bar{a} \\\\\na \\rightarrow b \\cdot c &amp;= (a \\rightarrow b)(a \\rightarrow c) \\\\\na \\rightarrow b + c &amp;= (a \\rightarrow b) + (a \\rightarrow c) \\\\\n(a + b) \\rightarrow c &amp;= (a \\rightarrow c)(b \\rightarrow c) \\\\\na \\rightarrow (b \\rightarrow c) &amp;= a \\cdot b \\rightarrow c \\\\\n(a \\rightarrow b) \\rightarrow c &amp;= (\\bar{a} \\rightarrow c)(b \\rightarrow c)\n\\end{aligned}a→ba→ba→ba→b⋅ca→b+c(a+b)→ca→(b→c)(a→b)→c​=aˉ+b=abˉ=bˉ→aˉ=(a→b)(a→c)=(a→b)+(a→c)=(a→c)(b→c)=a⋅b→c=(aˉ→c)(b→c)​\n\nImplication in an operator basis\n\naˉ=a→0a⋅b=a→bˉ‾a⋅b=(a→(b→0))→0a+b=(a→b)→b\\begin{aligned}\n\\bar{a} &amp;= a \\rightarrow 0 \\\\\na \\cdot b &amp;= \\overline{a \\rightarrow \\bar{b}} \\\\\na \\cdot b &amp;= (a \\rightarrow (b \\rightarrow 0)) \\rightarrow 0 \\\\\na + b &amp;= (a \\rightarrow b) \\rightarrow b\n\\end{aligned}aˉa⋅ba⋅ba+b​=a→0=a→bˉ=(a→(b→0))→0=(a→b)→b​\n\nXOR and equivalence properties\n\na⊕0=aa⊕1=aˉa⊕a=0a⊕b=abˉ+aˉba⊕b=aˉ⊕bˉa⊕b=aˉ↔b=a↔bˉa⊕b=a↔b‾a⋅(b⊕c)=ab⊕aca+b=ab⊕a⊕ba→b=ab⊕a⊕1a↔b=a⊕b⊕1a↔a=1a↔aˉ=0a↔b=(a→b)(b→a)a↔b=a⋅b+aˉ⋅bˉa↔b=(aˉ+b)(a+bˉ)a↔b=aˉ↔bˉa↔b=aˉ⊕b=a⊕bˉ\\begin{aligned}\na \\oplus 0 &amp;= a \\\\\na \\oplus 1 &amp;= \\bar{a} \\\\\na \\oplus a &amp;= 0 \\\\\na \\oplus b &amp;= a\\bar{b} + \\bar{a}b \\\\\na \\oplus b &amp;= \\bar{a} \\oplus \\bar{b} \\\\\na \\oplus b &amp;= \\bar{a} \\leftrightarrow b = a \\leftrightarrow \\bar{b} \\\\\na \\oplus b &amp;= \\overline{a \\leftrightarrow b} \\\\\na \\cdot (b \\oplus c) &amp;= ab \\oplus ac \\\\\na + b &amp;= ab \\oplus a \\oplus b \\\\\na \\rightarrow b &amp;= ab \\oplus a \\oplus 1 \\\\\na \\leftrightarrow b &amp;= a \\oplus b \\oplus 1 \\\\\na \\leftrightarrow a &amp;= 1 \\\\\na \\leftrightarrow \\bar{a} &amp;= 0 \\\\\na \\leftrightarrow b &amp;= (a \\rightarrow b)(b \\rightarrow a) \\\\\na \\leftrightarrow b &amp;= a \\cdot b + \\bar{a} \\cdot \\bar{b} \\\\\na \\leftrightarrow b &amp;= (\\bar{a} + b)(a + \\bar{b}) \\\\\na \\leftrightarrow b &amp;= \\bar{a} \\leftrightarrow \\bar{b} \\\\\na \\leftrightarrow b &amp;= \\bar{a} \\oplus b = a \\oplus \\bar{b}\n\\end{aligned}a⊕0a⊕1a⊕aa⊕ba⊕ba⊕ba⊕ba⋅(b⊕c)a+ba→ba↔ba↔aa↔aˉa↔ba↔ba↔ba↔ba↔b​=a=aˉ=0=abˉ+aˉb=aˉ⊕bˉ=aˉ↔b=a↔bˉ=a↔b=ab⊕ac=ab⊕a⊕b=ab⊕a⊕1=a⊕b⊕1=1=0=(a→b)(b→a)=a⋅b+aˉ⋅bˉ=(aˉ+b)(a+bˉ)=aˉ↔bˉ=aˉ⊕b=a⊕bˉ​\n\nNAND and NOR\n\naˉ=a⊼aaˉ=a⊻aa⊼b‾=aˉ⊻bˉa⊻b‾=aˉ⊼bˉa⋅b=(a⊼b)⊼(a⊼b)a⋅b=(a⊻a)⊻(b⊻b)a+b=(a⊼a)⊼(b⊼b)a+b=(a⊻b)⊻(a⊻b)a⊕b=(a⊼(a⊼b))⊼((a⊼b)⊼b)a⊕b=(a⊻b)⊻((a⊻a)⊻(b⊻b))a↔b=(a⊼b)⊼((a⊼a)⊼(b⊼b))a↔b=(a⊻(a⊻b))⊻((a⊻b)⊻b)\\begin{aligned}\n\\bar{a} &amp;= a \\barwedge a \\\\\n\\bar{a} &amp;= a \\veebar a \\\\\n\\overline{a \\barwedge b} &amp;= \\bar{a} \\veebar \\bar{b} \\\\\n\\overline{a \\veebar b} &amp;= \\bar{a} \\barwedge \\bar{b} \\\\\na \\cdot b &amp;= (a \\barwedge b) \\barwedge (a \\barwedge b) \\\\\na \\cdot b &amp;= (a \\veebar a) \\veebar (b \\veebar b) \\\\\na + b &amp;= (a \\barwedge a) \\barwedge (b \\barwedge b) \\\\\na + b &amp;= (a \\veebar b) \\veebar (a \\veebar b) \\\\\na \\oplus b &amp;= (a \\barwedge (a \\barwedge b))\\barwedge ((a \\barwedge b) \\barwedge b) \\\\\na \\oplus b &amp;= (a \\veebar b) \\veebar ((a \\veebar a) \\veebar (b \\veebar b)) \\\\\na \\leftrightarrow b &amp;= (a \\barwedge b) \\barwedge ((a \\barwedge a) \\barwedge (b \\barwedge b)) \\\\\na \\leftrightarrow b &amp;= (a \\veebar (a \\veebar b)) \\veebar ((a \\veebar b) \\veebar b)\n\\end{aligned}aˉaˉa⊼b​a⊻b​a⋅ba⋅ba+ba+ba⊕ba⊕ba↔ba↔b​=a⊼a=a⊻a=aˉ⊻bˉ=aˉ⊼bˉ=(a⊼b)⊼(a⊼b)=(a⊻a)⊻(b⊻b)=(a⊼a)⊼(b⊼b)=(a⊻b)⊻(a⊻b)=(a⊼(a⊼b))⊼((a⊼b)⊼b)=(a⊻b)⊻((a⊻a)⊻(b⊻b))=(a⊼b)⊼((a⊼a)⊼(b⊼b))=(a⊻(a⊻b))⊻((a⊻b)⊻b)​\n\nLinear clause forms\n\nThese minimum conjunctive normal forms are often used to construct a linear clause form of some formula. The CNF is then typically passed to a SAT-solver.\n\nx↔aˉ=(xˉ+aˉ)(x+a)x↔a⋅b=(xˉ+a)(xˉ+b)(x+aˉ+bˉ)x↔a+b=(x+aˉ)(x+bˉ)(xˉ+a+b)x↔a⊕b=(x+a+bˉ)(x+aˉ+b)(xˉ+a+b)(xˉ+aˉ+bˉ)x↔a→b=(x+a)(x+bˉ)(xˉ+aˉ+b)x↔a↔b=(xˉ+aˉ+b)(xˉ+a+bˉ)(x+aˉ+bˉ)(x+a+b)\\begin{aligned}\nx \\leftrightarrow \\bar{a} &amp;= (\\bar{x} + \\bar{a})(x + a) \\\\\nx \\leftrightarrow a \\cdot b &amp;= (\\bar{x} + a)(\\bar{x} + b)(x + \\bar{a} + \\bar{b}) \\\\\nx \\leftrightarrow a + b &amp;= (x + \\bar{a})(x + \\bar{b})(\\bar{x} + a + b) \\\\\nx \\leftrightarrow a \\oplus b &amp;= (x + a + \\bar{b})(x + \\bar{a} + b)(\\bar{x} + a + b)(\\bar{x} + \\bar{a} + \\bar{b}) \\\\\nx \\leftrightarrow a \\rightarrow b &amp;= (x + a)(x + \\bar{b})(\\bar{x} + \\bar{a} + b) \\\\\nx \\leftrightarrow a \\leftrightarrow b &amp;= (\\bar{x} + \\bar{a} + b)(\\bar{x} + a + \\bar{b})(x + \\bar{a} + \\bar{b})(x + a + b)\n\\end{aligned}x↔aˉx↔a⋅bx↔a+bx↔a⊕bx↔a→bx↔a↔b​=(xˉ+aˉ)(x+a)=(xˉ+a)(xˉ+b)(x+aˉ+bˉ)=(x+aˉ)(x+bˉ)(xˉ+a+b)=(x+a+bˉ)(x+aˉ+b)(xˉ+a+b)(xˉ+aˉ+bˉ)=(x+a)(x+bˉ)(xˉ+aˉ+b)=(xˉ+aˉ+b)(xˉ+a+bˉ)(x+aˉ+bˉ)(x+a+b)​\n",
      "url": "/other/boolean-algebra/"
    },{
      
      "title": "Integral table",
      
      "content": "Integral table\n\n\n  Integral table    \n      Inverse derivatives\n      Standart integrals\n    \n  \n\n\nInverse derivatives\n\n∫dx=x+C\\int \\mathrm{d}x = x + C∫dx=x+C\n\n∫0dx=C\\int 0\\mathrm{d}x = C∫0dx=C\n\n∫xmdx=xm+1m+1+C,m≠−1\\int x^m\\mathrm{d}x = \\frac{x^{m+1}}{m+1} + C, m \\neq -1∫xmdx=m+1xm+1​+C,m​=−1\n\n∫dxx=ln⁡∣x∣+C\\int \\frac{\\mathrm{d}x}{x} = \\ln|x| + C∫xdx​=ln∣x∣+C\n\n∫cos⁡(x)dx=sin⁡(x)+C\\int \\cos(x) \\mathrm{d}x = \\sin(x) + C∫cos(x)dx=sin(x)+C\n\n∫sin⁡(x)dx=−cos⁡(x)+C\\int \\sin(x) \\mathrm{d}x = -\\cos(x) + C∫sin(x)dx=−cos(x)+C\n\n∫dx1+x2=arctg⁡(x)+C=−arcctg⁡(x)+C\\int \\frac{\\mathrm{d}x}{1+x^2} = \\arctg(x) + C = -\\arcctg(x) + C∫1+x2dx​=arctg(x)+C=−arcctg(x)+C\n\n∫dx1−x2=arcsin⁡(x)+C=−arccos⁡(x)+C\\int \\frac{\\mathrm{d}x}{\\sqrt{1-x^2}} = \\arcsin(x) + C = -\\arccos(x) + C∫1−x2​dx​=arcsin(x)+C=−arccos(x)+C\n\n∫axdx=axln⁡a+C\\int a^x \\mathrm{d}x = \\frac{a^x}{\\ln a} + C∫axdx=lnaax​+C\n\n∫exdx=ex+C\\int e^x \\mathrm{d}x = e^x + C∫exdx=ex+C\n\n∫sec⁡(x)2dx=∫dxcos⁡(x)2=tg⁡(x)+C\\int \\sec(x)^2 \\mathrm{d}x = \\int \\frac{dx}{\\cos(x)^2} = \\tg(x) + C∫sec(x)2dx=∫cos(x)2dx​=tg(x)+C\n\n∫cosec⁡(x)2dx=∫dxsin⁡(x)2=−ctg⁡(x)+C\\int \\cosec(x)^2 \\mathrm{d}x = \\int \\frac{dx}{\\sin(x)^2} = -\\ctg(x) + C∫cosec(x)2dx=∫sin(x)2dx​=−ctg(x)+C\n\n∫sh⁡(x)dx=ch⁡(x)+C\\int \\sh(x)\\mathrm{d}x = \\ch(x) + C∫sh(x)dx=ch(x)+C\n\n∫ch⁡(x)dx=sh⁡(x)+C\\int \\ch(x)\\mathrm{d}x = \\sh(x) + C∫ch(x)dx=sh(x)+C\n\n∫dxch⁡(x)2=th⁡(x)+C\\int \\frac{\\mathrm{d}x}{\\ch(x)^2} = \\th(x) + C∫ch(x)2dx​=th(x)+C\n\n∫dxsh⁡(x)2=−cth⁡(x)+C\\int \\frac{\\mathrm{d}x}{\\sh(x)^2} = -\\cth(x) + C∫sh(x)2dx​=−cth(x)+C\n\nStandart integrals\n\n∫dxx2+a2=1aarctg⁡(xa)+C\\int \\frac{\\mathrm{d}x}{x^2 + a^2} = \\frac{1}{a} \\arctg\\Big(\\frac{x}{a}\\Big) + C∫x2+a2dx​=a1​arctg(ax​)+C\n\n∫dxx2−a2=12aln⁡∣x−ax+a∣+C\\int \\frac{\\mathrm{d}x}{x^2 - a^2} = \\frac{1}{2a}\\ln\\Big|\\frac{x-a}{x+a}\\Big| + C∫x2−a2dx​=2a1​ln∣∣∣∣​x+ax−a​∣∣∣∣​+C\n\n∫dxx2±a2=ln⁡∣x+x2±a2∣+C\\int \\frac{\\mathrm{d}x}{\\sqrt{x^2 \\pm a^2}} = \\ln|x + \\sqrt{x^2 \\pm a^2}| + C∫x2±a2​dx​=ln∣x+x2±a2​∣+C\n\n∫dxa2−x2=arcsin⁡(xa)+C\\int \\frac{\\mathrm{d}x}{\\sqrt{a^2 - x^2}} = \\arcsin \\Big(\\frac{x}{a}\\Big) + C∫a2−x2​dx​=arcsin(ax​)+C\n",
      "url": "/other/int-table/"
    },{
      
      "title": "Modular arithmetic",
      
      "content": "Modular arithmetic\n\n\n  Modular arithmetic    \n      Definition\n      Basic properties        \n          Addition and multiplication            \n              Proof\n            \n          \n          Powers            \n              Proof\n            \n          \n          Combination with different moduli\n        \n      \n      General simultaneous congruences        \n          Proof\n        \n      \n    \n  \n\nDefinition\n\na≡bmod  m:⇔∃k∈Z:a−b=k⋅ma \\equiv b \\mod m :\\Leftrightarrow \\exists k \\in \\mathbb{Z} : a - b = k \\cdot ma≡bmodm:⇔∃k∈Z:a−b=k⋅m\n\nBasic properties\n\nAddition and multiplication\n\na≡bmod  mc≡dmod  m}⇒{a+c≡b+dmod  ma−c≡b−dmod  ma⋅c≡b⋅dmod  m\\left.\\begin{aligned}\na &amp;\\equiv b \\mod m \\\\\nc &amp;\\equiv d \\mod m\n\\end{aligned}\\right\\rbrace \\Rightarrow \n\\left\\lbrace\\begin{aligned}\na + c &amp;\\equiv b + d \\mod m \\\\\na - c &amp;\\equiv b - d \\mod m \\\\\na \\cdot c &amp;\\equiv b \\cdot d \\mod m\n\\end{aligned}\\right.ac​≡bmodm≡dmodm​}⇒⎩⎪⎪⎨⎪⎪⎧​a+ca−ca⋅c​≡b+dmodm≡b−dmodm≡b⋅dmodm​\n\nProof\n\nBy definition,\n\na−b=k1⋅mc−d=k2⋅ma - b = k_1 \\cdot m \\\\\nc - d = k_2 \\cdot ma−b=k1​⋅mc−d=k2​⋅m\n\nTherefore:\n\n(1): a−b+c−d=k1m+k2ma - b + c - d = k_1 m + k_2 ma−b+c−d=k1​m+k2​m reordered gives (a+c)−(b+d)=(k1+k2)m(a + c) - (b + d) = (k_1 + k_2)m(a+c)−(b+d)=(k1​+k2​)m.\n\n(2): Follows trivially from (1)\n\n(3): ac−bd=ac−bc+bc−bd=c(a−b)+b(c−d)=ck1m+bk2m=(ck1+bk2)mac - bd = ac - bc + bc - bd = c(a - b) + b(c - d) = c k_1 m + b k_2 m = (c k_1 + b k_2)mac−bd=ac−bc+bc−bd=c(a−b)+b(c−d)=ck1​m+bk2​m=(ck1​+bk2​)m\n\nPowers\n\na≡bmod  m⇒∀k∈N:ak≡bkmod  ma \\equiv b \\mod m \\Rightarrow \\forall k \\in \\mathbb{N} : a^k \\equiv b ^k \\mod ma≡bmodm⇒∀k∈N:ak≡bkmodm\n\nProof\n\na≡bmod  ma≡bmod  m\\begin{aligned}\na \\equiv b \\mod m \\\\\na \\equiv b \\mod m\n\\end{aligned}a≡bmodma≡bmodm​\n\nimplies a2≡b2mod  ma^2 \\equiv b^2 \\mod ma2≡b2modm. For greater exponents the statement is true by induction.\n\nCombination with different moduli\n\nx≡amod  n1n2⇒{x≡amod  n1x≡amod  n2x \\equiv a \\mod n_1 n_2 \\Rightarrow \n\\left\\lbrace\\begin{aligned}\nx &amp;\\equiv a \\mod n_1 \\\\\nx &amp;\\equiv a \\mod n_2\n\\end{aligned}\\right.x≡amodn1​n2​⇒{xx​≡amodn1​≡amodn2​​\n\nIf gcd⁡(n1,n2)=1\\gcd(n_1, n_2) = 1gcd(n1​,n2​)=1, then the converse also holds:\n\nx≡amod  n1x≡amod  n2}⇒x≡amod  n1n2\\left.\\begin{aligned}\nx &amp;\\equiv a \\mod n_1 \\\\\nx &amp;\\equiv a \\mod n_2\n\\end{aligned}\\right\\rbrace \\Rightarrow\nx \\equiv a \\mod n_1 n_2xx​≡amodn1​≡amodn2​​}⇒x≡amodn1​n2​\n\nGeneral simultaneous congruences\n\nIt holds, that\n\nx≡a1mod  n1x≡a2mod  n2L≠∅}⇔{a1≡a2mod  gcd⁡(n1,n2)x1≡x2mod  lcm(n1,n2)∀x1,x2∈L\\left.\\begin{aligned}\nx &amp;\\equiv a_1 \\mod n_1 \\\\\nx &amp;\\equiv a_2 \\mod n_2 \\\\\nL &amp;\\neq \\varnothing\n\\end{aligned}\\right\\rbrace\n\\Leftrightarrow\n\\left\\lbrace\\begin{aligned}\na_1 &amp;\\equiv a_2 \\mod \\gcd(n_1, n_2) \\\\\nx_1 &amp;\\equiv x_2 \\mod \\lcm(n_1, n_2) \\quad\\forall x_1,x_2 \\in L\n\\end{aligned}\\right.xxL​≡a1​modn1​≡a2​modn2​​=∅​⎭⎪⎪⎬⎪⎪⎫​⇔{a1​x1​​≡a2​modgcd(n1​,n2​)≡x2​modlcm(n1​,n2​)∀x1​,x2​∈L​\n\nwhere LLL is the solution set.\n\nProof\n\n(⇒\\Rightarrow⇒): First, we prove that a1a_1a1​ must be equivalent to a2a_2a2​ modulo gcd⁡(n1,n2)\\gcd(n_1, n_2)gcd(n1​,n2​):\n\n{x≡a1mod  n1x≡a2mod  n2⇒{x−a1≡0mod  n1x−a2≡0mod  n2⇒{x−a1≡0mod  gcd⁡(n1,n2)x−a2≡0mod  gcd⁡(n1,n2)⇒x−a1−x+a2≡0mod  gcd⁡(n1,n2)⇒a2−a1≡0mod  gcd⁡(n1,n2)⇒a1≡a2mod  gcd⁡(n1,n2)\\begin{aligned}\n\\left\\lbrace\\begin{aligned}\nx &amp;\\equiv a_1 \\mod n_1 \\\\\nx &amp;\\equiv a_2 \\mod n_2\n\\end{aligned}\\right.\n&amp;\\Rightarrow\n\\left\\lbrace\\begin{aligned}\nx - a_1 &amp;\\equiv 0 \\mod n_1 \\\\\nx - a_2 &amp;\\equiv 0 \\mod n_2\n\\end{aligned}\\right. \\\\\n&amp;\\Rightarrow\n\\left\\lbrace\\begin{aligned}\nx - a_1 &amp;\\equiv 0 \\mod \\gcd(n_1, n_2) \\\\\nx - a_2 &amp;\\equiv 0 \\mod \\gcd(n_1, n_2)\n\\end{aligned}\\right. \\\\\n&amp;\\Rightarrow\nx - a_1 - x + a_2 \\equiv 0 \\mod \\gcd(n_1, n_2) \\\\\n&amp;\\Rightarrow\na_2 - a_1 \\equiv 0 \\mod \\gcd(n_1, n_2) \\\\\n&amp;\\Rightarrow\na_1 \\equiv a_2 \\mod \\gcd(n_1, n_2)\n\\end{aligned}{xx​≡a1​modn1​≡a2​modn2​​​⇒{x−a1​x−a2​​≡0modn1​≡0modn2​​⇒{x−a1​x−a2​​≡0modgcd(n1​,n2​)≡0modgcd(n1​,n2​)​⇒x−a1​−x+a2​≡0modgcd(n1​,n2​)⇒a2​−a1​≡0modgcd(n1​,n2​)⇒a1​≡a2​modgcd(n1​,n2​)​\n\nThe set of solutions LLL is defined modulo lcm(n1,n2)\\lcm(n_1, n_2)lcm(n1​,n2​), because given x1,x2∈L≠∅x_1, x_2 \\in L \\neq \\varnothingx1​,x2​∈L​=∅ it follows, that\n\nx1=a1+k1⋅n1x1=a2+k2⋅n2x2=a1+k3⋅n1x2=a2+k4⋅n2\\begin{aligned}\nx_1 &amp;= a_1 + k_1 \\cdot n_1 \\\\\nx_1 &amp;= a_2 + k_2 \\cdot n_2 \\\\\nx_2 &amp;= a_1 + k_3 \\cdot n_1 \\\\\nx_2 &amp;= a_2 + k_4 \\cdot n_2\n\\end{aligned}x1​x1​x2​x2​​=a1​+k1​⋅n1​=a2​+k2​⋅n2​=a1​+k3​⋅n1​=a2​+k4​⋅n2​​\n\nwhere k1,k2,k3,k4∈Zk_1, k_2, k_3, k_4 \\in \\mathbb{Z}k1​,k2​,k3​,k4​∈Z. By subtracting the third row from the first one and the fourth row from the second one we get:\n\nx1−x2=a1+k1⋅n1−a1−k3⋅n1=(k1−k3)⋅n1x1−x2=a2+k2⋅n2−a2−k4⋅n2=(k2−k4)⋅n2\\begin{aligned}\nx_1 - x_2 &amp;= a_1 + k_1\\cdot n_1 - a_1 - k_3 \\cdot n_1 = (k_1 - k_3) \\cdot n_1 \\\\\nx_1 - x_2 &amp;= a_2 + k_2 \\cdot n_2 - a_2 - k_4 \\cdot n_2 = (k_2 - k_4) \\cdot n_2\n\\end{aligned}x1​−x2​x1​−x2​​=a1​+k1​⋅n1​−a1​−k3​⋅n1​=(k1​−k3​)⋅n1​=a2​+k2​⋅n2​−a2​−k4​⋅n2​=(k2​−k4​)⋅n2​​\n\nSo x1−x2x_1 - x_2x1​−x2​ must be a multiple of both n1n_1n1​ and n2n_2n2​ and therefore:\n\nx1−x2≡0mod  lcm(n1,n2)x_1 - x_2 \\equiv 0 \\mod \\lcm(n_1, n_2)x1​−x2​≡0modlcm(n1​,n2​)\n\n(⇐\\Leftarrow⇐): By assumption, we know that for some k∈Zk \\in \\mathbb{Z}k∈Z\n\na1−a2=k⋅gcd⁡(n1,n2)a_1 - a_2 = k \\cdot \\gcd(n_1, n_2)a1​−a2​=k⋅gcd(n1​,n2​)\n\nAlso, as stated in the Bézout Lemma, we can write the greatest common divisor as a linear combination:\n\ngcd⁡(n1,n2):=u⋅n1+v⋅n2\\gcd(n_1, n_2) := u \\cdot n_1 + v \\cdot n_2gcd(n1​,n2​):=u⋅n1​+v⋅n2​\n\nBy replacing gcd⁡(n1,n2)\\gcd(n_1, n_2)gcd(n1​,n2​) with the linear combination we get:\n\na1−a2=k⋅u⋅n1+k⋅v⋅n2a_1 - a_2 =\nk \\cdot u \\cdot n_1 + k \\cdot v \\cdot n_2a1​−a2​=k⋅u⋅n1​+k⋅v⋅n2​\n\nThus,\n\na1−k⋅u⋅n1⏟≡a1 mod n1=a2+k⋅v⋅n2⏟≡a2 mod n2\\underbrace{a_1 - k \\cdot u \\cdot n_1}_{\\equiv a_1 \\bmod n_1} =\n\\underbrace{a_2 + k \\cdot v \\cdot n_2}_{\\equiv a_2 \\bmod n_2}≡a1​modn1​a1​−k⋅u⋅n1​​​=≡a2​modn2​a2​+k⋅v⋅n2​​​\n\nAssuming the solution is defined modulo lcm(n1,n2)\\lcm(n_1, n_2)lcm(n1​,n2​), it follows that:\n\nx≡a1−k⋅u⋅n1≡a2+k⋅v⋅n2mod  lcm(n1,n2)x \\equiv a_1 - k \\cdot u \\cdot n_1 \\equiv a_2 + k \\cdot v \\cdot n_2 \\mod \\lcm(n_1, n_2)x≡a1​−k⋅u⋅n1​≡a2​+k⋅v⋅n2​modlcm(n1​,n2​)\n",
      "url": "/other/modulo/"
    },{
      
      "title": "Regular expression cheat sheet",
      
      "content": "Regular expression cheat sheet\n\n\n  Regular expression cheat sheet    \n      Denotational semantics\n      The nullable function\n      Brzozowski derivatives\n      Basic laws\n      Other identities        \n          Simplifying Kleene star\n          General elimination rules\n        \n      \n      Arden’s Theorem\n    \n  \n\nDenotational semantics\n\n[ ⁣[∅] ⁣]:=∅[ ⁣[ε] ⁣]:={ε}[ ⁣[a] ⁣]:={a}[ ⁣[a⋅b] ⁣]:=[ ⁣[a] ⁣]⋅[ ⁣[b] ⁣][ ⁣[a+b] ⁣]:=[ ⁣[a] ⁣]∪[ ⁣[b] ⁣][ ⁣[a∗] ⁣]:=[ ⁣[a] ⁣]∗\\begin{aligned}\n[\\![\\varnothing]\\!] &amp;:= \\varnothing \\\\\n[\\![\\varepsilon]\\!] &amp;:= \\{\\varepsilon\\} \\\\\n[\\![a]\\!] &amp;:= \\{a\\} \\\\\n[\\![a \\cdot b]\\!] &amp;:= [\\![a]\\!] \\cdot [\\![b]\\!] \\\\\n[\\![a + b]\\!] &amp;:= [\\![a]\\!] \\cup [\\![b]\\!] \\\\\n[\\![a^*]\\!] &amp;:= [\\![a]\\!]^*\n\\end{aligned}[[∅]][[ε]][[a]][[a⋅b]][[a+b]][[a∗]]​:=∅:={ε}:={a}:=[[a]]⋅[[b]]:=[[a]]∪[[b]]:=[[a]]∗​\n\nwhere the definitions of concatenation of languages and the Kleene stars are as follows:\n\nL1⋅L2:={w1⋅w2:w1∈L1,w2∈L2}L0:={ε}Ln:=L⋅Ln−1L∗:=⋃n∈N0Ln\\begin{aligned}\nL_1 \\cdot L_2 &amp;:= \\{w_1 \\cdot w_2 : w_1 \\in L_1, w_2 \\in L_2\\} \\\\\nL^0 &amp;:= \\{\\varepsilon\\} \\\\\nL^n &amp;:= L \\cdot L^{n-1} \\\\\nL^* &amp;:= \\bigcup_{n \\in \\mathbb{N}_0}{L^n}\n\\end{aligned}L1​⋅L2​L0LnL∗​:={w1​⋅w2​:w1​∈L1​,w2​∈L2​}:={ε}:=L⋅Ln−1:=n∈N0​⋃​Ln​\n\nThe nullable function\n\nn(a)=⊥n(∅)=⊤n(r1⋅r2)=n(r1)∧n(r2)n(r1+r2)=n(r1)∨n(r2)n(r∗)=⊤\\begin{aligned}\nn(a) &amp;= \\bot \\\\\nn(\\varnothing) &amp;= \\top \\\\\nn(r_1 \\cdot r_2) &amp;= n(r_1) \\wedge n(r_2) \\\\\nn(r_1 + r_2) &amp;= n(r_1) \\vee n(r_2) \\\\\nn(r^*) &amp;= \\top\n\\end{aligned}n(a)n(∅)n(r1​⋅r2​)n(r1​+r2​)n(r∗)​=⊥=⊤=n(r1​)∧n(r2​)=n(r1​)∨n(r2​)=⊤​\n\nBrzozowski derivatives\n\nx−1⋅L:={a:x⋅a∈L}x−1⋅a={εa=x∅a≠xx−1⋅ε=∅x−1⋅∅=∅x−1⋅(a⋅b)={(x−1⋅a)⋅b+x−1⋅bn(a)(x−1⋅a)⋅botherwisex−1⋅(a+b)=(x−1⋅a)+(x−1⋅b)x−1⋅a∗=(x−1⋅a)⋅a∗\\begin{aligned}\nx^{-1} \\cdot L &amp;:= \\{a : x \\cdot a \\in L\\} \\\\\nx^{-1}\\cdot a &amp;= \\begin{cases}\n\\varepsilon &amp; a = x \\\\\n\\varnothing &amp; a \\neq x\n\\end{cases} \\\\\nx^{-1}\\cdot \\varepsilon &amp;= \\varnothing \\\\\nx^{-1}\\cdot \\varnothing &amp;= \\varnothing \\\\\nx^{-1}\\cdot (a \\cdot b) &amp;= \\begin{cases}\n(x^{-1} \\cdot a) \\cdot b + x^{-1} \\cdot b &amp; n(a) \\\\\n(x^{-1} \\cdot a) \\cdot b &amp; \\text{otherwise}\n\\end{cases} \\\\\nx^{-1}\\cdot (a + b) &amp;= (x^{-1} \\cdot a) + (x^{-1} \\cdot b) \\\\\nx^{-1} \\cdot a^* &amp;= (x^{-1} \\cdot a) \\cdot a^*\n\\end{aligned}x−1⋅Lx−1⋅ax−1⋅εx−1⋅∅x−1⋅(a⋅b)x−1⋅(a+b)x−1⋅a∗​:={a:x⋅a∈L}={ε∅​a=xa​=x​=∅=∅={(x−1⋅a)⋅b+x−1⋅b(x−1⋅a)⋅b​n(a)otherwise​=(x−1⋅a)+(x−1⋅b)=(x−1⋅a)⋅a∗​\n\nBasic laws\n\na+b=b+aa+a=a∅+a=a∅⋅a=a⋅∅=∅ε⋅a=a⋅ε=aa(b+c)=ab+ac(a+b)c=ac+bca∗a∗=a∗\\begin{aligned}\na + b &amp;= b + a \\\\\na + a &amp;= a \\\\\n\\varnothing + a &amp;= a \\\\\n\\varnothing \\cdot a = a \\cdot \\varnothing &amp;= \\varnothing \\\\\n\\varepsilon \\cdot a = a \\cdot \\varepsilon &amp;= a \\\\\na(b + c) &amp;= ab + ac \\\\\n(a + b)c &amp;= ac + bc \\\\\na^* a^* &amp;= a^*\n\\end{aligned}a+ba+a∅+a∅⋅a=a⋅∅ε⋅a=a⋅εa(b+c)(a+b)ca∗a∗​=b+a=a=a=∅=a=ab+ac=ac+bc=a∗​\n\nOther identities\n\nε∗=ε∅∗=∅aa∗=a∗a(a∗)∗=a∗ε+aa∗=a∗(ab)∗a=a(ba)∗\\begin{aligned}\n\\varepsilon^* &amp;= \\varepsilon \\\\\n\\varnothing^* &amp;= \\varnothing \\\\\na a^* &amp;= a^* a \\\\\n(a^*)^* &amp;= a^* \\\\\n\\varepsilon + aa^* &amp;= a^* \\\\\n(ab)^* a &amp;= a(ba)^*\n\\end{aligned}ε∗∅∗aa∗(a∗)∗ε+aa∗(ab)∗a​=ε=∅=a∗a=a∗=a∗=a(ba)∗​\n\nSimplifying Kleene star\n\n(a+b)∗=(a∗b∗)∗=(a∗+b∗)∗=a∗(ba∗)∗(a + b)^* = (a^* b^*)^* = (a^* + b^*)^* = a^*(ba^*)^*(a+b)∗=(a∗b∗)∗=(a∗+b∗)∗=a∗(ba∗)∗\n\nGeneral elimination rules\n\na+b={aL(b)⊆L(a)bL(a)⊆L(b)a∗b∗={a∗L(b)⊆L(a)b∗L(a)⊆L(b)\\begin{aligned}\na + b &amp;= \\begin{cases}\na &amp; L(b) \\subseteq L(a) \\\\\nb &amp; L(a) \\subseteq L(b)\n\\end{cases} \\\\\na^* b^* &amp;= \\begin{cases}\na^* &amp; L(b) \\subseteq L(a) \\\\\nb^* &amp; L(a) \\subseteq L(b)\n\\end{cases}\n\\end{aligned}a+ba∗b∗​={ab​L(b)⊆L(a)L(a)⊆L(b)​={a∗b∗​L(b)⊆L(a)L(a)⊆L(b)​​\n\nTypical examples of the application of this rule:\n\nab+(a+b)∗=(a+b)∗ε+(a+b)∗=(a+b)∗a∗b∗aba+(a+b+c)∗=(a+b+c)∗(a+b)∗a∗=(a+b)∗\\begin{aligned}\nab + (a + b)^* &amp;= (a + b)^* \\\\\n\\varepsilon + (a + b)^* &amp;= (a + b)^* \\\\\na^* b^* aba + (a + b + c)^* &amp;= (a + b + c)^* \\\\\n(a+b)^* a^* &amp;= (a+b)^*\n\\end{aligned}ab+(a+b)∗ε+(a+b)∗a∗b∗aba+(a+b+c)∗(a+b)∗a∗​=(a+b)∗=(a+b)∗=(a+b+c)∗=(a+b)∗​\n\nArden’s Theorem\n\nr=q+rp⇒r=qp∗r = q + rp \\Rightarrow r = qp^*r=q+rp⇒r=qp∗\n",
      "url": "/other/regexp/"
    },{
      
      "title": "Blog",
      "description": "The official Hydejack blog. Version updates, example content and how-to guides on how to blog with Jekyll.\n",
      "content": "\n",
      "url": "/blog/2/"
    }
  ], 
  "documents": [
    {
      "image": {"path":"/assets/img/blog/call-stack-buffer-overflow-tmb.jpg","srcset":{"1920w":"/assets/img/blog/call-stack-buffer-overflow-tmb.jpg","960w":"/assets/img/blog/call-stack-buffer-overflow-tmb@0,5x.jpg","480w":"/assets/img/blog/call-stack-buffer-overflow-tmb@0,25x.jpg","240w":"/assets/img/blog/call-stack-buffer-overflow-tmb@0,125x.jpg"}},
      "title": "Call Stack - buffer overflow vulnerability",
      "date": "2019-06-30 00:00:00 +0200",
      
      "content": "Buffer overflows are a kind of call stack vulnerability that occur when buffers are created on the stack, but accessed improperly. Buffer underruns are typically not so dangerous, because writing in the current stack frame or beyond the stack pointer will only affect local variables on that stack frame. On the other side, buffer overruns can allow the attacker to overwrite the return address and thus even modify the program’s behavior.\n\nBuffer overflow\n\nC programmers often allocate buffers on the stack to handle user input. If the input reading logic is implemented incorrectly and has no buffer length checks, a underflow/overflow can happen. If the user input is long enough, it will overwrite the saved ebp register of the previous stack frame and, what matters most, the return address.\n\n\n\nExample\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nvoid __attribute__((noinline)) fun(int a, int b, int c) {\n\n    char buffer[16] = {0};\n\n    int* prevEbp = &amp;a - 2;\n    int* ret = &amp;a - 1;\n\n    printf(\"Buffer start: %p Buffer start pointer address: %p\\n\", buffer, &amp;buffer);\n    printf(\"Previous EBP: %p Value: %d Value as hex: %x\\n\", prevEbp, *prevEbp, *prevEbp);\n    printf(\"Return address: %p Value: %x\\n\", ret, *ret);\n    printf(\"Buffer end: %p\\n\", buffer + 16);\n\n    fflush(stdout);\n\n}\n\nint main() {\n    printf(\"Ptr size: %d bytes\\n\", sizeof(void*));\n    fun(1, 2, 3)\n    return 0;\n}\n\n\nWe can calculate the return address position by taking addresses of the buffer and the function arguments. In this case we only take the pointer to the first argument, because it is added to the stack last. The previous base pointer size as well as the return address size are 4 bytes, so we can just subtract 1 (4 bytes) from the pointer to get the return address and 2 (8 bytes) to get the base pointer.\n\nWe can now compile the program with the -fno-stack-protector flag to disable stack protecting canary that gcc adds by default:\n\n$ gcc main.c -o viewret -fno-stack-protector\n\n\nBy running the program I got:\n\nPtr size: 4 bytes\nBuffer start: 0061FEE8 Buffer start pointer address: 0061FEE8\nPrevious EBP: 0061FF08 Value: 6422312 Value as hex: 61ff28\nReturn address: 0061FF0C Value: 401508\nBuffer end: 0061FEF8\n\n\nWe can easily alter the return address value now:\n\nvoid __attribute__((noinline)) fun(int a, int b, int c) {\n\n    char buffer[16] = {0};\n\n    int* prevEbp = &amp;a - 2;\n    int* ret = &amp;a - 1;\n\n    printf(\"Buffer start: %p Buffer start pointer address: %p\\n\", buffer, &amp;buffer);\n    printf(\"Previous EBP: %p Value: %d Value as hex: %x\\n\", prevEbp, *prevEbp, *prevEbp);\n    printf(\"Return address: %p Value: %x\\n\", ret, *ret);\n    printf(\"Buffer end: %p\\n\", buffer + 16);\n\n    fflush(stdout);\n\n    *ret = 0xcafeefac;\n\n}\n\n\nNow, if we run the program we will get a segmentation fault error because the function will try to jump back to the calee using an invalid address.\n\nWe can examine exactly how it works by running the GDB debugger:\n\n$ gdb viewret.exe\n\n\nOf course, we need to set the breakpoint at the fun function:\n\n(gdb) $ br fun\n\n\n[New Thread 3388.0x3368]\n[New Thread 3388.0x1a2c]\nPtr size: 4 bytes\n\nBreakpoint 1, 0x00401416 in fun ()\n\n\nBy using the frame command we can view the saved registers if the current stack frame.\n\n(gdb) $ info frame\n\n\nStack level 0, frame at 0x61ff10:\n eip = 0x401416 in fun; saved eip 0x401508\n called by frame at 0x61ff30\n Arglist at 0x61ff08, args:\n Locals at 0x61ff08, Previous frame's sp is 0x61ff10\n Saved registers:\n  ebp at 0x61ff08, eip at 0x61ff0c\n\n\nThe ebp register of the previous stack frame is at address 0x61ff08, the return address - at 0x61ff0c. The values are the same as generated by the program above.\n\n(gdb) $ c\n\n\nContinuing.\nBuffer start: 0061FEE8 Buffer start pointer address: 0061FEE8\nPrevious EBP: 0061FF08 Value: 6422312 Value as hex: 61ff28\nReturn address: 0061FF0C Value: 401508\nBuffer end: 0061FEF8\n\nProgram received signal SIGSEGV, Segmentation fault.\n0xcafeefac in ?? ()\n\n\nBy stepping over the breakpoint we can see the invalid return address that caused the segmentation fault.\n\nAltering variables\n\nLet’s examine another program that reads data from the standard input stream:\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n\nint main() {\n\t\n    volatile int zero;\n\n    char buffer[64];\n\n    zero = 0;\n\n    gets(buffer);\n\n    if (zero) {\n        printf(\"You changed the zero variable to %d (hex: %x)!\", zero, zero);\n    }\n    else {\n        puts(\"Variable not changed.\");\n    }\n\n    return 0;\n}\n\n\nThe zero variable is marked as volatile to prevent the compiler from optimizing it’s usage, e.g. by caching it’s value in one of the general-purpose registers.\n\nBy disassembling the program with gdb, we get:\n\n0x00401410 &lt;+0&gt;:     push   ebp ; save the previous ebp register\n0x00401411 &lt;+1&gt;:     mov    ebp,esp ; initializing ebp of the new stack frame\n0x00401413 &lt;+3&gt;:     and    esp,0xfffffff0 ; memory aligning\n0x00401416 &lt;+6&gt;:     sub    esp,0x60 ; memory allocation on the stack\n0x00401419 &lt;+9&gt;:     call   0x401980 &lt;__main&gt;\n0x0040141e &lt;+14&gt;:    mov    DWORD PTR [esp+0x5c],0x0 ; assign to zero\n; eax = esp + 0x1c\n0x00401426 &lt;+22&gt;:    lea    eax,[esp+0x1c]\n; the address calculated with the previous instruction gets saved on the stack\n0x0040142a &lt;+26&gt;:    mov    DWORD PTR [esp],eax\n0x0040142d &lt;+29&gt;:    call   0x403ae8 &lt;gets&gt; ; gets() call\n; load the value from the memory for comparison\n0x00401432 &lt;+34&gt;:    mov    eax,DWORD PTR [esp+0x5c]\n0x00401436 &lt;+38&gt;:    test   eax,eax ; test if it is zero\n0x00401438 &lt;+40&gt;:    je     0x401458 &lt;main+72&gt; \n0x0040143a &lt;+42&gt;:    mov    edx,DWORD PTR [esp+0x5c]\n; commands needed for printf\n0x0040143e &lt;+46&gt;:    mov    eax,DWORD PTR [esp+0x5c]\n0x00401442 &lt;+50&gt;:    mov    DWORD PTR [esp+0x8],edx\n0x00401446 &lt;+54&gt;:    mov    DWORD PTR [esp+0x4],eax\n0x0040144a &lt;+58&gt;:    mov    DWORD PTR [esp],0x405044\n0x00401451 &lt;+65&gt;:    call   0x403ac8 &lt;printf&gt; ; success print\n0x00401456 &lt;+70&gt;:    jmp    0x401464 &lt;main+84&gt; ; jump over the else branch\n0x00401458 &lt;+72&gt;:    mov    DWORD PTR [esp],0x405073\n0x0040145f &lt;+79&gt;:    call   0x403ac0 &lt;puts&gt; ; error print\n; return with exit code 0\n0x00401464 &lt;+84&gt;:    mov    eax,0x0\n0x00401469 &lt;+89&gt;:    leave\n0x0040146a &lt;+90&gt;:    ret\n0x0040146b &lt;+91&gt;:    nop\n0x0040146c &lt;+92&gt;:    xchg   ax,ax\n0x0040146e &lt;+94&gt;:    xchg   ax,ax\n\n\nWe can set 2 breakpoints - before and after the gets() call.\n\n(gdb) $ br *0x0040142d\n(gdb) $ br *0x00401432\n\n\nWith gdb, we can define what commands to run when these breakpoints are reached:\n\n(gdb) $ define hook-stop\n&gt;info registers\n&gt;x/24wx $esp\n&gt;x/2i $eip\n&gt;end\n\n\nBu running the commands above we will see the register state, 24 machine words on the stack and two next instructions after the instruction pointer:\n\neax            0x61fedc 6422236\necx            0x4018f0 4200688\nedx            0x50000018       1342177304\nebx            0x2d2000 2957312\nesp            0x61fec0 0x61fec0\nebp            0x61ff28 0x61ff28\nesi            0x4012d0 4199120\nedi            0x4012d0 4199120\neip            0x40142d 0x40142d &lt;main+29&gt;\neflags         0x202    [ IF ]\ncs             0x23     35\nss             0x2b     43\nds             0x2b     43\nes             0x2b     43\nfs             0x53     83\ngs             0x2b     43\n0x61fec0:       0x0061fedc      0x00000008      0x772c8023      0x772c801a\n0x61fed0:       0xb3b6879d      0x004012d0      0x004012d0      0x00000000\n0x61fee0:       0x004018f0      0x0061fed0      0x0061ff08      0x0061ffcc\n0x61fef0:       0x772cdd70      0xc4e6dd59      0xfffffffe      0x772c801a\n0x61ff00:       0x772c810d      0x004018f0      0x0061ff50      0x0040195b\n0x61ff10:       0x004018f0      0x00000000      0x002d2000      0x00000000\n=&gt; 0x40142d &lt;main+29&gt;:  call   0x403ae8 &lt;gets&gt;\n   0x401432 &lt;main+34&gt;:  mov    eax,DWORD PTR [esp+0x5c]\n\nBreakpoint 1, 0x0040142d in main ()\n\n\nNow we can examine how the input affects the stack:\n\n(gdb) $ c\nContinuing.\n0000000000000000000000000000000000000000000\n\n\neax            0x61fedc 6422236\necx            0x772eb098       1999548568\nedx            0xa      10\nebx            0x2d2000 2957312\nesp            0x61fec0 0x61fec0\nebp            0x61ff28 0x61ff28\nesi            0x4012d0 4199120\nedi            0x4012d0 4199120\neip            0x401432 0x401432 &lt;main+34&gt;\neflags         0x216    [ PF AF IF ]\ncs             0x23     35\nss             0x2b     43\nds             0x2b     43\nes             0x2b     43\nfs             0x53     83\ngs             0x2b     43\n0x61fec0:       0x0061fedc      0x00000008      0x772c8023      0x772c801a\n0x61fed0:       0xb3b6879d      0x004012d0      0x004012d0      0x30303030\n0x61fee0:       0x30303030      0x30303030      0x30303030      0x30303030\n0x61fef0:       0x30303030      0x30303030      0x30303030      0x30303030\n0x61ff00:       0x30303030      0x00303030      0x0061ff50      0x0040195b\n0x61ff10:       0x004018f0      0x00000000      0x002d2000      0x00000000\n=&gt; 0x401432 &lt;main+34&gt;:  mov    eax,DWORD PTR [esp+0x5c]\n   0x401436 &lt;main+38&gt;:  test   eax,eax\n\nBreakpoint 2, 0x00401432 in main ()\n\n\nAs we can see, 43 zero-characters (ascii code 0x30) was not enough to get to the zero value that we want it to occur. In order to get past the end of the buffer, we need at more than 64 bytes (because the buffer size is 64). For demonstration purposes, we will use the following string as the input:\n\n000011111111111111112222222222222222333333333333333344444444444456\n\n\nThis string contains 66 characters, so the two last characters 5 and 6 should overwrite the 2 least significant bytes (because memory endianness is little-endian) of the variable.\n\n(gdb) $ c\nContinuing.\n000011111111111111112222222222222222333333333333333344444444444456\n\n\neax            0x61fedc 6422236\necx            0x772eb098       1999548568\nedx            0xa      10\nebx            0x3f9000 4165632\nesp            0x61fec0 0x61fec0\nebp            0x61ff28 0x61ff28\nesi            0x4012d0 4199120\nedi            0x4012d0 4199120\neip            0x401432 0x401432 &lt;main+34&gt;\neflags         0x216    [ PF AF IF ]\ncs             0x23     35\nss             0x2b     43\nds             0x2b     43\nes             0x2b     43\nfs             0x53     83\ngs             0x2b     43\n0x61fec0:       0x0061fedc      0x00000008      0x772c8023      0x772c801a\n0x61fed0:       0xe53b01b1      0x004012d0      0x004012d0      0x30303030\n0x61fee0:       0x31313131      0x31313131      0x31313131      0x31313131\n0x61fef0:       0x32323232      0x32323232      0x32323232      0x32323232\n0x61ff00:       0x33333333      0x33333333      0x33333333      0x33333333\n0x61ff10:       0x34343434      0x34343434      0x34343434      0x00003635\n=&gt; 0x401432 &lt;main+34&gt;:  mov    eax,DWORD PTR [esp+0x5c]\n   0x401436 &lt;main+38&gt;:  test   eax,eax\n\nBreakpoint 2, 0x00401432 in main ()\n\n\nBy continuing we see that the variable now contains 0x3635 or 13877 in decimal.\n\n(gdb) $ c\n\n\nContinuing.\nYou changed the zero variable to 13877 (hex: 3635)![Inferior 1 (process 4848) exited normally]\nError while running hook_stop:\nThe program has no registers now.\n\n\nIn order to alter the zero variable we need to represent the number in the little endian form and write the corresponding bytes to the 65, 66, 67 and 68 offsets in in buffer.\n\nProtection against buffer overflows\n\nCompilers and operating systems have some techniques to prevent such stack exploits. In gcc, for example, if the function allocates a buffer on the stack, an additional so-called stack canary is added. A stack canary is just a random integer generated when the function is called. Before returning the function makes sure that the canary has the same value. If the canary has been altered, the program is terminated with a fatal Stack smashing detected error.\n\nAnother technique used by operating systems is restricting code evaluation on the stack. When the stack overflow is exploited, hackers will try to overwrite the return address so that it points at the buffer location with the malicious code injected. Even if the exact address is not known, it is possible to construct a NO-OP-instruction slide in the stack buffer so that a jump at any address within this slide will lead to malicious code execution. Exact stack addresses are typically different after every program run because operating systems push environmental variables onto it.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/call-stack-buffer-overflow/"
    },{
      "image": {"path":"/assets/img/blog/extended-euklidian-code.jpg","srcset":{"911w":"/assets/img/blog/extended-euklidian-code.jpg","455w":"/assets/img/blog/extended-euklidian-code@0,5x.jpg","227w":"/assets/img/blog/extended-euklidian-code@0,25x.jpg","113w":"/assets/img/blog/extended-euklidian-code@0,125x.jpg"}},
      "title": "Extended Euclidean algorithm without stack or recursion",
      "date": "2020-02-21 00:00:00 +0100",
      
      "content": "Typical implementation of the extended Euclidean algorithm on the internet will just iteratively calculate modulo until 0 is reached. However, sometimes you also need to calculate the linear combination coefficients for the greatest common divisor.\n\nExtended Euclidean algorithm\n\nThe extended Euclidean algorithm allows us not only to calculate the gcd (greatest common divisor) of 2 numbers, but gives us also a representation of the result in a form of a linear combination:\n\ngcd⁡(a,b)=u⋅a+v⋅bu,v∈Z\\gcd(a, b) = u \\cdot a + v \\cdot b \\quad u,v \\in \\mathbb{Z}gcd(a,b)=u⋅a+v⋅bu,v∈Z\n\ngcd of more than 2 numbers can always be done by iteratively calculating the gcd of 2 numbers.\n\nFor example, let’s calculate gcd⁡(14,5)\\gcd(14, 5)gcd(14,5):\n\n14=5⋅2+45=4⋅1+14=1⋅4+0\\begin{aligned}\n14 &amp;= 5 \\cdot 2 + 4 \\\\\n5 &amp;= 4 \\cdot 1 + 1 \\\\\n4 &amp;= 1 \\cdot 4 + 0\n\\end{aligned}1454​=5⋅2+4=4⋅1+1=1⋅4+0​\n\nSo the greatest common divisor of 141414 and 555 is 111.\n\nWe can find the linear combination coefficients by writing 111 in terms of 141414 and 555:\n\n1=5−4⋅1=5−(14−5⋅2)⋅1=5−14+5⋅2=3⋅5+(−1)⋅14\\begin{aligned}\n1 &amp;= 5 - 4 \\cdot 1 \\\\\n&amp;= 5 - (14 - 5 \\cdot 2) \\cdot 1 \\\\\n&amp;= 5 - 14 + 5 \\cdot 2 \\\\\n&amp;= 3 \\cdot 5 + (-1) \\cdot 14\n\\end{aligned}1​=5−4⋅1=5−(14−5⋅2)⋅1=5−14+5⋅2=3⋅5+(−1)⋅14​\n\nSo in this case u=3u = 3u=3 and v=−1v = -1v=−1:\n\ngcd⁡(14,5)=(−1)⋅14+3⋅5=1\\gcd(14, 5) = (-1) \\cdot 14 + 3 \\cdot 5 = 1gcd(14,5)=(−1)⋅14+3⋅5=1\n\nWe can calculate the linear combination coefficients by doing back substitution. But it is not so easy to implement this without recursion, because the back substitution is done when we are climbing out of the recursive calls. We will implement the algorithm recursively first.\n\nRecursive implementation\n\nThe formula\n\ngcd⁡(a,b)={b,if a=0gcd⁡(b mod a,a),otherwise\\gcd(a, b) =\n\t\\begin{cases}\n\tb, &amp; \\text{if}\\ a = 0 \\\\\n\t\\gcd(b \\bmod a, a), &amp; \\text{otherwise}\n\t\\end{cases}gcd(a,b)={b,gcd(bmoda,a),​if a=0otherwise​\n\nallows us to describe the algorithm in a functional way:\n\n\n  If a=0a = 0a=0, then the greatest common divisor is bbb. Coefficients u=0u = 0u=0 and v=0v = 0v=0.\n  Else, we make the problem simpler by calculating gcd⁡(b mod a,a)\\gcd(b \\bmod a, a)gcd(bmoda,a). We can calculate the new coefficients based on the coefficients of the simpler problem.\n\n\nSo, how can we calculate uuu and vvv so that\n\ngcd⁡(a,b)=u⋅a+v⋅b\\gcd(a, b) = u \\cdot a + v \\cdot bgcd(a,b)=u⋅a+v⋅b\n\nby knowing u′u&#x27;u′ and v′v&#x27;v′ with:\n\ngcd⁡(b mod a,a)=u′⋅(b mod a)+v′⋅a\\gcd(b \\bmod a, a) = u&#x27; \\cdot (b \\bmod a) + v&#x27; \\cdot agcd(bmoda,a)=u′⋅(bmoda)+v′⋅a\n\nIn order to do that we can write b mod ab \\bmod abmoda in terms of initial aaa and bbb:\n\ngcd⁡(b mod a,a)=u′⋅(b mod a)+v′⋅a=u′⋅(b−⌊ba⌋⋅a)+v′⋅a=u′⋅b−u′⋅⌊ba⌋⋅a+v′⋅a=(v′−u′⋅⌊ba⌋)⋅a+u′⋅b\\begin{aligned}\n\t\\gcd(b \\bmod a, a)\n    \t&amp;= u&#x27; \\cdot (b \\bmod a) + v&#x27; \\cdot a \\\\\n    \t&amp;= u&#x27; \\cdot (b - \\left\\lfloor \\frac{b}{a} \\right\\rfloor \\cdot a) + v&#x27; \\cdot a \\\\\n    \t&amp;= u&#x27; \\cdot b - u&#x27; \\cdot \\left\\lfloor \\frac{b}{a} \\right\\rfloor \\cdot a + v&#x27; \\cdot a \\\\\n    \t&amp;= (v&#x27; - u&#x27; \\cdot \\left\\lfloor \\frac{b}{a} \\right\\rfloor) \\cdot a + u&#x27; \\cdot b\n\\end{aligned}gcd(bmoda,a)​=u′⋅(bmoda)+v′⋅a=u′⋅(b−⌊ab​⌋⋅a)+v′⋅a=u′⋅b−u′⋅⌊ab​⌋⋅a+v′⋅a=(v′−u′⋅⌊ab​⌋)⋅a+u′⋅b​\n\nSo the new linear combination coefficients are:\n\nu=v′−u′⋅⌊ba⌋v=u′\\begin{aligned}\n    u &amp;= v&#x27; - u&#x27; \\cdot \\left\\lfloor \\frac{b}{a} \\right\\rfloor \\\\\n    v &amp;= u&#x27;\n\\end{aligned}uv​=v′−u′⋅⌊ab​⌋=u′​\n\nWith this formula we are now ready to implement the algorithm:\n\ndef extended_gcd(a: int, b: int) -&gt; (int, int, int):\n    if a == 0:\n        return b, 0, 1\n    (g, u, v) = extended_gcd(b % a, a)\n    new_u = v - (b // a) * u\n    new_v = u\n    return g, new_u, new_v\n\n\nNon-recursive implementation\n\nThe recursion in the algorithm above cannot be easily eliminated because the function is not tail-recursive.\n\nIn order to implement the algorithm with a loop we need to define a sequence of division remainders and then update the coefficients as we calculate the remainders. Formally, we can define a finite sequence rnr_nrn​:\n\nr1=ar2=brn+2=rn mod rn+1\\begin{aligned}\nr_1 &amp;= a \\\\\nr_2 &amp;= b \\\\\nr_{n+2} &amp;= r_n \\bmod r_{n+1}\n\\end{aligned}r1​r2​rn+2​​=a=b=rn​modrn+1​​\n\nIf rn+1=0r_{n+1} = 0rn+1​=0, rn+2r_{n+2}rn+2​ is not defined. We can write each rnr_nrn​ as a linear combination of uuu and vvv. Now we are interested in how uuu and vvv change as we calculate remainders. To do this formally, we will need to define two new finite sequences unu_nun​ and vnv_nvn​ which will represent the linear combination coefficients:\n\nrn=un⋅a+vn⋅br_n = u_n \\cdot a + v_n \\cdot brn​=un​⋅a+vn​⋅b\n\nBy definition, r1=ar_1  = ar1​=a and r2=br_2 = br2​=b, so we can directly write the linear combination coefficients for r1r_1r1​ and r2r_2r2​:\n\nu1=1v1=0u2=0v2=1\\begin{aligned}\n    u_1 &amp;= 1 \\\\\n    v_1 &amp;= 0 \\\\\n    u_2 &amp;= 0 \\\\\n    v_2 &amp;= 1\n\\end{aligned}u1​v1​u2​v2​​=1=0=0=1​\n\nLet qnq_nqn​ be the finite sequence of integer divisions in rnr_nrn​:\n\nrn=rn+1⋅qn+2+rn+2r_n = r_{n+1} \\cdot q_{n+2} + r_{n+2}rn​=rn+1​⋅qn+2​+rn+2​\n\nNow we can write unu_nun​ and vnv_nvn​ in terms of qnq_nqn​:\n\nrn+2=rn−rn+1⋅qn+2=un⋅a+vn⋅b−rn+1⋅qn+2=un⋅a+vn⋅b−(un+1⋅a+vn+1⋅b)⋅qn+2=un⋅a+vn⋅b−un+1⋅a⋅qn+2−vn+1⋅b⋅qn+2=(un−un+1⋅qn+2)⋅a+(vn−vn+1⋅qn+2)⋅b\\begin{aligned} \n    r_{n+2} &amp;= r_n - r_{n+1} \\cdot q_{n+2} \\\\\n    &amp;= u_n \\cdot a + v_n \\cdot b - r_{n+1} \\cdot q_{n+2} \\\\\n    &amp;= u_n \\cdot a + v_n \\cdot b - (u_{n+1} \\cdot a + v_{n+1} \\cdot b) \\cdot q_{n+2} \\\\\n    &amp;= u_n \\cdot a + v_n \\cdot b - u_{n+1} \\cdot a \\cdot q_{n+2} - v_{n+1} \\cdot b \\cdot q_{n+2} \\\\\n    &amp;= (u_n - u_{n+1} \\cdot q_{n+2}) \\cdot a + (v_n - v_{n+1} \\cdot q_{n+2}) \\cdot b\n\\end{aligned}rn+2​​=rn​−rn+1​⋅qn+2​=un​⋅a+vn​⋅b−rn+1​⋅qn+2​=un​⋅a+vn​⋅b−(un+1​⋅a+vn+1​⋅b)⋅qn+2​=un​⋅a+vn​⋅b−un+1​⋅a⋅qn+2​−vn+1​⋅b⋅qn+2​=(un​−un+1​⋅qn+2​)⋅a+(vn​−vn+1​⋅qn+2​)⋅b​\n\nTo get the formula for unu_nun​ and vnv_nvn​ we can just substitute nnn instead of n+2n + 2n+2:\n\nun=un−2−qn⋅un−1vn=vn−2−qn⋅vn−1\\begin{aligned}\n    u_n &amp;= u_{n-2} - q_n \\cdot u_{n-1} \\\\\n    v_n &amp;= v_{n-2} - q_n \\cdot v_{n-1}\n\\end{aligned}un​vn​​=un−2​−qn​⋅un−1​=vn−2​−qn​⋅vn−1​​\n\nWith this formula and the initial values of the unu_nun​ and vnv_nvn​ sequences we can now implement the extended Euclidean algorithm without recursion:\n\ndef extended_gcd(a: int, b: int) -&gt; (int, int, int):\n    if a == 0:\n        # The algorithm will work correctly without this check\n        # But it will take one iteration of the inner loop\n        return b, 0, 1\n\n    un_prev = 1\n    vn_prev = 0\n    un_cur = 0\n    vn_cur = 1\n\n    while b != 0:\n        # Calculate new element of the qn sequence\n        qn = a // b\n\n        # Calculate new element of the rn sequence\n        new_r = a % b\n        a = b\n        b = new_r\n\n        # Calculate new coefficients with the formula above\n        un_new = un_prev - qn * un_cur\n        vn_new = vn_prev - qn * vn_cur\n\n        # Shift coefficients\n        un_prev = un_cur\n        vn_prev = vn_cur\n        un_cur = un_new\n        vn_cur = vn_new\n\n    return a, un_prev, vn_prev\n\n\nExample\n\nWe can visualize the finite sequences we defined and see how the algorithm works with a table. We will calculate gcd⁡(104,47)\\gcd(104, 47)gcd(104,47) and it’s linear combination coefficients:\n\ngcd⁡(104,47)=u⋅104+v⋅47\\gcd(104, 47) = u \\cdot 104 + v \\cdot 47gcd(104,47)=u⋅104+v⋅47\n\n\n  \n    \n      rnr_nrn​\n      qnq_nqn​\n      unu_nun​\n      vnv_nvn​\n    \n  \n  \n    \n      104\n      -\n      1\n      0\n    \n    \n      47\n      -\n      0\n      1\n    \n    \n      10\n      2\n      1\n      -2\n    \n    \n      7\n      4\n      -4\n      9\n    \n    \n      3\n      1\n      5\n      -11\n    \n    \n      1\n      2\n      -14\n      31\n    \n    \n      0\n      3\n      33\n      20\n    \n  \n\n\nAt each step we first calculate the next element from the qnq_nqn​ sequence and then use it to calculate new linear combination coefficients unu_nun​ and vnv_nvn​.\n\nThe result of the algorithm:\n\ngcd⁡(104,47)=−14⋅104+31⋅47=1\\gcd(104, 47) = -14 \\cdot 104 + 31 \\cdot 47 = 1gcd(104,47)=−14⋅104+31⋅47=1\n\nImprovement of the non-recusive solution\n\nAs we see in the example above, we don’t need to calculate the last row of the table because we aren’t interested in the linear combination that forms zero. We can terminate the algorithm directly after calculating the new element of the rnr_nrn​ sequence:\n\ndef extended_gcd(a: int, b: int) -&gt; (int, int, int):\n    if a == 0: # Optional check\n        return b, 0, 1\n\n    if b == 0: # Without this check the first iteration will divide by zero\n        return a, 1, 0\n\n    un_prev = 1\n    vn_prev = 0\n    un_cur = 0\n    vn_cur = 1\n\n    while True:\n        qn = a // b\n        new_r = a % b\n        a = b\n        b = new_r\n\n        if b == 0:\n            return a, un_cur, vn_cur\n\n        # Update coefficients\n        un_new = un_prev - qn * un_cur\n        vn_new = vn_prev - qn * vn_cur\n\n        # Shift coefficients\n        un_prev = un_cur\n        vn_prev = vn_cur\n        un_cur = un_new\n        vn_cur = vn_new\n\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/non-recursive-extended-euklidean-algorithm/"
    },{
      "image": {"path":"/assets/img/blog/numbers-are-regular-languages-tmb.jpg","srcset":{"1920w":"/assets/img/blog/numbers-are-regular-languages-tmb.jpg","960w":"/assets/img/blog/numbers-are-regular-languages-tmb@0,5x.jpg","480w":"/assets/img/blog/numbers-are-regular-languages-tmb@0,25x.jpg","240w":"/assets/img/blog/numbers-are-regular-languages-tmb@0,125x.jpg"}},
      "title": "Numbers in congruence classes are regular languages",
      "date": "2020-08-29 00:00:00 +0200",
      
      "content": "In this post we will consider natural Radix-b numbers in positional number systems. The congruence class of any such arbitrary natural number can be determined by a finite automata, and thus, intuitively speaking, the language of all Radix-b numbers that satisfy some fixed properties modulo b is regular.\n\nRadix-b numbers divisible by m\n\nThe key idea behind the automation construction is that after reading a digit, the new congruence class depends only on the current read digit and on the previous congruence class. There are only m&lt;∞m &lt; \\inftym&lt;∞ congruence classes so we can construct a finite automata that accepts all numbers divisible by m:\n\nA=(Z,Σ,δ,z0,E)Z:={0,…,m−1}Σ:={0,…,b−1}E:={0}z0:=0δ(r,d):=r⋅b+d mod m\\begin{aligned}\nA &amp;= (Z,\\Sigma,\\delta, z_0, E) \\\\\nZ &amp;:= \\{0,\\dots, m-1\\} \\\\\n\\Sigma &amp;:= \\{0,\\dots, b-1\\} \\\\\nE &amp;:= \\{0\\} \\\\\nz_0 &amp;:= 0 \\\\\n\\delta(r,d) &amp;:= r \\cdot b + d \\bmod{m}\n\\end{aligned}AZΣEz0​δ(r,d)​=(Z,Σ,δ,z0​,E):={0,…,m−1}:={0,…,b−1}:={0}:=0:=r⋅b+dmodm​\n\nThe correctness of the transition function can be seen as follows: when the automation is in state rrr, the number that has been read is in the congruence class rrr modulo mmm. So we can write it as mk+rmk+rmk+r. By multiplying with bbb (i.e. shift the number by 1 digit to the left) and adding ddd we get the new congruence class:\n\nδ(r,d)=(mk+r)⋅b+d mod m=mkb+rb+d mod m=rb+d mod m\\begin{aligned}\n\\delta(r,d) &amp;= (mk + r)\\cdot b + d \\bmod{m} \\\\\n&amp;= mkb + rb + d \\bmod{m} \\\\\n&amp;= rb + d \\bmod{m}\n\\end{aligned}δ(r,d)​=(mk+r)⋅b+dmodm=mkb+rb+dmodm=rb+dmodm​\n\nOf course, by altering the set of final states EEE we can accept not only multiples of mmm, but any such number nnn with n mod m∈En \\bmod{m} \\in Enmodm∈E for any E⊆ZE \\subseteq ZE⊆Z.\n\nBinary numbers modulo 4\n\nFor example, we can construct a finite automation that accepts all binary (b=2b = 2b=2) numbers divisible by 4 (m=4m = 4m=4):\n\n\n\nThe transitions are obtained as follows:\n\n0⋅2 mod 4=0  ⟹  δ(0,0)=0,δ(0,1)=11⋅2 mod 4=2  ⟹  δ(1,0)=2,δ(1,1)=32⋅2 mod 4=0  ⟹  δ(2,0)=0,δ(2,1)=13⋅2 mod 4=2  ⟹  δ(3,0)=2,δ(3,1)=3\\begin{aligned}\n0 \\cdot 2 \\bmod{4} = 0 &amp;\\implies \\delta(0, 0) = 0, \\delta(0, 1) = 1 \\\\\n1 \\cdot 2 \\bmod{4} = 2 &amp;\\implies \\delta(1, 0) = 2, \\delta(1, 1) = 3 \\\\\n2 \\cdot 2 \\bmod{4} = 0 &amp;\\implies \\delta(2, 0) = 0, \\delta(2, 1) = 1 \\\\\n3 \\cdot 2 \\bmod{4} = 2 &amp;\\implies \\delta(3, 0) = 2, \\delta(3, 1) = 3\n\\end{aligned}0⋅2mod4=01⋅2mod4=22⋅2mod4=03⋅2mod4=2​⟹δ(0,0)=0,δ(0,1)=1⟹δ(1,0)=2,δ(1,1)=3⟹δ(2,0)=0,δ(2,1)=1⟹δ(3,0)=2,δ(3,1)=3​\n\nIn this particular case the constructed automation is not minimal. States s1 and s3 are equivalent (this can be formally proven with the Myhill-Nerode theorem) and can be replaced by one state:\n\n\n\nAs languages accepted by DFA’s are exactly the regular languages, we can transform any DFA in a regular expression to see the exact structure of numbers divisible, for example, by 4. Unfortunately such regular expressions are very long when constructed with the Kleene or with the Arden method by a computer. These regular expressions must be massively simplified in order to be readable. With the Kleene construction I’ve implemented in this project we get the following regular expression:\n\nε|0|(ε|0)0*(ε|0)|(1|(ε|0)0*1)(1|0*1)*0*(ε|0)|(1|(ε|0)0*1)(1|0*1)*0((1|00*1)(1|0*1)*0)*(0|00*(ε|0)|(1|00*1)(1|0*1)*0*(ε|0))\n\n\nIt can be simplified by computer heuristics down to:\n\nε|0|1(1|01)*00|(0|ε|1(1|01)*00)(0|1(1|01)*00)*(0|ε|1(1|01)*00)\n\n\nStill, the regular expression is not so readable. By transforming it further by hand, we can prove that it is equivalent to:\n\nε|0|1(1|01)*00|(0|ε|1(1|01)*00)(0|1(1|01)*00)*(0|ε|1(1|01)*00) =\nε|0|1(1|01)*00|(ε|0|1(1|01)*00)(0|1(1|01)*00)*(ε|0|1(1|01)*00) =\nε|0|1(1|01)*00|(0|1(1|01)*00)*(ε|0|1(1|01)*00) =\nε|0|1(1|01)*00|(0|1(1|01)*00)* =\nε|0|(0|1(1|01)*00)* =\nε|0|(0|1(1|0)*00)* =\nε|0|0*(1(1|0)*000*)* =\nε|0|0*(1(1|0)*0*00)* =\nε|0|0*(1(1|0)*00)* =\nε|0|0*(1(0|1)*00)* =\nε|0|0*(1(0|1)*00|ε) =\nε|0|0*|0*1(0|1)*00 =\nε|0|0*|0*(0|1)*00 =\nε|0|0*(0|1)*00 =\nε|0|(0|1)*00\n\n\nSo, any binary number divisible by 4 must be zero or end with 00.\n\nOther examples\n\nDecimal numbers divisible by 20\n\n\n\nDecimal numbers divisible by 75\n\n\n\nHexadecimal numbers divisible by 24\n\n\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/numbers-are-regular-languages/"
    },{
      "image": {"path":"/assets/img/blog/compressing-congruence-automata-tmb.jpg","srcset":{"1920w":"/assets/img/blog/compressing-congruence-automata-tmb.jpg","960w":"/assets/img/blog/compressing-congruence-automata-tmb@0,5x.jpg","480w":"/assets/img/blog/compressing-congruence-automata-tmb@0,25x.jpg","240w":"/assets/img/blog/compressing-congruence-automata-tmb@0,125x.jpg"}},
      "title": "Efficient compression of congruence class automations",
      "date": "2020-09-01 00:00:00 +0200",
      
      "content": "As already discussed in the previous post, any radix-b number nnn with n mod m∈E⊆Zn \\bmod{m} \\in E \\subseteq Znmodm∈E⊆Z can be accepted by finite automata in a digit-by-digit manner. However, the construction is not always optimal. The amount of states required is not always the amount of congruence classes. In this post we will examine when exactly the finite automata can be simplified by combining states. Reducing the amount of states will also help produce a much simpler regular expression, for example, with the Kleene construction.\n\nGeneral way to optimize an automation\n\nOf course, we can optimize the DFA with the well-known algorithm based on the Myhill-Nerode theorem. Unfortunately, even an optimized implementation is not efficient enough to do the optimization for large mmm and bbb, because the complexity of the algorithm expressed in these terms is O(b⋅m2)O(b \\cdot m^2)O(b⋅m2), as the algorithm needs to iterate over all m2m^2m2 pairs of states and consider all the outgoing transitions (bbb in each state). With this tool we can run the algorithm and get the following results for b=10b = 10b=10 and 1≤m≤201 \\le m \\le 201≤m≤20:\n\nm= 1 |Z|= 1  Z = [[0]]\nm= 2 |Z|= 2  Z = [[0], [1]]\nm= 3 |Z|= 3  Z = [[0], [1], [2]]\nm= 4 |Z|= 3  Z = [[0], [3, 1], [2]]\nm= 5 |Z|= 2  Z = [[0], [2, 1, 3, 4]]\nm= 6 |Z|= 4  Z = [[0], [4, 1], [5, 2], [3]]\nm= 7 |Z|= 7  Z = [[0], [1], [2], [3], [4], [5], [6]]\nm= 8 |Z|= 5  Z = [[0], [5, 1], [6, 2], [7, 3], [4]]\nm= 9 |Z|= 9  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8]]\nm=10 |Z|= 2  Z = [[0], [5, 4, 3, 2, 7, 6, 9, 8, 1]]\nm=11 |Z|=11  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\nm=12 |Z|= 7  Z = [[0], [7, 1], [8, 2], [9, 3], [10, 4], [11, 5], [6]]\nm=13 |Z|=13  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12]]\nm=14 |Z|= 8  Z = [[0], [8, 1], [9, 2], [10, 3], [11, 4], [12, 5], [13, 6], [7]]\nm=15 |Z|= 4  Z = [[0], [13, 10, 7, 4, 1], [5, 2, 8, 11, 14], [9, 6, 12, 3]]\nm=16 |Z|= 9  Z = [[0], [9, 1], [10, 2], [11, 3], [12, 4], [13, 5], [14, 6], [15, 7], [8]]\nm=17 |Z|=17  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16]]\nm=18 |Z|=10  Z = [[0], [10, 1], [11, 2], [12, 3], [13, 4], [14, 5], [15, 6], [16, 7], [17, 8], [9]]\nm=19 |Z|=19  Z = [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18]]\nm=20 |Z|= 3  Z = [[0], [13, 11, 15, 17, 5, 3, 7, 9, 19, 1], [12, 10, 14, 16, 4, 2, 6, 8, 18]]\n\n\nHere, ZZZ is the set of states in the new, minimal DFA. ZZZ is a set of equivalence classes that represent which states can be combined into one state. These equivalence classes have been computed for E:={0}E := \\{0\\}E:={0}. That is why, for example, for m=8m = 8m=8 states 0 and 4 aren’t in one equivalence class. They can’t be equivalent, because state 4 is intermediate while state 0 is final.\n\nComputing equivalence classes ahead-of-time\n\nAs you can see, there is clearly a pattern in the equivalence states above. For example, we can see that no states are equivalent when mmm is prime. By further examining the data above we can even conclude that the DFA probably can’t be simplified if gcd⁡(b,m)=1\\gcd(b, m) = 1gcd(b,m)=1 i.e. if bbb and mmm are coprime.\n\nAfter formally examining when the states can be combined I came up with the following result:\n\nTheorem\n\nIf m≤bm \\le bm≤b and E≠∅E \\neq \\varnothingE​=∅, then the minimum finite automation can be constructed as follows:\n\nZ=⋃A∈Z′⋅{A∩E,A\\E}\\{∅}Z = \\overset{\\cdot}{\\bigcup_{A\\in Z&#x27;}} \\{A \\cap E, A \\backslash E\\}\\backslash\\{\\varnothing\\}Z=A∈Z′⋃​⋅​{A∩E,A&lt;/span&gt;E}&lt;/span&gt;{∅}\n\nwhere Z′Z&#x27;Z′ is the set of equivalence classes of the following equivalence relation:\n\nx∼y⇔x≡ymod  mgcd⁡(b,m)x \\sim y \\Leftrightarrow x \\equiv y \\mod{\\frac{m}{\\gcd(b, m)}}x∼y⇔x≡ymodgcd(b,m)m​\n\nProof\n\nBy definition of δ\\deltaδ, we know that:\n\nδ(r,d)=k  ⟹  δ(r,d+1 mod b)=k+1 mod m\\delta(r, d) = k \\implies \\delta(r, d + 1 \\bmod b) = k + 1 \\bmod mδ(r,d)=k⟹δ(r,d+1modb)=k+1modm\n\nBecause m≤bm \\le bm≤b, it follows that the mapping δ(x,Σ)\\delta(x, \\Sigma)δ(x,Σ) is surjective. In other words, m≤bm \\le bm≤b implies that every state has at least one transition to any other state. Any change to this mapping will shift it. As E≠∅E \\neq \\varnothingE​=∅, any shifted mapping δ′\\delta&#x27;δ′ will lead to at least 1 digit ddd such that δ(x,d)∈E\\delta(x, d) \\in Eδ(x,d)∈E and δ′(x,d)∉E\\delta&#x27;(x, d) \\notin Eδ′(x,d)∈/​E.\n\nThus, 2 states x,y∈Zx,y \\in Zx,y∈Z are equivalent if and only if they have equivalent mappings which is equivalent to:\n\nδ(x,Σ)=δ(y,Σ)⇔δ(x,d)=δ(y,d)∀d∈Σ⇔xb+d mod m=yb+d mod m⇔xb+d≡yb+dmod  m⇔xb≡ybmod  m⇔xb−yb≡0mod  m⇔(x−y)⋅b≡0mod  m⇔x−y≡0mod  mgcd⁡(b,m)⇔x≡ymod  mgcd⁡(b,m)\\begin{aligned}\n\\delta(x, \\Sigma) = \\delta(y, \\Sigma) &amp;\\Leftrightarrow \\delta(x, d) = \\delta(y, d) \\quad \\forall d \\in \\Sigma \\\\\n&amp;\\Leftrightarrow xb + d \\bmod m = yb + d \\bmod m \\\\\n&amp;\\Leftrightarrow xb + d \\equiv yb + d \\mod{m} \\\\\n&amp;\\Leftrightarrow xb \\equiv yb \\mod{m} \\\\\n&amp;\\Leftrightarrow xb - yb \\equiv 0 \\mod{m} \\\\\n&amp;\\Leftrightarrow (x-y) \\cdot b \\equiv 0 \\mod{m} \\\\\n&amp;\\Leftrightarrow x-y \\equiv 0 \\mod{\\frac{m}{\\gcd(b, m)}} \\\\\n&amp;\\Leftrightarrow x \\equiv y \\mod{\\frac{m}{\\gcd(b, m)}}\n\\end{aligned}δ(x,Σ)=δ(y,Σ)​⇔δ(x,d)=δ(y,d)∀d∈Σ⇔xb+dmodm=yb+dmodm⇔xb+d≡yb+dmodm⇔xb≡ybmodm⇔xb−yb≡0modm⇔(x−y)⋅b≡0modm⇔x−y≡0modgcd(b,m)m​⇔x≡ymodgcd(b,m)m​​\n\nFirst corollary\n\nIt follows, that if m≤bm \\le bm≤b, E≠∅E \\neq \\varnothingE​=∅ and gcd⁡(b,m)=1\\gcd(b, m) = 1gcd(b,m)=1, then no states can be combined and the minimal automation has mmm states.\n\nSecond corollary\n\nIf m≤bm \\le bm≤b and E≠∅E \\neq \\varnothingE​=∅, then the amount of states Z′Z&#x27;Z′ (before splitting each equivalence class in final states and non final ones as shown in the theorem) is equal to the following set of orbits:\n\nZ′=(Z/m)/⟨m/gcd⁡(b,m)‾⟩Z&#x27; = (\\mathbb{Z}/m)/\\langle \\overline{m / \\gcd(b, m)} \\rangleZ′=(Z/m)/⟨m/gcd(b,m)​⟩\n\nExample implementation\n\nThis Java method uses the results above to compute the equivalence classes efficiently:\n\npublic static List&lt;List&lt;Integer&gt;&gt; computeEquivalenceClasses(int b, int m, HashSet&lt;Integer&gt; finalStates) {\n    assert m &lt;= b;\n    assert !finalStates.isEmpty();\n\n    // Union-Find datastructure, preferrably with union-by-rank and path compression\n    UnionFind equivalenceClasses = new UnionFind(m);\n\n    int bmgcd = Euklidian.gcd(b, m);\n\n    if (bmgcd == 1) {\n        return equivalenceClasses.getDisjointSets();\n    }\n\n    int optimizedM = m / bmgcd;\n\n    // loop through every set in the new set of equivalence classes\n    for (int anchor = 0; anchor &lt; optimizedM; anchor++) {\n\n        int current = anchor; // [anchor] is the equivalence class\n        // oppositeAnchor is the element that is final if anchor is intermediate\n        // and intermediate if anchor is final\n        // if oppositeAnchor == anchor, then it is considered as not yet found\n        int oppositeAnchor = anchor;\n\n        boolean anchorFinal = finalStates.contains(anchor);\n\n        for (;;) {\n            current = (current + optimizedM) % m;\n\n            if (current == anchor) {\n                break;\n            }\n\n            boolean currentFinal = finalStates.contains(current);\n\n            if (currentFinal == anchorFinal) {\n                equivalenceClasses.union(current, anchor);\n            }\n            else if (oppositeAnchor == anchor) {\n                // \"initialize\" oppositeAnchor\n                oppositeAnchor = current;\n            }\n            else {\n                equivalenceClasses.union(current, oppositeAnchor);\n            }\n        }\n\n    }\n\n    return equivalenceClasses.getDisjointSets();\n}\n\n\nIf we use a union-find datastructure with union in O(1)O(1)O(1) (e.g. union-by-rank with path compression), then the complexity of the algorithm is O(m)O(m)O(m) which is much better than O(b⋅m2)O(b \\cdot m^2)O(b⋅m2).\n\nCase when m &gt; b\n\nIn case m&gt;bm &gt; bm&gt;b the equivalence relation depends very much on the set of final states EEE. With the Myhill Nerode theorem we get:\n\nx∼y⇔(xc∈L⇔yc∈L∀c∈Σ∗)⇔(x⋅b∣c∣+c mod m∈E⇔y⋅b∣c∣+c mod m∈E)\\begin{aligned}\nx \\sim y &amp;\\Leftrightarrow (xc \\in L \\Leftrightarrow yc \\in L \\quad \\forall c \\in \\Sigma^*) \\\\\n&amp;\\Leftrightarrow (x \\cdot b^{|c|} + c \\bmod m \\in E \\Leftrightarrow y \\cdot b^{|c|} + c \\bmod m \\in E)\n\\end{aligned}x∼y​⇔(xc∈L⇔yc∈L∀c∈Σ∗)⇔(x⋅b∣c∣+cmodm∈E⇔y⋅b∣c∣+cmodm∈E)​\n\nConclusion\n\nWith the above theorem it is easy to implement a much faster algorithm that computes equivalent states. It also gives an intuition, why for example it is harder to test whether a decimal number is divisible by 9 than to test whether a decimal number is divisible by 5, if we measure “hardness” by the amount of states in the minimal DFA. It is the case because gcd⁡(5,10)=5\\gcd(5, 10) = 5gcd(5,10)=5 and gcd⁡(9,10)=1\\gcd(9, 10) = 1gcd(9,10)=1.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/compressing-congruence-automata/"
    },{
      "image": {"path":"/assets/img/blog/kadane-algorithm-tmb.jpg","srcset":{"1280w":"/assets/img/blog/kadane-algorithm-tmb.jpg","640w":"/assets/img/blog/kadane-algorithm-tmb@0,5x.jpg","320w":"/assets/img/blog/kadane-algorithm-tmb@0,25x.jpg","160w":"/assets/img/blog/kadane-algorithm-tmb@0,125x.jpg"}},
      "title": "Kadane's algorithm and the idea behind it",
      "date": "2020-09-19 00:00:00 +0200",
      
      "content": "People often refer to Kadane’s algorithm only in the context of the maximum subarray problem. However, the idea behind this algorithm allows one to use it for solving a variety of problems that have something to do with finding a continuous subarray with a given property. The algorithm can also be used in 2d or multidimensional arrays but in this post we will only consider regular one-dimensional arrays.\n\nMotivation &amp; idea\n\nIf we are searching for a subarray with some property, the easiest solution would be to try all possible intervals. A naive algorithm can do that in O(n3)O(n^3)O(n3) time, but we can improve that by computing solutions for intervals of increasing length and reusing already computed solutions (aka dynamic programming). In this case the complexity of the algorithm is O(n2)O(n^2)O(n2). Of course, this will work not for all problems. It will work, for example, for the computation of any binary associative operation for all intervals in the array. Another example for such an algorithm would be finding all palindromes in a given string:\n\n\n  Mark all letters as palindromes of length 1\n  Mark all adjacent equal letters as palindromes of length 2\n  For every fixed length kkk, starting with k=3k = 3k=3 traverse all substrings of length kkk in the string and mark the current substring as a palindrome if the letters at the start and at the end match and if the middle part is a palindrome.\n\n\nO(n2)O(n^2)O(n2) is the optimal worst-case complexity if the problem cannot be solved without traversing the entire search space. For example, this is the case for the problem of computing all palindromes of a given string (a worst-case example is an∈Σ∗a^n \\in \\Sigma^*an∈Σ∗ where the amount of palindromes is ∑i=1ni=n⋅(n+1)2∈Ω(n2)\\sum_{i=1}^{n}i =\\frac{n \\cdot (n + 1)}{2} \\in \\Omega(n^2)∑i=1n​i=2n⋅(n+1)​∈Ω(n2)).\n\nThe key question is: How can we traverse only some subset of the search space and still benefit from dynamic programming?\n\nWell, we can identify a problem with only one side of the interval in the array. We will choose the right side of the interval because that is what is commonly used in real algorithms. That way it is possible to construct a linear time algorithm the following way:\n\n\n  Create a recursive function f(k)f(k)f(k) that computes the trivial solution if k=1k = 1k=1 and computes the new solution based on the already computed result, e.g. f(k−1)f(k-1)f(k−1).\n  Find the best solution among {f(k):0≤k&lt;n}\\{f(k) : 0 \\le k &lt; n\\}{f(k):0≤k&lt;n}.\n\n\nBoth parts take linear (O(n)O(n)O(n)) time if we cache and reuse the result of f(k)f(k)f(k) or if we use dynamic programming which is better most of the times.\n\nMaximum sum subarray\n\nA good example for a construction of such an algorithm is the maximum sum subarray problem - given an array AAA of integers the algorithm should find such indices a,b∈{1,…,n−1}a,b \\in \\{1, \\dots, n-1\\}a,b∈{1,…,n−1} with a≤ba \\le ba≤b such that ∑i=abA[i]\\sum_{i=a}^b {A[i]}∑i=ab​A[i] is maximal.\n\nWe can define the problem only in terms of the right border kkk - “what is the maximum subarray ending at kkk”?\n\nClearly, the maximum subarray of the whole array is the maximum of subarrays ending at kkk for all 0≤k&lt;n0 \\le k &lt; n0≤k&lt;n. Also, the maximum sum subarray ending at k is either A[k]A[k]A[k] itself or the maximum sum subarray ending at k−1k - 1k−1 combined with A[k]A[k]A[k]. So we can now write the algorithm formally with pseudocode:\n\nmaxSumSubarrayEndingAt(A, k) {\n    if (k == 0) {\n        return (A[0], 1);\n    }\n    (sum, left) = maxSumSubarrayEndingAt(k - 1);\n    if (sum &gt;= 0) { /* sum + A[k] &gt;= A[k] */\n        /* the max subarray (ending at k) is the previous subarray together with this element */\n        return (sum + A[k], left);\n    }\n    else {\n        /* the max subarray (ending at k) is the current element */\n        return (A[k], k);\n    }\n}\n\n\nAnd the maximum subarray sum and indices can be computed with a simple maximim-search algorithm:\n\nmaxSumSubarray(A) {\n    n = size(A);\n    maxSum = A[0];\n    maxLeft = 0;\n    maxRight = 0;\n    for (r = 1; r &lt; n; r++) {\n        (s, l) = maxSumSubarrayEndingAt(A, r);\n        if (s &gt; maxSum) {\n            maxSum = s;\n            maxLeft = l;\n            maxRight = r;\n        }\n    }\n    return (maxSum, maxLeft, maxRight);\n}\n\n\nThis intermediate pseudocode solution doesn’t cache maxSumSubarrayEndingAt results and is therefore inefficient “as-is”. But we can remove recursion and rewrite the same solution with dynamic programming (Java):\n\npublic static void maxSumSubarray(int[] a) {\n    int maxSum = a[0];\n    int maxLeft = 0;\n    int maxRight = 0;\n\n    int currentSum = a[0];\n    int currentLeft = 0;\n\n    for (int r = 1; r &lt; a.length; r++) {\n\n        if (currentSum &gt;= 0) {\n            currentSum += a[r];\n        }\n        else {\n            currentSum = a[r];\n            currentLeft = r;\n        }\n\n        if (currentSum &gt; maxSum) {\n            maxSum = currentSum;\n            maxLeft = currentLeft;\n            maxRight = r;\n        }\n    }\n\n    // the sum between A[maxLeft] and A[maxRight] (inclusive) is equal maxSum and maximal\n    System.out.printf(\"Max sum: %5d Indices from %5d to %5d.\\n\", maxSum, maxLeft, maxRight);\n}\n\n\nThis is the efficient Θ(n)\\Theta(n)Θ(n) time and Θ(1)\\Theta(1)Θ(1) space solution that uses the idea of Kadane’s algorithm to compute the maximum sum subarray.\n\nExample\n\nConsider the array {-2, 1, -3, 4, -1, 2, 1, -5, -2, 5}. By running the algorithm we get the following maximum subarrays ending at a specific index:\n\n\n\nThe maximum subarray is marked red.\n\nOther problems that can be solved analogously\n\n\n  Smallest sum subarray problem\n  Largest product subarray problem\n  Smallest product subarray problem\n  Maximum circular sum\n\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/kadane-algorithm/"
    },{
      "image": {"path":"/assets/img/blog/lowest-common-ancestor-tmb.jpg","srcset":{"1920w":"/assets/img/blog/lowest-common-ancestor-tmb.jpg","960w":"/assets/img/blog/lowest-common-ancestor-tmb@0,5x.jpg","480w":"/assets/img/blog/lowest-common-ancestor-tmb@0,25x.jpg","240w":"/assets/img/blog/lowest-common-ancestor-tmb@0,125x.jpg"}},
      "title": "Computing the lowest common ancestor in a full binary tree",
      "date": "2020-09-23 00:00:00 +0200",
      
      "content": "The lowest common ancestor (LCA) problem is important in many applications of binary trees. For example, by knowing the lowest common ancestor we can easily compute the shortest path between any two vertices in a tree. The most common way to compute the lca of vertices uuu and vvv is to iteratively go up until we get to the root of the subtree containing both uuu and vvv. This method works only if uuu and vvv are on the same level. If not, we can first measure the difference of heights ddd between uuu and vvv and then find the lowest common ancestor of the ddd-th parent of the lowest vertex and the higher vertex.\n\nLCA in a full binary tree\n\nFull binary trees are often represented implicitly in memory. That means, parent-child relations are determined by looking at the positions of nodes in the array. For example, in this tree the nodes are numbered the way they will be positioned in the array:\n\n\n\nIn a full binary tree the parent of node xxx is p(x):=⌊x−12⌋p(x):=\\left\\lfloor\\frac{x-1}{2}\\right\\rfloorp(x):=⌊2x−1​⌋, and children are 2⋅x+12 \\cdot x + 12⋅x+1 and 2⋅x+22 \\cdot x + 22⋅x+2. Formally speaking, for nodes aaa and bbb on the same level we would like to find the value of pn(a)p^n(a)pn(a) such that pn(a)=pn(b)p^n(a) = p^n(b)pn(a)=pn(b) for n minimal. In binary representation, dividing by 2 and discarding the remainder is equivalent to shifting the number by 1 bit to the right. In order to use this we can just add 1 to each node index:\n\n\n\nWith this change we can implement the lca algorithm:\n\nuint32_t lca_sameLevel(uint32_t a, uint32_t b) {\n    while (a != b) {\n        a &gt;&gt;= 1u;\n        b &gt;&gt;= 1u;\n    }\n    return a;\n}\n\n\nThis algorithm works because the lowest common ancestor is the longest common prefix of the binary representation of both nodes.\n\nIf we address nodes starting with one, then any nodewith level lll will be greater than or equal to 2l2^l2l. Therefore, the amount of leading zeroes in the binary representation of nodes in the same level is equal. Moreover, the difference of amounts of leading zeroes is equal to the difference of levels. With this idea we can now implement the algorithm that correctly finds the lowest common ancestor for any pair of nodes.\n\nuint32_t lca(uint32_t a, uint32_t b) {\n    \n    uint32_t aLeadingZeroes = __builtin_clz(a);\n    uint32_t bLeadingZeroes = __builtin_clz(b);\n    \n    while (aLeadingZeroes &gt; bLeadingZeroes) {\n        b &gt;&gt;= 1u;\n        bLeadingZeroes++;\n    }\n    \n    while (bLeadingZeroes &gt; aLeadingZeroes) {\n        a &gt;&gt;= 1u;\n        aLeadingZeroes++;\n    }\n    \n    while (a != b) {\n        a &gt;&gt;= 1u;\n        b &gt;&gt;= 1u;\n    }\n    \n    return a;\n}\n\n\nIt is also easy to modify the algorithm slightly so that it works for elements indexed starting from zero.\n\n#include &lt;stdint.h&gt;\n\nuint32_t lca(uint32_t a, uint32_t b) {\n    a++;\n    b++;\n    \n    uint32_t aLeadingZeroes = __builtin_clz(a);\n    uint32_t bLeadingZeroes = __builtin_clz(b);\n    \n    while (aLeadingZeroes &gt; bLeadingZeroes) {\n        b &gt;&gt;= 1u;\n        bLeadingZeroes++;\n    }\n    \n    while (bLeadingZeroes &gt; aLeadingZeroes) {\n        a &gt;&gt;= 1u;\n        aLeadingZeroes++;\n    }\n    \n    while (a != b) {\n        a &gt;&gt;= 1u;\n        b &gt;&gt;= 1u;\n    }\n    \n    return a - 1u;\n}\n\n\nComplexity: O(log⁡(n))O(\\log(n))O(log(n)) where nnn is the amount if nodes in the tree.\n\nExample\n\nIf the we want to find the lowest common ancestor of 8 and 10 (indexed from zero), then we add 1 to both of them and find the longest common prefix. In this case it is the longest common prefix of 1001 and 1011 which is 10 = 2 in decimal. 2 is the lowest common ancestor in the tree with increased indices, so in the original tree the lowest common ancestor is 2 - 1 = 1.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/lowest-common-ancestor/"
    },{
      "image": {"path":"/assets/img/blog/regular-language-size-np-hard-tmb.jpg","srcset":{"1520w":"/assets/img/blog/regular-language-size-np-hard-tmb.jpg","760w":"/assets/img/blog/regular-language-size-np-hard-tmb@0,5x.jpg","380w":"/assets/img/blog/regular-language-size-np-hard-tmb@0,25x.jpg","190w":"/assets/img/blog/regular-language-size-np-hard-tmb@0,125x.jpg"}},
      "title": "Measuring the size of a regular language is NP-Hard",
      "date": "2020-10-15 00:00:00 +0200",
      
      "content": "In this post we will examine an interesting connection between the NP\\mathcal{NP}NP-complete CNF-SAT problem and the problem of computing the amount of words generated by some efficient representation of a formal language. Here, I call a way of representing a formal language efficient, if it has a polynomially-long encoding but can possibly describe an exponentially-large formal language. Examples of such efficient representations are nondeteministic finite automations and regular expressions.\n\nFormal definition of the problem\n\nWe can formally define the problem of determining the language size the following way:\n\nName: NFA Language Size\n\nInput: A nondeterministic finite automata (NFA) M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) and a number m∈Nm \\in \\mathbb{N}m∈N.\n\nQuestion: Is ∣L(M)∣≤m\\st L(M) \\st \\le m∣L(M)∣≤m?\n\nProof of NP-hardness\n\nWe will construct a reduction from the NP\\mathcal{NP}NP-complete CNF-SAT problem. Consider a formula φ\\varphiφ with kkk clauses and nnn variables:\n\n\n  Create an NFA M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) with the alphabet Σ:={0,1}\\Sigma := \\{0, 1\\}Σ:={0,1}.\n  For every variable 0≤v≤n0 \\le v \\le n0≤v≤n and clause 1≤c≤k1 \\le c \\le k1≤c≤k create a state z(v,c)∈Zz(v, c) \\in Zz(v,c)∈Z. Intuitively, if the NFA is in the state z(v,c)z(v, c)z(v,c), it means that it has already read vvv symbols of some variable assignment where clause ccc is zero.\n  For each clause 1≤c≤k1 \\le c \\le k1≤c≤k of the formula, construct a boolean cube ψ:V→{0,1,∗}\\psi : V \\to \\{0, 1, *\\}ψ:V→{0,1,∗} with ψ−1(1)\\psi^{-1}(1)ψ−1(1) containing variables that are negated in the clause and ψ−1(0)\\psi^{-1}(0)ψ−1(0) containing positive variables (other variables are mapped to ∗*∗). By doing this, we essentially construct a cube C(ψ):=(⋀ψ(x)=1x)∧(⋀ψ(x)=0xˉ)C(\\psi) := (\\bigwedge_{\\psi(x) = 1}{x}) \\wedge (\\bigwedge_{\\psi(x) = 0}{\\bar{x}})C(ψ):=(⋀ψ(x)=1​x)∧(⋀ψ(x)=0​xˉ) that is the negation of the clause. In other words, all full variable assignments ψ′:V→{0,1}\\psi&#x27; : V \\to \\{0, 1\\}ψ′:V→{0,1} such that ψ(x)≠∗⇒ψ′(x)=ψ(x)∀x∈V\\psi(x) \\neq * \\Rightarrow \\psi&#x27;(x) = \\psi(x) \\quad\\forall x \\in Vψ(x)​=∗⇒ψ′(x)=ψ(x)∀x∈V will make the clause (and the whole formula φ\\varphiφ) false. For all 0≤v&lt;n0 \\le v &lt; n0≤v&lt;n create the following transitions:\n    \n      Set δ(z(v,c),0):={z(v+1,c)}\\delta(z(v,c), 0) := \\{z(v + 1,c)\\}δ(z(v,c),0):={z(v+1,c)} if ψ(v+1)=0\\psi(v + 1) = 0ψ(v+1)=0.\n      Set δ(z(v,c),1):={z(v+1,c)}\\delta(z(v,c), 1) := \\{z(v + 1,c)\\}δ(z(v,c),1):={z(v+1,c)} if ψ(v+1)=1\\psi(v + 1) = 1ψ(v+1)=1.\n      Set δ(z(v,c),0):={z(v+1,c)}\\delta(z(v,c), 0) := \\{z(v + 1,c)\\}δ(z(v,c),0):={z(v+1,c)} and δ(z(v,c),1):={z(v+1,c)}\\delta(z(v,c), 1) := \\{z(v + 1,c)\\}δ(z(v,c),1):={z(v+1,c)} if ψ(v+1)=∗\\psi(v + 1) = *ψ(v+1)=∗.\n    \n  \n  Mark all states with v=0v = 0v=0 as initial: S:={z(0,c):1≤c≤k}S := \\{z(0, c) : 1 \\le c \\le k\\}S:={z(0,c):1≤c≤k}.\n  Mark all states with v=nv = nv=n as final: E:={z(n,c):1≤c≤k}E := \\{z(n, c) : 1 \\le c \\le k\\}E:={z(n,c):1≤c≤k}.\n\n\nBy construction, MMM will accept any variable assignment a1,…,an∈Σna_1,\\dots,a_n \\in \\Sigma^na1​,…,an​∈Σn where f(a1,…,an)=0f(a_1, \\dots, a_n) = 0f(a1​,…,an​)=0. As φ\\varphiφ has exactly 2n2^n2n variable assignments, it is satisfiable if and only if the set of accepted variable assignments L(M)L(M)L(M) has at most m:=2n−1m := 2^n - 1m:=2n−1 elements.\n\nThis reduction can clearly be done in polynomial time.\n\nIntuition &amp; Example\n\nConsider the following function:\n\nφ:=(cˉ+d)(aˉ+c+dˉ)(aˉ+bˉ+d)\\varphi := (\\bar{c} + d)(\\bar{a} + c + \\bar{d})(\\bar{a} + \\bar{b} + d)φ:=(cˉ+d)(aˉ+c+dˉ)(aˉ+bˉ+d)\n\nThe cubes that describe variable assignments where φ\\varphiφ is false are **10, 1*01 and 11*0 (variable order: a,b,c,da, b, c, da,b,c,d). These three cubes make the first, second and the third clauses false, respectively.\n\nWe can visualize all possible variable assignments with a 4-dimensional cube (as φ\\varphiφ has 4 variables):\n\n\n\nIn this cube, every edge connects assignments that differ in exactly one bit (i.e. hamming distance = 1). Any lower dimensional cubes that are subgraphs of this cube are implicants if the vertices in the subgraph cover only assignments where the function is true. If a lower dimensional cube isn’t a part of some higher dimensional cube that still covers only assignments where the function is true, such a cube is a prime implicant. In this case, if we think of CNF-clauses as implicants of the negated function, then we can visualize them the following way:\n\n\n\nThe idea of the reduction is that if we can count the amount of vertices covered by these cubes, then we can compare this amount to the total number of vertices and if it is less than 2n2^n2n where nnn is the number of variables, then the function is satisfiable, otherwise not. The problem is that implicants aren’t always disjoint. So, the satisfiability problem is essentially the problem of comparing 2n2^n2n with the size of the union of variable assignments described by cubes.\n\nThese variable assignments that are parts of some cube(s) can be accepted with a nondeterministic finite automata (NFA) with nnn states. We can create such an NFA for each clause and then union them by marking multiple states as initial. In this example, we get the following NFA:\n\n\n\nThe top row of the NFA accepts variable assignments generated by the 11*0 cube, the middle row corresponds to the 1*01 cube and the bottom one - to **10. φ\\varphiφ is satisfiable if and only if there is at least one word of length 4, such that this NFA doesn’t accept it.\n\nConverting the NFA to a BDD\n\nThe idea of this reduction can be used to compute all the satisfying assignments of φ\\varphiφ. Consider the example above - we can apply the Rabin-Scott algorithm to convert the NFA to a DFA:\n\n\n\nThe computed DFA will always be acyclic (without the ∅\\varnothing∅-node), because the initial NFA had no cycles in it. The satisfying assignments are the ones that are not accepted by the NFA. Therefore, they are exactly the paths of length nnn, leading to ∅\\varnothing∅. A program can easily output all satisfying assignments with a DFS or BFS-search in this tree.\n\nIf we replace ∅\\varnothing∅ with the positive leaf and all final states with a negative leaf (the transitions after them can be removed), then the graph will become an ordered binary decision diagram (OBDD) of φ\\varphiφ:\n\n\n\nOf course, the nodes should be renamed to variable names:\n\n\n\nThe Rabin-Scott algorithm doesn’t always output minimal deterministic automations and therefore in most cases the OBDD will also not be minimal, like in this case where the redundant checks are clearly seen. In order to get a ROBDD we will still need to apply the elimination and the isomorphism rules (remove redundant checks and reuse subtrees):\n\n\n\nNP-Complete special case\n\nWe can tweak the problem defined above to make it belong to NP\\mathcal{NP}NP:\n\nName: NFA Rejected m-string\n\nInput: A nondeterministic finite automata (NFA) M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) and a unary-encoded number m∈Nm \\in \\mathbb{N}m∈N.\n\nQuestion: Exists such a string α∈Σ∗\\alpha \\in \\Sigma^*α∈Σ∗ of length mmm such that α∉L(M)\\alpha \\notin L(M)α∈/​L(M)?\n\nThis problem is in NP\\mathcal{NP}NP, because a string of length mmm can be used as a certificate. Then, by using the idea of the Rabin-Scott theorem, we can compute all potential states after mmm transitions and therefore test whether the given string is rejected or not in time in O(∣Z∣2⋅m)O(\\st Z \\st^2 \\cdot m)O(∣Z∣2⋅m), which is polynomial in the length of the input. The NP\\mathcal{NP}NP-hardness can be shown with a reduction from CNF-SAT as follows:\n\n\n  Consider a formula φ\\varphiφ with kkk clauses and nnn variables.\n  Construct an NFA M=(Z,Σ,δ,S,E)M = (Z, \\Sigma, \\delta, S, E)M=(Z,Σ,δ,S,E) by following the same steps as in the proof of NP\\mathcal{NP}NP-hardness of NFA Language Size.\n  Set m:=nm := nm:=n.\n\n\nφ\\varphiφ is satisfiable if and only if there is a rejected string of length mmm in L(M)L(M)L(M) which is exactly some satisfying assignment for φ\\varphiφ. Clearly, this is a polynomial-time reduction.\n\nFinding any rejected string is NP-Complete\n\nWe can also define another version of the problem and proof it’s NP\\mathcal{NP}NP-completeness:\n\nName: NFA Rejected String\n\nInput: A nondeterministic finite automata (NFA) M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) and a unary-encoded number m∈Nm \\in \\mathbb{N}m∈N.\n\nQuestion: Exists such a string α∈Σ∗\\alpha \\in \\Sigma^*α∈Σ∗ with ∣α∣≤m\\st \\alpha \\st \\le m∣α∣≤m such that α∉L(M)\\alpha \\notin L(M)α∈/​L(M)?\n\nThis problem is in NP\\mathcal{NP}NP, because a string of length mmm can be used as a certificate, like in the NFA Rejected m-string problem.\n\nWe will prove NP\\mathcal{NP}NP-hardness by reducing NFA Rejected m-string to this problem. Consider an NFA M:=(Z,Σ,δ,S,E)M := (Z, \\Sigma, \\delta, S, E)M:=(Z,Σ,δ,S,E) and a number m′∈Nm&#x27; \\in \\mathbb{N}m′∈N:\n\n\n  Create mmm new nodes q1,…,qm∈Zq_1, \\dots, q_m \\in Zq1​,…,qm​∈Z.\n  For all 1≤s&lt;m1 \\le s &lt; m1≤s&lt;m and a∈Σa \\in \\Sigmaa∈Σ, set δ(qs,a):={qs+1}\\delta(q_s, a) := \\{q_{s+1}\\}δ(qs​,a):={qs+1​}.\n  Mark q1q_1q1​ as initial: S:=S∪{q1}S := S \\cup \\{q_1\\}S:=S∪{q1​}.\n  Mark q1,…,qmq_1, \\dots, q_mq1​,…,qm​ as final: E:=E∪{qi:1≤i≤m}E := E \\cup \\{q_i : 1 \\le i \\le m\\}E:=E∪{qi​:1≤i≤m}.\n  Set m:=m′m := m&#x27;m:=m′.\n\n\nBy construction, the new NFA will accept all strings of length m−1m - 1m−1 or less. Thus, there is a rejected string of length mmm if and only if there is a rejected string of length at most mmm in the new NFA. Obviously, this is a polynomial-time reduction.\n\nA few words about complexity\n\nThese problems illustrate an interesting connection between NP\\mathcal{NP}NP-complete problems, that can be solved in polynomial time by nondeterministic turing machines and the problem of just counting the language size described by an NFA. By the Rabin-Scott theorem, it is possible to convert any NFA to an equivalent DFA, but the worst-case complexity of the algorithm is Θ(2n)\\Theta(2^n)Θ(2n), because a set with nnn elements has 2n2^n2n subsets and all of these subsets will be reachable in the worst-case. Would the complexity of the subset-construction be polynomial, then it would mean that P=NP\\mathcal{P}  = \\mathcal{NP}P=NP as we can just search for all paths in the DFA that lead to ∅\\varnothing∅ after exactly nnn transitions (these paths are exactly the variable assignments that satisfy φ\\varphiφ).\n\nThe reduction discussed above can also be done in reverse. Any set of cubes can be transformed to a boolean function in conjunctive normal form. So, if P=NP\\mathcal{P} = \\mathcal{NP}P=NP and the CNF-SAT problem is solvable in polynomial time, then it is also possible to compare 2n2^n2n with the size of the union of some arbitrary boolean cubes. The cube union size problem is essentially a special case of the inclusion–exclusion principle, where the formula to compute the size of a union of nnn sets is also exponentially long, because the amount of possible intersections grows exponentially.\n\nIt seems impossible to compute the size of the union of some arbitrary cubes in polynomial time, because the variable assignments that some cube describes is exponential in the length of the encoding of the cube. And the amount of ways to intersect some cubes is also exponential. Any polynomial encoding for a cube will allow us to distinguish between an exponential amount of cubes. However, the amount of ways to intersect some kkk nnn-variable-cubes is in Ω(3(3n⋅(k−1)))\\Omega(3^{(3^{n \\cdot (k - 1)})})Ω(3(3n⋅(k−1))), so it is impossible to precisely encode the union of these kkk cubes with a polynomially long encoding. However, it could be possible to divide the search space so that it is still possible to compare 2n2^n2n with the size of the union of the cubes in polynomial time. Of course, these thoughts aren’t even close to a formal proof that P≠NP\\mathcal{P} \\neq \\mathcal{NP}P​=NP.\n",
      "categories": ["cs"],
      "tags": ["complexity","logic"],
      
      "collection": "posts",
      "url": "/blog/cs/regular-language-size-np-hard/"
    },{
      "image": {"path":"/assets/img/blog/palu-tmb.jpg","srcset":{"1280w":"/assets/img/blog/palu-tmb.jpg","640w":"/assets/img/blog/palu-tmb@0,5x.jpg","320w":"/assets/img/blog/palu-tmb@0,25x.jpg","160w":"/assets/img/blog/palu-tmb@0,125x.jpg"}},
      "title": "Using the PA=LU factorization to solve linear systems of equations for many right-hand sides efficiently",
      "date": "2020-11-28 00:00:00 +0100",
      
      "content": "Linear systems of equations come up in almost any technical discipline. The PA=LUPA=LUPA=LU factorization method is a well-known numerical method for solving those types of systems of equations against multiple input vectors. It is also possible to preserve numerical stability by implementing some pivot strategy. In this post we will consider performance and numerical stability issues and try to cope with them by using the PA=LUPA = LUPA=LU factorization.\n\nIdea behind the algorithm\n\nSuppose we have a matrix AAA and some set of vectors {b1,…,bn}\\{b_1, \\dots, b_n\\}{b1​,…,bn​} and we need to solve Ax=biAx = b_iAx=bi​ for all 1≤i≤n1 \\le i \\le n1≤i≤n. Of course, we can just run the Gaussian elimination algorithm for each vector bib_ibi​ and compute the upper-triangular form of the matrix AAA and then backward-substitute the vector bib_ibi​. However, if the set of vectors is large or the matrix itself is big, this is inefficient because we have to convert the matrix to upper-triangular form nnn times.\n\nWhile doing Gaussian elimination, all row operations are applied both to the matrix and the vector. But when we choose what operation should be applied in some step, we never consider the entries of the vector. The key idea behind the PA=LUPA = LUPA=LU factorization algorithm is to compute the upper-triangular form of the matrix once and save all steps that have been performed so that we can perform the same steps on the current vector bib_ibi​ without recomputing the upper-triangular form of AAA for backward substitution. All Gaussian elimination steps can be encoded in a matrix L−1L^{-1}L−1, such that if we multiply it by some vector bib_ibi​, L−1L^{-1}L−1 will apply all the operations performed while computing the upper-triangular form of AAA to bib_ibi​. After that we can just do backward substitution to find xxx.\n\nDuring Gaussian elimination it is possible that we are forced to swap some rows, for example if the pivot on the diagonal is zero. As we will see, it is also a good idea to swap rows not only when we are forced to do so. Anyway, in order for the algorithm to work with any matrix AAA, we need to store the permutations of the rows of AAA in some way. For simplicity, we will store the permutation in a permutation matrix PPP that essentially just swaps rows.\n\nSo, formally speaking, we are interested in finding such matrices PPP, LLL and UUU, such that PA=LUPA = LUPA=LU where PPP is the permutation matrix, LLL is a lower-triangular matrix and UUU is an upper triangular matrix. We need LLL and UUU to be triangular matrices in order to be able to quickly solve linear systems of equations with them.\n\nForward and backward substitution\n\nAs already mentioned above, we need to implement forward and backward substitution in order to proceed with the PA=LUPA = LUPA=LU factorization algorithm:\n\n# Solves a linear system of equations Lx = b where the L matrix is lower-triangular\ndef forward_substitution(L, b):\n    \n    m = len(b)\n    x = np.empty(m)\n    \n    for v in range(m):\n        if L[v][v] == 0:\n            # the value on the diagonal is zero\n            x[v] = 0\n            # the current variable's value is irrelevant\n            continue\n        # calculate v-th variable value\n        value = b[v]\n        # subtract linear combination of the already known variables\n        # in the bottom left corner of the matrix\n        for i in range(v):\n            value -= L[v][i] * x[i]\n        # divide by the coefficient by the v-th variable to get it's value\n        value /= L[v][v]\n        # store the value in the resulting vector\n        x[v] = value\n        \n    return x\n\n\nAnalogously, we can implement backward substitution:\n\n# Solves a linear system of equations Ux = b where the U matrix is upper-triangular\ndef backward_substitution(U, b):\n\n    m = len(b)\n    x = np.empty(m)\n    \n    for v in range(m - 1, -1, -1):\n        if U[v][v] == 0:\n            # the value on the diagonal is zero\n            x[v] = 0\n            continue\n        # calculate v-th variable value\n        value = b[v]\n        # subtract linear combination of the already known variables\n        # in the top right corner of the matrix\n        for i in range(v + 1, m, 1):\n            value -= U[v][i] * x[i]\n        # divide by the coefficient before the i-th variable to get it's value\n        value /= U[v][v]\n        # store the value in the resulting vector\n        x[v] = value\n        \n    return x\n\n\nSolutions for non-homogeneous systems of equations are affine subspaces. If the input matrix is full-rank, i.e. doesn’t reduce the dimension of the vector space during a linear transformation, then there will be only one solution vector and the algorithms will calculate it correctly. For the sake of simplicity these algorithms just set the corresponding variables to zero if there is a zero on the diagonal, so in case the matrix is not full-rank, the algorithms will return only one vector from a set of vectors. In order to get the whole solution affine subspace it is necessary to give back a parametrized set of vectors.\n\nPA=LU factorization\n\nThe factorization algorithm consists of the following steps:\n\n\n  Initialize UUU with the initial matrix AAA and let PPP and LLL be the identity matrices. We will transform UUU to upper-triangular form.\n  Find some non-zero pivot element in the current column of the matrix.\n  If the found pivot element is not on the diagonal, swap the corresponding rows in the UUU and PPP matrices. Also, apply the swap to the part of the LLL matrix under the main diagonal.\n  Subtract the row with the pivot on the diagonal from all rows underneath so that all element under the pivot element become zero. After U[i][j]U[i][j]U[i][j] has been eliminated by subtracting α⋅U[j]\\alpha \\cdot U[j]α⋅U[j] from row iii, set the coefficient L[i][j]:=αL[i][j] := \\alphaL[i][j]:=α such that, in other words, α=U[i][j]U[j][j]\\alpha = \\frac{U[i][j]}{U[j][j]}α=U[j][j]U[i][j]​.\n  Continue from step 2 with the next column.\n\n\nThe idea behind the adjustment of the LLL matrix in step 3 is to add α⋅Pivot row\\alpha \\cdot \\text{Pivot row}α⋅Pivot row to the row to be eliminated and thus reverse this step. We only need to calculate the elements under the diagonal of the LLL matrix because the row to be eliminated is always under the pivot row.\n\nExample\n\nConsider the following matrix:\n\nA:=(−116−4−8621623)A := \\left(\n\\begin{array}{ccc}\n-1  &amp; 1 &amp; 6 \\\\\n-4 &amp; -8 &amp; 6 \\\\\n2 &amp; 16 &amp; 23\n\\end{array}\n\\right)A:=⎝⎜⎛​−1−42​1−816​6623​⎠⎟⎞​\n\nWe can start computing the PA=LUPA = LUPA=LU factorization by writing the initial decomposition as follows:\n\n(100010001)⋅A=(100010001)⋅(−116−4−8621623)\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-1  &amp; 1 &amp; 6 \\\\\n-4 &amp; -8 &amp; 6 \\\\\n2 &amp; 16 &amp; 23\n\\end{array}\n\\right)⎝⎜⎛​100​010​001​⎠⎟⎞​⋅A=⎝⎜⎛​100​010​001​⎠⎟⎞​⋅⎝⎜⎛​−1−42​1−816​6623​⎠⎟⎞​\n\nFor the first column, let’s choose the pivot −4-4−4 in the second row. It is not on the diagonal, so we must swap the first two rows:\n\n(010100001)⋅A=(100010001)⋅(−4−86−11621623)\\left(\n\\begin{array}{ccc}\n    0 &amp; 1 &amp; 0 \\\\\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-4 &amp; -8 &amp; 6 \\\\\n-1  &amp; 1 &amp; 6 \\\\\n2 &amp; 16 &amp; 23\n\\end{array}\n\\right)⎝⎜⎛​010​100​001​⎠⎟⎞​⋅A=⎝⎜⎛​100​010​001​⎠⎟⎞​⋅⎝⎜⎛​−4−12​−8116​6623​⎠⎟⎞​\n\nNow the pivot is on the diagonal and it is time to subtract −1−4=14\\frac{-1}{-4} = \\frac{1}{4}−4−1​=41​ of the first row from the second row and 2−4=−12\\frac{2}{-4} = -\\frac{1}{2}−42​=−21​ of the first row from the third row. We write these coefficients to the first column of the LLL matrix, accordingly:\n\n(010100001)⋅A=(1001410−1201)⋅(−4−86034.501226)\\left(\n\\begin{array}{ccc}\n    0 &amp; 1 &amp; 0 \\\\\n    1 &amp; 0 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    \\frac{1}{4} &amp; 1 &amp; 0 \\\\\n    -\\frac{1}{2} &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-4 &amp; -8 &amp; 6 \\\\\n0  &amp; 3 &amp; 4.5 \\\\\n0 &amp; 12 &amp; 26\n\\end{array}\n\\right)⎝⎜⎛​010​100​001​⎠⎟⎞​⋅A=⎝⎜⎛​141​−21​​010​001​⎠⎟⎞​⋅⎝⎜⎛​−400​−8312​64.526​⎠⎟⎞​\n\nNow we proceed with the second column. Assume we choose 121212 as a pivot. 121212 is not on the diagonal, so we must swap the second row with the third one. This time we also need to swap the corresponding rows in the LLL matrix:\n\n(010001100)⋅A=(100−12101401)⋅(−4−8601226034.5)\\left(\n\\begin{array}{ccc}\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1 \\\\\n    1 &amp; 0 &amp; 0\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    -\\frac{1}{2} &amp; 1 &amp; 0 \\\\\n    \\frac{1}{4} &amp; 0 &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-4 &amp; -8 &amp; 6 \\\\\n0 &amp; 12 &amp; 26 \\\\\n0  &amp; 3 &amp; 4.5\n\\end{array}\n\\right)⎝⎜⎛​001​100​010​⎠⎟⎞​⋅A=⎝⎜⎛​1−21​41​​010​001​⎠⎟⎞​⋅⎝⎜⎛​−400​−8123​6264.5​⎠⎟⎞​\n\nWe are now ready to eliminate 333 from the third row by subtracting 312=14\\frac{3}{12} = \\frac{1}{4}123​=41​ of the second row from it:\n\n(010001100)⋅A=(100−121014141)⋅(−4−860122600−2)\\left(\n\\begin{array}{ccc}\n    0 &amp; 1 &amp; 0 \\\\\n    0 &amp; 0 &amp; 1 \\\\\n    1 &amp; 0 &amp; 0\n\\end{array}\n\\right)\n\\cdot\nA =\n\\left(\n\\begin{array}{ccc}\n    1 &amp; 0 &amp; 0 \\\\\n    -\\frac{1}{2} &amp; 1 &amp; 0 \\\\\n    \\frac{1}{4} &amp; \\frac{1}{4} &amp; 1\n\\end{array}\n\\right)\n\\cdot\n\\left(\n\\begin{array}{ccc}\n-4 &amp; -8 &amp; 6 \\\\\n0 &amp; 12 &amp; 26 \\\\\n0  &amp; 0 &amp; -2\n\\end{array}\n\\right)⎝⎜⎛​001​100​010​⎠⎟⎞​⋅A=⎝⎜⎛​1−21​41​​0141​​001​⎠⎟⎞​⋅⎝⎜⎛​−400​−8120​626−2​⎠⎟⎞​\n\nThis is the PA=LUPA = LUPA=LU decomposition of AAA.\n\nNon-stable implementation\n\nWe can implement the decomposition algorithm described above in Python the following way:\n\n# Compute the PA = LU decomposition of the matrix A\ndef plu(A):\n    m = len(A)\n    \n    P = np.identity(m) # create an identity matrix of size m\n    L = np.identity(m)\n    \n    for x in range(m):\n        \n        pivotRow = x\n        \n        if A[pivotRow][x] == 0:\n            # search for a non-zero pivot\n            for y in range(x + 1, m, 1):\n                if A[y][x] != 0:\n                    pivotRow = y\n                    break\n                            \n        if A[pivotRow][x] == 0:\n            # we didn't find any row with a non-zero leading coefficient\n            # that means that the matrix has all zeroes in this column\n            # so we don't need to search for pivots after all for the current column x\n            continue\n            \n        # did we just use a pivot that is not on the diagonal?\n        if pivotRow != x:\n            # so we need to swap the part of the pivot row after x including x\n            # with the same right part of the x row where the pivot was expected\n            for i in range(x, m, 1):\n                # swap the two values columnwise\n                (A[x][i], A[pivotRow][i]) = (A[pivotRow][i], A[x][i])\n            \n            # we must save the fact that we did this swap in the permutation matrix\n            # swap the pivot row with row x\n            P[[x, pivotRow]] = P[[pivotRow, x]]\n            \n            # we also need to swap the rows in the L matrix\n            # however, the relevant part of the L matrix is only the bottom-left corner\n            # and before x\n            for i in range(x):\n                (L[x][i], L[pivotRow][i]) = (L[pivotRow][i], L[x][i])\n            \n        # now the pivot row is x\n        # search for rows where the leading coefficient must be eliminated\n        for y in range(x + 1, m, 1):\n            currentValue = A[y][x]\n            if currentValue == 0:\n                # variable already eliminated, nothing to do\n                continue\n            \n            pivot = A[x][x]\n            assert pivot != 0 # just in case, we already made sure the pivot is not zero\n            \n            pivotFactor = currentValue / pivot\n            \n            # subtract the pivot row from the current row\n            A[y][x] = 0\n            for i in range(x + 1, m, 1):\n                A[y][i] -= pivotFactor * A[x][i]\n            \n            # write the pivot factor to the L matrix\n            L[y][x] = pivotFactor\n        \n        # the pivot must anyway be at the correct position if we found at least one\n        # non-zero leading coefficient in the current column x\n        assert A[x][x] != 0\n        for y in range(x + 1, m, 1):\n            assert A[y][x] == 0\n    \n    return (P, L, A)\n\n\nWe are now interested in solving Ax=bAx = bAx=b with the computed decomposition of AAA into PA=LUPA = LUPA=LU. It follows that LUx=PbLUx = PbLUx=Pb:\n\nPA=LU⇒PAx=LUx⇒LUx=PAx⇒LUx=Pb\\begin{aligned}\nPA = LU &amp;\\Rightarrow PAx = LUx \\\\\n&amp;\\Rightarrow LUx = PAx \\\\\n&amp;\\Rightarrow LUx = Pb\n\\end{aligned}PA=LU​⇒PAx=LUx⇒LUx=PAx⇒LUx=Pb​\n\nWe can reorder components of the vector bbb by multiplying it with PPP (let z:=Pbz := Pbz:=Pb) and then solve Ly=zLy = zLy=z for yyy using forward substitution (as LLL is lower-triangular). Intuitively, y:=Uxy := Uxy:=Ux is the vector with all the Gaussian elimination steps applied to it. After that, we can compute xxx by solving Ux=yUx = yUx=y using backward substitution (as UUU is upper-triangular).\n\nWe can implement these steps as follows:\n\ndef plu_solve(A, b):\n    (P, L, U) = plu(A)\n    b = np.matmul(P, b) # multiply matrix P with vector b\n    y = forward_substitution(L, b)\n    x = backward_substitution(U, y)\n    return x\n\n\nHowever, there it still a problem with the above implementation of plu. We can illustrate it by approximating the function f(x)=sin⁡(12⋅x)x2+1f(x) = \\frac{\\sin(12 \\cdot x)}{x^2 + 1}f(x)=x2+1sin(12⋅x)​ on the interval x∈[−1,1]x \\in [-1, 1]x∈[−1,1] with the PA=LUPA = LUPA=LU decomposition applied to a Vandermonde matrix:\n\n\n\nAs you can see, the approximation isn’t entirely right. The low-degree terms of the polynomial approximating f(x)f(x)f(x) are getting lost due to rounding errors in floating point arithmetic. These low-degree terms are exactly the ones that are responsible for approximating the right part of the graph and that’s why we see rounding errors there.\n\nNumerical stability\n\nThe issue that causes such precision problems lies in choosing the wrong pivot element. For example, consider the following matrix\n\nA:=(ε111)A := \\left(\n\\begin{array}{cc}\n\\varepsilon &amp; 1 \\\\\n1 &amp; 1\n\\end{array}\n\\right)A:=(ε1​11​)\n\nwhere ε&gt;0\\varepsilon &gt; 0ε&gt;0 is some very small number.\n\nIf we choose ε\\varepsilonε as a pivot, then we need to subtract the second row multiplied with 1ε\\frac{1}{\\varepsilon}ε1​ to perform an elimination step. As ε\\varepsilonε is very small, 1ε\\frac{1}{\\varepsilon}ε1​ is very big. So by choosing such a pivot, we not only preserve ε\\varepsilonε in the matrix but also add a very large number to it, like 1−1ε1 - \\frac{1}{\\varepsilon}1−ε1​ in this case.\n\nThere are many advanced strategies to cope with this problem, but one of the most easy and effective ways to improve numerical stability is to just choose the element with the greatest absolute value as the pivot.\n\nThus, we can improve the implementation by adjusting the code responsible for choosing the pivot:\n\ndef palu(A):\n    m = len(A)\n    \n    P = np.identity(m) # create an identity matrix of size m\n    L = np.identity(m)\n    \n    for x in range(m):\n        \n        pivotRow = x\n        \n        # search for the best pivot\n        for y in range(x + 1, m, 1):\n            if abs(A[y][x]) &gt; abs(A[pivotRow][x]):\n                pivotRow = y\n                            \n        if A[pivotRow][x] == 0:\n            # we didn't find any row with a non-zero leading coefficient\n            # that means that the matrix has all zeroes in this column\n            # so we don't need to search for pivots after all for the current column x\n            continue\n            \n        # did we just use a pivot that is not on the diagonal?\n        if pivotRow != x:\n            # so we need to swap the part of the pivot row after x including x\n            # with the same right part of the x row where the pivot was expected\n            for i in range(x, m, 1):\n                # swap the two values columnwise\n                (A[x][i], A[pivotRow][i]) = (A[pivotRow][i], A[x][i])\n            \n            # we must save the fact that we did this swap in the permutation matrix\n            # swap the pivot row with row x\n            P[[x, pivotRow]] = P[[pivotRow, x]]\n            \n            # we also need to swap the rows in the L matrix\n            # however, the relevant part of the L matrix is only the bottom-left corner\n            # and before x\n            for i in range(x):\n                (L[x][i], L[pivotRow][i]) = (L[pivotRow][i], L[x][i])\n            \n        # now the pivot row is x\n        # search for rows where the leading coefficient must be eliminated\n        for y in range(x + 1, m, 1):\n            currentValue = A[y][x]\n            if currentValue == 0:\n                # variable already eliminated, nothing to do\n                continue\n            \n            pivot = A[x][x]\n            assert pivot != 0 # just in case, we already made sure the pivot is not zero\n            \n            pivotFactor = currentValue / pivot\n            \n            # subtract the pivot row from the current row\n            A[y][x] = 0\n            for i in range(x + 1, m, 1):\n                A[y][i] -= pivotFactor * A[x][i]\n            \n            # write the pivot factor to the L matrix\n            L[y][x] = pivotFactor\n        \n        # the pivot must anyway be at the correct position if we found at least one\n        # non-zero leading coefficient in the current column x\n        assert A[x][x] != 0\n        for y in range(x + 1, m, 1):\n            assert A[y][x] == 0\n    \n    return (P, L, A)\n\n\nWith this implementation we get a much better approximation of f(x)f(x)f(x):\n\n\n\nUsing only one matrix to store the decomposition\n\nAs LLL is a lower-triangular matrix and UUU is upper-triangular, we can actually create only one matrix (apart from PPP) to store the decomposition. It also allows us to simplify the way we do swap operations:\n\ndef forward_substitution(L, b):\n    m = len(b)\n    x = np.empty(m)\n    \n    for v in range(m):\n        # calculate v-th variable value\n        value = b[v]\n        # subtract linear combination of the already known variables\n        # in the bottom left corner of the matrix\n        for i in range(v):\n            value -= L[v][i] * x[i]\n        # no need to divide by L[v][v] as it is implicitly 1, by construction\n        # (this 1 is not stored in the matrix)\n        # store the value in the resulting vector\n        x[v] = value\n        \n    return x\n\ndef plu(A):\n    m = len(A)\n    \n    P = np.identity(m)\n    \n    for x in range(m):\n        pivotRow = x\n        \n        # search for the best pivot\n        for y in range(x + 1, m, 1):\n            if abs(A[y][x]) &gt; abs(A[pivotRow][x]):\n                pivotRow = y\n                    \n        if A[pivotRow][x] == 0:\n            # we didn't find any row with a non-zero leading coefficient\n            # that means that the matrix has all zeroes in this column\n            # so we don't need to search for pivots after all for the current column x\n            continue\n            \n        # did we just use a pivot that is not on the diagonal?\n        if pivotRow != x:\n            # swap the pivot row with the current row in both A and L matrices\n            A[[x, pivotRow]] = A[[pivotRow, x]]\n            \n            # we must save the fact that we did this swap in the permutation matrix\n            # swap the pivot row with row x\n            P[[x, pivotRow]] = P[[pivotRow, x]]\n                        \n        # now the pivot row is x\n        # search for rows where the leading coefficient must be eliminated\n        for y in range(x + 1, m, 1):\n            currentValue = A[y][x]\n            if currentValue == 0:\n                # variable already eliminated, nothing to do\n                continue\n            \n            pivot = A[x][x]\n            assert pivot != 0 # just in case, we already made sure the pivot is not zero\n            \n            pivotFactor = currentValue / pivot\n            \n            # subtract the pivot row from the current row\n            for i in range(x + 1, m, 1):\n                A[y][i] -= pivotFactor * A[x][i]\n            \n            # write the pivot factor to the L matrix\n            A[y][x] = pivotFactor\n    \n    return (P, A)\n\ndef plu_solve(P, LU, b):\n    b = np.matmul(P, b)\n    y = forward_substitution(LU, b)\n    x = backward_substitution(LU, y)\n    return x\n\n\nIt is possible to further optimize the algorithms by removing the permutation matrix PPP and replacing it with a simple array that permutes it’s indices (or some other permutation datastructure).\n\nAs already mentioned in the beginning of the post, the key advantage of the PA=LUPA = LUPA=LU decomposition is that it allows us to solve linear systems of equations for many input vectors from B:={b1,…,bn}B := \\{b_1, \\dots, b_n\\}B:={b1​,…,bn​} by doing Gaussian elimination only once\n\n(P, LU) = plu(A)\nfor b in B:\n    solution = plu_solve(P, LU, b)\n\n\ninstead of doing full Gaussian elimination nnn times.\n\nLet mmm be the size of the vector and AAA be an m×mm \\times mm×m matrix. Then the complexity of computing the PA=LUPA = LUPA=LU factorization is O(m3)O(m^3)O(m3). If we optimize the permutation matrix so that permuting elements takes time in O(m2)O(m^2)O(m2), then the solving algorithm’s complexity is O(m2)O(m^2)O(m2).\n\nTherefore, the overall complexity of the algorithm that takes AAA and a set of nnn vectors BBB and solves Ax=bAx = bAx=b for every b∈Bb \\in Bb∈B (as shown in the code snippet above) is O(m3+n⋅m2)O(m^3 + n \\cdot m^2)O(m3+n⋅m2), which is better than O(n⋅m3)O(n \\cdot m^3)O(n⋅m3) if we apply Gaussian elimination nnn times.\n",
      "categories": ["cs"],
      "tags": ["linalg"],
      
      "collection": "posts",
      "url": "/blog/cs/pa-lu-factorization/"
    },{
      "image": {"path":"/assets/img/blog/linear-transformation-3d-tmb.jpg","srcset":{"1920w":"/assets/img/blog/linear-transformation-3d-tmb.jpg","960w":"/assets/img/blog/linear-transformation-3d-tmb@0,5x.jpg","480w":"/assets/img/blog/linear-transformation-3d-tmb@0,25x.jpg","240w":"/assets/img/blog/linear-transformation-3d-tmb@0,125x.jpg"}},
      "title": "Visualizing 3D linear transformations and Gaussian elimination with Python and Manim",
      "date": "2020-12-15 00:00:00 +0100",
      
      "content": "Matrices are omnipresent in linear algebra. Columns of a matrix describe where the corresponding basis vectors land relative to the initial basis. All transformed vectors are linear combinations of transformed basis vectors which are the columns of the matrix, this is also called linearity. Algorithms that operate on matrices essentially just alter the way vectors get transformed, preserving some properties. Unfortunately, many algorithms are typically presented to students in a numerical fashion, without describing the whole graphical meaning. In this post we will first visualize simple linear transformations and then we will visualize Gaussian Elimination (with row swaps) steps as a sequence of linear transformations. To do this, we will use Python together with a popular Open-Source library manim.\n\nVisualizing 3D transformations\n\nIn manim, there is a special ApplyMatrix animation that allows us to natively apply a matrix to every 3D vertex of the object.\n\nfrom manimlib.imports import *\n\nclass LinearTransformation3D(ThreeDScene):\n\n    CONFIG = {\n        \"x_axis_label\": \"$x$\",\n        \"y_axis_label\": \"$y$\",\n        \"basis_i_color\": GREEN,\n        \"basis_j_color\": RED,\n        \"basis_k_color\": GOLD\n    }\n\n    def create_matrix(self, np_matrix):\n\n        m = Matrix(np_matrix)\n\n        m.scale(0.5)\n        m.set_column_colors(self.basis_i_color, self.basis_j_color, self.basis_k_color)\n\n        m.to_corner(UP + LEFT)\n\n        return m\n\n    def construct(self):\n\n        M = np.array([\n            [2, 2, -1],\n            [-2, 1, 2],\n            [3, 1, -0]\n        ])\n\n        axes = ThreeDAxes()\n        axes.set_color(GRAY)\n        axes.add(axes.get_axis_labels())\n\n        self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES)\n\n        # basis vectors i,j,k\n        basis_vector_helper = TextMobject(\"$i$\", \",\", \"$j$\", \",\", \"$k$\")\n        basis_vector_helper[0].set_color(self.basis_i_color)\n        basis_vector_helper[2].set_color(self.basis_j_color)\n        basis_vector_helper[4].set_color(self.basis_k_color)\n\n        basis_vector_helper.to_corner(UP + RIGHT)\n\n        self.add_fixed_in_frame_mobjects(basis_vector_helper)\n\n        # matrix\n        matrix = self.create_matrix(M)\n\n        self.add_fixed_in_frame_mobjects(matrix)\n\n        # axes &amp; camera\n        self.add(axes)\n\n        self.begin_ambient_camera_rotation(rate=0.2)\n\n        cube = Cube(side_length=1, fill_color=BLUE, stroke_width=2, fill_opacity=0.1)\n        cube.set_stroke(BLUE_E)\n\n        i_vec = Vector(np.array([1, 0, 0]), color=self.basis_i_color)\n        j_vec = Vector(np.array([0, 1, 0]), color=self.basis_j_color)\n        k_vec = Vector(np.array([0, 0, 1]), color=self.basis_k_color)\n\n        i_vec_new = Vector(M @ np.array([1, 0, 0]), color=self.basis_i_color)\n        j_vec_new = Vector(M @ np.array([0, 1, 0]), color=self.basis_j_color)\n        k_vec_new = Vector(M @ np.array([0, 0, 1]), color=self.basis_k_color)\n\n        self.play(\n            ShowCreation(cube),\n            GrowArrow(i_vec),\n            GrowArrow(j_vec),\n            GrowArrow(k_vec),\n            Write(basis_vector_helper)\n        )\n\n        self.wait()\n\n        matrix_anim = ApplyMatrix(M, cube)\n\n        self.play(\n            matrix_anim,\n            Transform(i_vec, i_vec_new, rate_func=matrix_anim.get_rate_func(),\n                      run_time=matrix_anim.get_run_time()),\n            Transform(j_vec, j_vec_new, rate_func=matrix_anim.get_rate_func(),\n                      run_time=matrix_anim.get_run_time()),\n            Transform(k_vec, k_vec_new, rate_func=matrix_anim.get_rate_func(),\n                      run_time=matrix_anim.get_run_time())\n        )\n\n        self.wait()\n\n        self.wait(7)\n\n\nWith this implementation we can visualize, for example, the following matrix presented in the video (top left corner):\n\n\n\t\n\tYour browser does not support the video tag.\n\n\nWe can also visualize what it means for 2 columns to be linear dependent — the cube gets squashed into a plane:\n\n\n\t\n\tYour browser does not support the video tag.\n\n\nVisualizing Gaussian elimination\n\nIn the previous post we implemented an advanced version of the Gaussian elimination algorithm with pivoting (the PA=LUPA = LUPA=LU decomposition). We can analogously implement the pure Gaussian Elimination algorithm with pivoting the following way:\n\ndef gauss(a):\n    m = a.shape[0]\n\n    for x in range(m):\n        pivotRow = x\n\n        # search for the best pivot\n        for y in range(x + 1, m, 1):\n            if abs(a[y][x]) &gt; abs(a[pivotRow][x]):\n                pivotRow = y\n\n        if a[pivotRow][x] == 0:\n            # we didn't find any row with a non-zero leading coefficient\n            # that means that the matrix has all zeroes in this column\n            # so we don't need to search for pivots after all for the current column x\n            continue\n\n        # did we just use a pivot that is not on the diagonal?\n        if pivotRow != x:\n            # swap the pivot row with the current row in both A and L matrices\n            a[[x, pivotRow]] = a[[pivotRow, x]]\n            yield a\n\n        # now the pivot row is x\n        # search for rows where the leading coefficient must be eliminated\n        for y in range(x + 1, m, 1):\n            currentValue = a[y][x]\n            if currentValue == 0:\n                # variable already eliminated, nothing to do\n                continue\n\n            pivot = a[x][x]\n            assert pivot != 0  # just in case, we already made sure the pivot is not zero\n\n            pivotFactor = currentValue / pivot\n\n            # subtract the pivot row from the current row\n\n            a[y][x] = 0\n\n            for i in range(x + 1, m, 1):\n                a[y][i] -= pivotFactor * a[x][i]\n\n            yield a\n\n\nNotice that after every matrix operation we want to visualize I’ve added a yield statement. Generators in Python are perfect for visualizing algorithms, because they allow us to save the current state, yield the value and then continue the execution when the caller handled the received value. In this case, we yield every version of the matrix as the algorithm runs.\n\nNow we can adjust and extend the code used to render linear transformations to do this for every step of the gaussian elimination:\n\nclass Gauss3D(ThreeDScene):\n\n    CONFIG = {\n        \"x_axis_label\": \"$x$\",\n        \"y_axis_label\": \"$y$\",\n        \"basis_i_color\": GREEN,\n        \"basis_j_color\": RED,\n        \"basis_k_color\": GOLD\n    }\n\n    def create_matrix(self, np_matrix):\n\n        m = Matrix(np_matrix)\n\n        m.scale(0.5)\n        m.set_column_colors(self.basis_i_color, self.basis_j_color, self.basis_k_color)\n\n        m.to_corner(UP + LEFT)\n\n        return m\n\n    def construct(self):\n\n        M = np.array([\n            [-1.0, 1.0, -2.0],\n            [-4.0, -2.0, 1.0],\n            [-2.0, 2.0, 3.0]\n        ])\n\n        # axes\n        axes = ThreeDAxes()\n        axes.set_color(GRAY)\n        axes.add(axes.get_axis_labels())\n\n        self.set_camera_orientation(phi=55 * DEGREES, theta=-45 * DEGREES)\n\n        # basis vectors i,j,k\n        basis_vector_helper = TextMobject(\"$i$\", \",\", \"$j$\", \",\", \"$k$\")\n        basis_vector_helper[0].set_color(self.basis_i_color)\n        basis_vector_helper[2].set_color(self.basis_j_color)\n        basis_vector_helper[4].set_color(self.basis_k_color)\n\n        basis_vector_helper.to_corner(UP + RIGHT)\n\n        self.add_fixed_in_frame_mobjects(basis_vector_helper)\n\n        # matrix\n        matrix = self.create_matrix(M)\n\n        self.add_fixed_in_frame_mobjects(matrix)\n\n        # axes &amp; camera\n        self.add(axes)\n\n        self.begin_ambient_camera_rotation(rate=0.15)\n\n        cube = Cube(side_length=1, fill_color=BLUE, stroke_width=2, fill_opacity=0.1)\n        cube.set_stroke(BLUE_E)  # cube.set_stroke(TEAL_E)\n\n        i_vec = Vector(np.array([1, 0, 0]), color=self.basis_i_color)\n        j_vec = Vector(np.array([0, 1, 0]), color=self.basis_j_color)\n        k_vec = Vector(np.array([0, 0, 1]), color=self.basis_k_color)\n\n        i_vec_new = Vector(M @ np.array([1, 0, 0]), color=self.basis_i_color)\n        j_vec_new = Vector(M @ np.array([0, 1, 0]), color=self.basis_j_color)\n        k_vec_new = Vector(M @ np.array([0, 0, 1]), color=self.basis_k_color)\n\n        self.play(\n            ShowCreation(cube),\n            GrowArrow(i_vec),\n            GrowArrow(j_vec),\n            GrowArrow(k_vec),\n            Write(basis_vector_helper)\n        )\n\n        self.wait()\n\n        matrix_anim = ApplyMatrix(M, cube)\n\n        self.play(\n            matrix_anim,\n            ReplacementTransform(i_vec, i_vec_new, rate_func=matrix_anim.get_rate_func(),\n                                 run_time=matrix_anim.get_run_time()),\n            ReplacementTransform(j_vec, j_vec_new, rate_func=matrix_anim.get_rate_func(),\n                                 run_time=matrix_anim.get_run_time()),\n            ReplacementTransform(k_vec, k_vec_new, rate_func=matrix_anim.get_rate_func(),\n                                 run_time=matrix_anim.get_run_time())\n        )\n\n        self.wait()\n\n        i_vec, j_vec, k_vec = i_vec_new, j_vec_new, k_vec_new\n\n        self.wait(2)\n\n        for a in gauss(M):\n\n            a_rounded = np.round(a.copy(), 2)\n\n            self.remove(matrix)\n\n            matrix = self.create_matrix(a_rounded)\n\n            self.add_fixed_in_frame_mobjects(matrix)\n\n            # transformed cube\n            new_cube = Cube(side_length=1, fill_color=BLUE, stroke_width=2, fill_opacity=0.1)\n            new_cube.set_stroke(BLUE_E)\n\n            new_cube.apply_matrix(a)\n\n            # vectors\n            i_vec_new = Vector(a @ np.array([1, 0, 0]), color=self.basis_i_color)\n            j_vec_new = Vector(a @ np.array([0, 1, 0]), color=self.basis_j_color)\n            k_vec_new = Vector(a @ np.array([0, 0, 1]), color=self.basis_k_color)\n\n            # prepare and run animation\n            cube_anim = ReplacementTransform(cube, new_cube)\n\n            self.play(\n                cube_anim,\n                ReplacementTransform(i_vec, i_vec_new, rate_func=cube_anim.get_rate_func(),\n                                     run_time=cube_anim.get_run_time()),\n                ReplacementTransform(j_vec, j_vec_new, rate_func=cube_anim.get_rate_func(),\n                                     run_time=cube_anim.get_run_time()),\n                ReplacementTransform(k_vec, k_vec_new, rate_func=cube_anim.get_rate_func(),\n                                     run_time=cube_anim.get_run_time())\n            )\n\n            self.wait()\n\n            cube = new_cube\n            i_vec, j_vec, k_vec = i_vec_new, j_vec_new, k_vec_new\n\n            self.wait(1)\n\n        self.wait(1)\n\n\nWe can now see graphically how the column vectors get aligned as we bring the matrix to upper triangular form:\n\n\n\t\n\tYour browser does not support the video tag.\n\n",
      "categories": ["cs"],
      "tags": ["linalg"],
      
      "collection": "posts",
      "url": "/blog/cs/linear-transformations-3d-manim/"
    },{
      "image": {"path":"/assets/img/blog/kmp-tmb.jpg","srcset":{"1170w":"/assets/img/blog/kmp-tmb.jpg","585w":"/assets/img/blog/kmp-tmb@0,5x.jpg","292w":"/assets/img/blog/kmp-tmb@0,25x.jpg","146w":"/assets/img/blog/kmp-tmb@0,125x.jpg"}},
      "title": "Knuth-Morris-Pratt (KMP) algorithm explained",
      "date": "2020-12-20 00:00:00 +0100",
      
      "content": "Almost every program has to somehow process and transform strings. Very often we want to identify some fixed pattern (substring) in the string, or, formally speaking, find an occurrence of β:=b0b1…bn−1\\beta := b_0b_1 \\dots b_{n-1}β:=b0​b1​…bn−1​ in α:=a0a1…am−1≠ε\\alpha := a_0a_1 \\dots a_{m-1} \\neq \\varepsilonα:=a0​a1​…am−1​​=ε. A naive algorithm would just traverse α\\alphaα and check if β\\betaβ starts at the current character of α\\alphaα. Unfortunately, this approach leads us to an algorithm with a complexity of O(n⋅m)O(n \\cdot m)O(n⋅m). In this post we will discuss a more efficient algorithm solving this problem - the Knuth-Morris-Pratt (KMP) algorithm.\n\nThe Knuth-Morris-Pratt algorithm\n\nObviously, the substring search algorithm has to somehow compare both strings character-after-character. Suppose we are scanning the string from left to right and we found b0b_0b0​. Then we can start comparing further characters of β\\betaβ with the next characters of α\\alphaα. If the remaining characters match, we’ve found an occurrence of β\\betaβ in α\\alphaα. So the key question is what the algorithm should do if there is a mismatch during this comparison process, as shown in this example:\n\n\n\nIf the characters we are currently comparing don’t match, then we need to somehow shift the green, “matching” area of β\\betaβ. This green area that we expand if the characters match and shift otherwise is often referred to as the window. If we found a mismatch, it is possible that some suffix of the window together with the current character form a prefix of β\\betaβ, so we must shift the window so that this prefix of β\\betaβ ends at the current position. In the example above, we must shift the window by 5 characters to the right so that the arrow denoting the current position points at the first character b of the window.\n\nOr, for example, in this situation\n\n\n\nwe should shift the window by 2 characters to the right and get:\n\n\n\nThe current elements (above the arrow) match so we can proceed by comparing further characters. By the way, it is important to shift only by 2 characters and not by 4, because otherwise it is possible that we skip too many characters and don’t detect a valid occurrence of β\\betaβ in α\\alphaα.\n\nTo generalize and formalize how we shift the window when the current characters don’t match, we will define the so-called failure function:\n\nLet f(j)f(j)f(j) be the length of the longest proper prefix of b0b1…bjb_0b_1 \\dots b_jb0​b1​…bj​, such that this prefix is also a suffix of b0b1…bjb_0b_1 \\dots b_jb0​b1​…bj​. In this context, proper means that it is not the string itself so the prefix is not trivial.\n\nThe idea behind the failure function is that when we find mismatching characters while comparing the window with α\\alphaα, we should shift the window such that the f(i)f(i)f(i)-th character of the window is at the position of the last character iii of the window (green area). In other words, the window should be shifted by w−f(i)w - f(i)w−f(i) characters to the right, where www is the size of the window. If, after shifting the window, there is still a mismatch, then it is possible that there is a shorter suffix that matches some prefix of b0…bib_0 \\dots b_ib0​…bi​ so we need to repeat the process again.\n\nWith this idea and assuming the failure function is computed and stored in the f array, we can implement the Knuth-Morris-Pratt algorithm the following way:\n\n# returns the index of the first occurrence of b in a\ndef kmp(a: str, b: str, f: list) -&gt; int:\n    \n    m = len(a)\n    assert m &gt;= 1\n    n = len(b)\n    p = 0\n    \n    for i in range(m):\n        \n        while p != 0 and a[i] != b[p]:\n            p = f[p - 1]\n        \n        if a[i] == b[p]:\n            p += 1\n\n        # invariant formally defined below\n        assert a[i + 1 - p:i + 1] == b[0:p]\n\n        if p == n:\n            return i + 1 - n\n    \n    return -1 # no match found\n\n\nProof of correctness: To formally prove the correctness of the above program, we will define the following invariant which is also verified with the assert statement in code: After every iteration of the for loop, before the last if statement, it holds that ai+1−p…ai=b0…bp−1a_{i+1-p} \\dots a_i = b_0 \\dots b_{p-1}ai+1−p​…ai​=b0​…bp−1​. Intuitively, it means that the window is correct:\n\n\n\nInduction base: We excluded the case α=ε\\alpha = \\varepsilonα=ε in the formal definition of the problem, so the loop will be entered. On the first iteration, the window is empty (p=0p = 0p=0) and the while loop will not be entered because we don’t need to shift the empty window. If the first characters match, then ppp will be incremented establishing the invariant. Otherwise, the window will remain empty.\n\nInduction step: By induction hypothesis, we assume that the invariant holds for some iii and ppp. If p=np = np=n, it means that ai+1−n…ai=b0…bn−1=βa_{i+1-n} \\dots a_i = b_0 \\dots b_{n-1} = \\betaai+1−n​…ai​=b0​…bn−1​=β, so we found an occurrence of β\\betaβ in α\\alphaα at the position i+1−ni + 1 - ni+1−n, and the algorithm terminates. If this is not the case and there is a further character in α\\alphaα, we enter a new iteration of the loop and shift the window by p−f(p−1)p - f(p - 1)p−f(p−1) characters on the right by assigning p to point to the f(p−1)f(p - 1)f(p−1) character of the string. Intuitively by doing this we shift the window so that the new window is the greatest possible proper prefix of the old window, that is also it’s suffix. For this reason it is guaranteed that the window remains valid and we only need to compare the current elements: If a[i] != b[i], then it means that the prefix we took is probably too large and we should search for a smaller one by repeating the window shifting step, until we either find a match or ppp becomes zero meaning that the window is empty. The body of the if statement after the loop expands the window in case the current characters match, establishing the invariant.\n\nIt is also important to note, that the window should be shifted by p−f(p−1)p - f(p - 1)p−f(p−1) characters to the right, because if we take some shorter prefix that is also a suffix of b0…bp−1b_0 \\dots b_{p - 1}b0​…bp−1​, it is possible that we don’t detect an occurrence of β\\betaβ in α\\alphaα. This completes the proof.\n\nIn the example above, the window gets shifted by p−f(p−1)=5−f(4)=5−3=2p - f(p - 1) = 5 - f(4) = 5 - 3 = 2p−f(p−1)=5−f(4)=5−3=2 characters to the right, iii gets incremented and the window gets expanded (by incrementing ppp) because a[i] == b[p]:\n\n\n\nComputing the failure function\n\nAn important part of the KMP algorithm is, of course, the computation of the failure function we defined above. We can come up with an efficient algorithm computing it by approaching the problem with dynamic programming. Suppose we want to compute f(n−1)f(n - 1)f(n−1) and we already computed f(i)f(i)f(i) for all 0≤i≤n−20 \\le i \\le n - 20≤i≤n−2. The value of f(n−2)f(n - 2)f(n−2) gives us the length of the longest suffix ending at bn−2b_{n-2}bn−2​ such that there is a corresponding prefix of this length (equal parts of the string are visualized with braces):\n\nβ=b0…bf(n−2)−1⏟f(n−2)bf(n−2)…bn−2−f(n−2)bn−1−f(n−2)…bn−2⏟f(n−2)bn−1\\beta = \\underbrace{b_0 \\dots b_{f(n-2)-1}}_{f(n-2)}\nb_{f(n-2)} \\dots b_{n-2-f(n-2)}\n\\underbrace{b_{n-1-f(n-2)} \\dots b_{n-2}}_{f(n-2)}\nb_{n-1}β=f(n−2)b0​…bf(n−2)−1​​​bf(n−2)​…bn−2−f(n−2)​f(n−2)bn−1−f(n−2)​…bn−2​​​bn−1​\n\nBy the way, is is possible that the parts of the formula marked with braces intersect, because there is no guarantee that f(n−2)−1&lt;n−1−f(n−2)⇔f(n−2)&lt;n2f(n-2) - 1 &lt; n - 1 - f(n-2) \\Leftrightarrow f(n-2) &lt; \\frac{n}{2}f(n−2)−1&lt;n−1−f(n−2)⇔f(n−2)&lt;2n​.\n\nIf the characters after the equal parts in braces are also the same, or, formally, if bf(n−2)=bn−1b_{f(n-2)} = b_{n-1}bf(n−2)​=bn−1​, then we can conclude that f(n−1)=f(n−2)+1f(n-1) = f(n-2) + 1f(n−1)=f(n−2)+1.\n\nIf bf(n−2)≠bn−1b_{f(n-2)} \\neq b_{n-1}bf(n−2)​​=bn−1​, then the longest prefix of β\\betaβ that is also it’s suffix must have length k≤f(n−2)k \\le f(n-2)k≤f(n−2). Suppose we found such maximum k&lt;f(n−2)k &lt; f(n-2)k&lt;f(n−2) such that:\n\nb0…bk−1=bn−1−k…bn−2b0…bk−1⏞kbk…bf(n−2)−1⏟f(n−2)=bn−1−f(n−2)…bn−2−kbn−1−k…bn−2⏞k⏟f(n−2)\\begin{aligned}\nb_0 \\dots b_{k-1} &amp;= b_{n-1-k} \\dots b_{n-2} \\\\\n\\underbrace{\\overbrace{b_0 \\dots b_{k-1}}^{k} b_{k} \\dots b_{f(n-2)-1}}_{f(n-2)} &amp;=\n\\underbrace{b_{n-1-f(n-2)} \\dots b_{n-2-k} \\overbrace{b_{n-1-k} \\dots b_{n-2}}^{k}}_{f(n-2)}\n\\end{aligned}b0​…bk−1​f(n−2)b0​…bk−1​​k​bk​…bf(n−2)−1​​​​=bn−1−k​…bn−2​=f(n−2)bn−1−f(n−2)​…bn−2−k​bn−1−k​…bn−2​​k​​​​\n\nThen, by combining these equations, it follows that there must be a suffix of length kkk equal to the prefix of\n\nb0…bk−1bk…bf(n−2)−1b_0 \\dots b_{k-1} b_{k} \\dots b_{f(n-2)-1}b0​…bk−1​bk​…bf(n−2)−1​\n\nSo, the maximum possible suffix that is also a prefix has length k:=f(n−2)k := f(n-2)k:=f(n−2), if bk=bn−1b_k = b_{n-1}bk​=bn−1​. We can apply this important fact we derived multiple times — if bk≠bn−1b_k \\neq b_{n-1}bk​​=bn−1​, then the maximum suffix/prefix we are searching for has length l:=f(k−1)l := f(k - 1)l:=f(k−1), if bl=bn−1b_l = b_{n-1}bl​=bn−1​, and so on.\n\nThe base case for the dynamic programming approach is simple: f(0)=0f(0) = 0f(0)=0, because the only proper prefix of b0b_0b0​ is the empty string ε\\varepsilonε.\n\nWe are now ready to implement the computation of the failure function:\n\ndef compute_f(b):\n\n    n = len(b)\n    f = [0]\n\n    for i in range(1, n):\n        \n        k = f[i - 1]\n        \n        while k != 0 and b[i] != b[k]:\n            k = f[k - 1]\n\n        if b[i] == b[k]:\n            k += 1\n\n        f.append(k)  # f[i] = k\n\n    return f\n\n\nIn the case of a mismatch the while-loop searches for potential shorter suffix, that is also a prefix of β\\betaβ. If it was found, then the loop terminates with kkk representing the first character after the prefix. If the loop terminated because of k=0k = 0k=0, then it means that there is no shorter prefix that is also a suffix (it is empty), so the value of the failure function is either 0 or 1, depending on whether the current character is equal to the first one.\n\nOf course, we can run the functions above and check the result:\n\na = \"baabbbaabbaabbbabaabbbaabaabababba\"\nb = \"baababa\"\nprint(\"a:\", a)\nprint(\"b:\", b)\nf = compute_f(b)\nprint(\"Failure function:\")\nprint(\"i   :\", [i for i in range(len(b))])\nprint(\"f(i):\", f)  # [0, 0, 0, 1, 2, 1, 2]\nfound = kmp(a, b, f)\nprint(\"Result:\", found)  # 24\n\n\nFinding all matches\n\nThe Knuth-Morris-Pratt algorithm can be easily extended to detect all occurrences of β\\betaβ in α\\alphaα:\n\ndef kmp(a: str, b: str, f: list) -&gt; list:\n\n    m = len(a)\n    assert m &gt;= 1\n    n = len(b)\n    p = 0\n\n    matches = []\n\n    for i in range(m):\n        \n        while p != 0 and a[i] != b[p]:\n            p = f[p - 1]\n        \n        if a[i] == b[p]:\n            p += 1\n\n        if p == n:\n            matches.append(i + 1 - n)\n            p = f[p - 1]\n    \n    return matches\n\n\nIf the window has length nnn and we thus found an occurrence of β\\betaβ in α\\alphaα, all we need to do is to shift the window by n−f(n−1)n - f(n - 1)n−f(n−1) characters to the right. We need to do it in the body of the if p == n: statement, because the while loop expects that the window is a proper prefix of β\\betaβ and thus ppp is a valid index.\n\nComplexity analysis\n\nIt may seem that both the computation of the failure function as well as the matching itself have a complexity of Θ(n⋅m)\\Theta(n \\cdot m)Θ(n⋅m) because of the 2 nested loops in both algorithms. However, this is not the case. We can first show, that the failure function can be computed in linear time:\n\nFailure function computation complexity\n\nWe’ve already argued above that f(i)≤f(i−1)+1∀i∈{1,…,n−1}f(i) \\le f(i - 1) + 1 \\quad\\forall i \\in \\{1, \\dots, n - 1\\}f(i)≤f(i−1)+1∀i∈{1,…,n−1} and f(0)=0f(0) = 0f(0)=0. It follows, that f(i)≤if(i) \\le if(i)≤i also holds for all 0≤i&lt;n0 \\le i &lt; n0≤i&lt;n. Because of that, during a single iteration of the for loop, the while loop makes at most kkk iterations where kkk is the initial value of the k variable.\n\nWe will now prove that the body of of the while loop (in particular, the k = func[k - 1] statement) gets executed at most nnn times during the entire algorithm: the maximum value of kkk is n−1n - 1n−1, so if the while loop is entered, then it will make at most n−1n - 1n−1 iterations and because f(i)≤f(i−1)+1f(i) \\le f(i - 1) + 1f(i)≤f(i−1)+1, the value of kkk will decrease by at least the amount of iteration of the while loop.\n\nTherefore, the overall running time of the algorithm is in O(n)O(n)O(n).\n\nMatching algorithm complexity\n\nAssuming the failure function is computed and stored in an array so that the access time is constant, the complexity of the Knuth-Morris-Pratt algorithm is O(m)O(m)O(m). The proof of this fact is analogous to the previous proof.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/knuth-morris-pratt/"
    },{
      "image": {"path":"/assets/img/blog/gray-codes-tmb.jpg","srcset":{"1920w":"/assets/img/blog/gray-codes-tmb.jpg","960w":"/assets/img/blog/gray-codes-tmb@0,5x.jpg","480w":"/assets/img/blog/gray-codes-tmb@0,25x.jpg","240w":"/assets/img/blog/gray-codes-tmb@0,125x.jpg"}},
      "title": "Gray codes and their beautiful symmetries",
      "date": "2020-12-28 00:00:00 +0100",
      
      "content": "This post is about gray codes a.k.a. gray encoding of binary numbers. We will also cover some interesting properties, ideas and symmetries behind this encoding.\n\nGray codes\n\nThe Gray encoding is a special binary encoding for numbers, which is especially efficient for counting, because the hamming distance between any 2 successive values differ in exactly 1 bit (i.e. have Hamming distance 1). For this reason, counters in low power embedded environments are often implemented with this encoding. For example, for numbers from 000 to 151515 the corresponding gray codes are:\n\n\n  \n    \n      Number\n      Gray\n      Number\n      Gray\n    \n  \n  \n    \n      0000\n      0000\n      1000\n      1100\n    \n    \n      0001\n      0001\n      1001\n      1101\n    \n    \n      0010\n      0011\n      1010\n      1111\n    \n    \n      0011\n      0010\n      1011\n      1110\n    \n    \n      0100\n      0110\n      1100\n      1010\n    \n    \n      0101\n      0111\n      1101\n      1011\n    \n    \n      0110\n      0101\n      1110\n      1001\n    \n    \n      0111\n      0100\n      1111\n      1000\n    \n  \n\n\nThe idea behind the increment operation is the following:\n\n\n  Go through the bits of the previous gray code one-by-one starting from the rightmost bit.\n  If flipping the current bit leads to a new code word, flip it and return the code word.\n  Otherwise, proceed with the next bit.\n\n\nEncoding\n\nThe gray code of a binary number x=xn−1,…,x0x = x_{n-1}, \\dots, x_0x=xn−1​,…,x0​ is g=gn−1,…,g0g = g_{n-1}, \\dots, g_0g=gn−1​,…,g0​ (right hand sides in these equations mean bit sequences), where:\n\ngi:={xii=n−1xi⊕xi+1i&lt;n−1g_i :=\n\\begin{cases}\nx_i &amp; i = n - 1 \\\\\nx_i \\oplus x_{i+1} &amp; i &lt; n-1\n\\end{cases}gi​:={xi​xi​⊕xi+1​​i=n−1i&lt;n−1​\n\nFor example, for x=110102x = {11010}_2x=110102​, the gray code would be g=101112g = {10111}_2g=101112​:\n\n⊕110100110110111\\begin{array}{r}\n\\oplus\n\\begin{array}{r}\n11010\\\\\n01101\\\\\n\\end{array} \\\\\n\\hline\n\\begin{array}{r}\n10111\n\\end{array}\n\\end{array}⊕1101001101​10111​​​\n\nThis encoding can be easily implemented, for example in Python:\n\ndef gray_encode(x: int) -&gt; int:\n    return x ^ (x &gt;&gt; 1)\n\n\nDecoding\n\nWe can calculate the bits of xxx by knowing the gray code g=gn−1,…,g0g = g_{n-1}, \\dots, g_0g=gn−1​,…,g0​ as follows:\n\nxk=⨁i=kn−1gix_k = \\bigoplus^{n-1}_{i=k}{g_i}xk​=i=k⨁n−1​gi​\n\nContinuing the example above, we can decode g=101112g = {10111}_2g=101112​ and get the initial codeword x=110102x = {11010}_2x=110102​ by calculating the prefix sums of ggg in this notation:\n\n⊕101110101100101000100000111010\\begin{array}{r}\n\\oplus\n\\begin{array}{r}\n10111 \\\\\n01011 \\\\\n00101 \\\\\n00010 \\\\\n00001 \\\\\n\\end{array} \\\\\n\\hline\n\\begin{array}{r}\n11010\n\\end{array}\n\\end{array}⊕1011101011001010001000001​11010​​​\n\nProof of correctness: We can prove that the formula above for decoding gray codes is correct by induction on the length nnn of the codeword:\n\nInduction base: k=n−1k = n - 1k=n−1:\n\n⨁i=n−1n−1gi=gn−1=xn−1\\bigoplus^{n-1}_{i=n-1}{g_i} = g_{n-1} = x_{n-1}i=n−1⨁n−1​gi​=gn−1​=xn−1​\n\nInduction step: k&gt;0,k→k−1k &gt; 0, k \\rightarrow k - 1k&gt;0,k→k−1:\n\n⨁i=k−1n−1gi=gk−1⊕⨁i=kn−1gi=IHgk−1⊕xk=xk−1⊕xk⊕xk=xk−1\\bigoplus^{n-1}_{i=k-1}{g_i} = g_{k-1} \\oplus \\bigoplus^{n-1}_{i=k}{g_i} \\stackrel{\\mathrm{IH}}{=} g_{k-1} \\oplus x_k = x_{k-1} \\oplus x_k \\oplus x_k = x_{k-1}i=k−1⨁n−1​gi​=gk−1​⊕i=k⨁n−1​gi​=IHgk−1​⊕xk​=xk−1​⊕xk​⊕xk​=xk−1​\n\nThe implementation of the decoding algorithm is also not hard:\n\ndef gray_decode(g: int) -&gt; int:\n    b = 0\n    while g != 0:\n        b ^= g\n        g &gt;&gt;= 1\n    return b\n\n\nRecursive definition for gray codes\n\nWe can formalize the intuition behind the increment operation above by an alternative, recursive definition for gray codes:\n\nLet BiB_iBi​ for i≥1i \\ge 1i≥1 be a mapping that maps numbers from 000 to 2i−12^i - 12i−1 to the corresponding gray code as a bit string.\n\nThe first 2 gray codes are 000 and 111, so we define:\n\nB1:{0,1}→{0,1}∗0↦01↦1\\begin{aligned}\nB_1 : \\{0, 1\\} &amp;\\rightarrow \\{0, 1\\}^* \\\\\n0 &amp;\\mapsto 0 \\\\\n1 &amp;\\mapsto 1\n\\end{aligned}B1​:{0,1}01​→{0,1}∗↦0↦1​\n\nFor i≥2i \\ge 2i≥2, define recursively:\n\nBi:{0,…,2i−1}→{0,1}∗0↦0⋅Bi−1(0)⋮2i−1−1↦0⋅Bi−1(2i−1−1)2i−1↦1⋅Bi−1(2i−1−1)⋮2i−1↦1⋅Bi−1(0)\\begin{aligned}\nB_i : \\{0, \\dots, 2^i - 1\\} &amp;\\rightarrow \\{0, 1\\}^* \\\\\n0 &amp;\\mapsto 0 \\cdot B_{i-1}(0) \\\\\n&amp;\\quad \\vdots \\\\\n2^{i-1} - 1 &amp;\\mapsto 0 \\cdot B_{i-1}(2^{i-1} - 1) \\\\\n2^{i-1} &amp;\\mapsto 1 \\cdot B_{i-1}(2^{i-1} - 1) \\\\\n&amp;\\quad \\vdots \\\\\n2^i - 1 &amp;\\mapsto 1 \\cdot B_{i-1}(0)\n\\end{aligned}Bi​:{0,…,2i−1}02i−1−12i−12i−1​→{0,1}∗↦0⋅Bi−1​(0)⋮↦0⋅Bi−1​(2i−1−1)↦1⋅Bi−1​(2i−1−1)⋮↦1⋅Bi−1​(0)​\n\nThe idea behind this definition is that we “flip” the BiB_iBi​ mapping and prepend 1 to all of the gray codes in BiB_iBi​ in reverse order (because of the “flip”). This is how we obtain the second half of the Bi+1B_{i+1}Bi+1​ mapping. Because of the reverse order, the hamming distance between 2 successive values is always 1. We can visualize this idea of the recursive mapping for the first 25=322^5 = 3225=32 gray codes:\n\n\n\nSo, the key patterns that we see in the gray code table are the following:\n\n\n  The second half of any BiB_iBi​ is the first part of BiB_iBi​ in reverse order with a leading 1-bit.\n  In order to reverse the first part of BiB_iBi​, it is sufficient to just flip the first bit of every gray code.\n\n\nWe can formalize the second very important fact with the following theorem:\n\nSymmetry theorem\n\nFor all i≥2i \\ge 2i≥2 and 2i−1≤j&lt;2i2^{i-1} \\le j &lt; 2^i2i−1≤j&lt;2i in the second half of BiB_iBi​, it holds that:\n\nBi(j)=1,bn−1⊕1,bn−2,…,b0bn−1,bn−2,…,b0:=Bi−1(j−2i−1)\\begin{aligned}\nB_i(j) &amp;= 1, b_{n-1} \\oplus 1, b_{n-2}, \\dots, b_0 \\\\\nb_{n-1},b_{n-2}, \\dots, b_0 &amp;:= B_{i-1}(j - 2^{i-1})\n\\end{aligned}Bi​(j)bn−1​,bn−2​,…,b0​​=1,bn−1​⊕1,bn−2​,…,b0​:=Bi−1​(j−2i−1)​\n\nIn other words, it means that Bi(j)B_i(j)Bi​(j) is equal to 1⋅Bi−1(j−2i−1)1 \\cdot B_{i-1}(j - 2^{i-1})1⋅Bi−1​(j−2i−1) with the first bit of Bi−1(j−2i−1)B_{i-1}(j - 2^{i-1})Bi−1​(j−2i−1) flipped.\n\nProof: by induction on iii:\n\nInduction base: (i=2i = 2i=2):\n\nB2(2)=1⋅(B2(0)⊕1)=1⋅(0⊕1)=1⋅1=11=1⋅B1(1)B2(3)=1⋅(B2(1)⊕1)=1⋅(1⊕1)=1⋅0=10=1⋅B1(0)\\begin{aligned}\nB_2(2) &amp;= 1 \\cdot (B_2(0) \\oplus 1) = 1 \\cdot (0 \\oplus 1) = 1 \\cdot 1 = 11 = 1 \\cdot B_1(1) \\\\\nB_2(3) &amp;= 1 \\cdot (B_2(1) \\oplus 1) = 1 \\cdot (1 \\oplus 1) = 1 \\cdot 0 = 10 = 1 \\cdot B_1(0)\n\\end{aligned}B2​(2)B2​(3)​=1⋅(B2​(0)⊕1)=1⋅(0⊕1)=1⋅1=11=1⋅B1​(1)=1⋅(B2​(1)⊕1)=1⋅(1⊕1)=1⋅0=10=1⋅B1​(0)​\n\nInduction step: (i−1→ii - 1 \\rightarrow ii−1→i):\n\n\n  When defining Bi−1B_{i-1}Bi−1​, we prepended bit 1 to every code jjj in the second half (2i−2≤j&lt;2i−12^{i-2} \\le j &lt; 2^{i-1}2i−2≤j&lt;2i−1). Because of that, it is correct to flip the first bit of Bi−1(j−2i−1)B_{i-1}(j - 2^{i-1})Bi−1​(j−2i−1).\n  We now argue why it is correct to just leave other bits as-is. Consider all the bits at an arbitrary, but fixed position 0≤k≤i−40 \\le k \\le i - 40≤k≤i−4 in the gray codes generated by Bi−1B_{i-1}Bi−1​. Formally, let a(l):=bka(l) := b_ka(l):=bk​ where bn−1…b0:=Bi−1(l)b_{n-1}\\dots b_0 := B_{i-1}(l)bn−1​…b0​:=Bi−1​(l) and a:=a(0),…,a(2i−2−1),a(2i−2),…,a(2i−1−1)a := a(0), \\dots, a(2^{i-2}-1), a(2^{i-2}), \\dots, a(2^{i-1}-1)a:=a(0),…,a(2i−2−1),a(2i−2),…,a(2i−1−1).\n  By definition of Bi−1B_{i-1}Bi−1​, the second part of aaa is the reversed first half of aaa, or, formally, a(2i−2),…,a(2i−1−1)=a(2i−2−1),…,a(0)a(2^{i-2}), \\dots, a(2^{i-1}-1) = a(2^{i-2}-1), \\dots, a(0)a(2i−2),…,a(2i−1−1)=a(2i−2−1),…,a(0).\n  By induction hypothesis, we know that a(0),…,a(2i−2−1)=a(2i−2),…,a(2i−1−1)a(0), \\dots, a(2^{i-2}-1) = a(2^{i-2}), \\dots, a(2^{i-1}-1)a(0),…,a(2i−2−1)=a(2i−2),…,a(2i−1−1).\n  By combining the last 2 observations, it follows that both a(0),…,a(2i−2−1)a(0), \\dots, a(2^{i-2}-1)a(0),…,a(2i−2−1) and a(2i−2),…,a(2i−1−1)a(2^{i-2}), \\dots, a(2^{i-1}-1)a(2i−2),…,a(2i−1−1) are palindromes.\n  aaa is a concatenation of 2 equal palindromes and is therefore also a palindrome.\n\n\nThis completes the proof. We can also illustrate, for example, the 3→43 \\rightarrow 43→4 induction step:\n\n\n\nHere, aaa is illustrated for k=0k = 0k=0.\n\nEquivalence of definitions\n\nOf course, we need to prove that both definitions (recursive and non-recursive) are equivalent. Formally, we will prove the equivalence of definitions of all gray codes 0≤x&lt;2i0 \\le x &lt; 2^i0≤x&lt;2i by induction for all iii:\n\nInduction base: (i∈{1,2}i \\in \\{1, 2\\}i∈{1,2}) Both definitions are equivalent for the first 4 gray codes.\n\nInduction step: (i→i+1i \\rightarrow i + 1i→i+1):\n\n\n  BiB_iBi​ is part of Bi+1B_{i+1}Bi+1​, so we only need to prove that the definitions for gray codes for 2i≤x≤2i+1−12^i \\le x \\le 2^{i+1} - 12i≤x≤2i+1−1 from the second part of Bi+1B_{i+1}Bi+1​ are equivalent.\n  Let x=xi,…,x0x = x_i, \\dots, x_0x=xi​,…,x0​. As Bi+1B_{i+1}Bi+1​ defines first 2i+12^{i+1}2i+1 gray codes, it follows that xi=1x_i = 1xi​=1 (this fact is clearly seen on the illustrations above — the leading ones are marked with orange).\n  By the symmetry theorem, we know that the gray code of xxx is equal to the gray code of x−2ix - 2^ix−2i with the first bit flipped and 1 prepended. On the other hand, by the first definition of gray codes and because of Bi+1B_{i+1}Bi+1​ defining 2i+12^{i+1}2i+1 gray codes, the gray code for xxx is also equal to the gray code of x−2ix - 2^ix−2i with the first bit flipped because gi−1=xi−1⊕xi=xi−1⊕1g_{i-1} = x_{i-1} \\oplus x_i = x_{i-1} \\oplus 1gi−1​=xi−1​⊕xi​=xi−1​⊕1 and 1 prepended (see above). By induction hypothesis, both definitions for x−2ix - 2^ix−2i were equivalent, so the definitions are equivalent for xxx.\n\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/gray-codes/"
    },{
      "image": {"path":"/assets/img/blog/svd-image-compression-tmb.jpg","srcset":{"1280w":"/assets/img/blog/svd-image-compression-tmb.jpg","640w":"/assets/img/blog/svd-image-compression-tmb@0,5x.jpg","320w":"/assets/img/blog/svd-image-compression-tmb@0,25x.jpg","160w":"/assets/img/blog/svd-image-compression-tmb@0,125x.jpg"}},
      "title": "Compressing images with singular value decomposition (SVD)",
      "date": "2021-01-14 00:00:00 +0100",
      
      "content": "The singular matrix decomposition plays a major role in linear algebra and has a lot of applications, including lossy image compression. In this post we will discuss it in the context of the mentioned image compression with the focus on the intuition behind the algorithm, without going deep into the theory.\n\nSVD and the idea behind it\n\nAny matrix A∈Rm×nA \\in \\mathbb{R}^{m\\times n}A∈Rm×n describes some linear transformation from Rn\\mathbb{R}^nRn to Rm\\mathbb{R}^mRm by specifying in it’s columns where the basis vectors should land relative to the initial basis. Basically, linear transformations are able to stretch, rotate and flip space and, of course, do any combination of these actions at once. So we might be interested in creating some kind of normal form for linear transformations consisting of only geometrically simple operations, so that we are able to convert any linear transformation to this normal form. This is the key idea behind SVD — it is a normal form that consists of 2 rotations and 1 scaling.\n\nFormally, any matrix A∈Rm×nA \\in \\mathbb{R}^{m\\times n}A∈Rm×n can be written as\n\nA=UΣV⊤A = U \\Sigma V^\\topA=UΣV⊤\n\nwhere:\n\n\n  U∈Rm×mU \\in \\mathbb{R}^{m\\times m}U∈Rm×m is an orthogonal matrix that rotates space, column vectors of this matrix are often referred to as left singular vectors.\n  Σ∈Rm×n\\Sigma \\in \\mathbb{R}^{m\\times n}Σ∈Rm×n is a diagonal matrix (all elements that are not on the diagonal are zero, diagonal elements are called singular values), which scales space.\n  V⊤∈Rn×nV^\\top \\in \\mathbb{R}^{n\\times n}V⊤∈Rn×n is an orthogonal matrix, which only rotates space. Row vectors of V⊤V^\\topV⊤ are also known as right singular vectors.\n  Singular values (on the diagonal) of Σ\\SigmaΣ are sorted in descending order and are different. We will denote them with σ1&gt;σ2&gt;⋯&gt;σr&gt;0\\sigma_1 &gt; \\sigma_2 &gt; \\dots &gt; \\sigma_r &gt; 0σ1​&gt;σ2​&gt;⋯&gt;σr​&gt;0 where rrr is the amount of non-zero entries on the diagonal (it also follows, that r=rank(Σ)=rank(A)r = \\rank(\\Sigma) = \\rank(A)r=rank(Σ)=rank(A)).\n\n\nSo, any linear transformation AAA can be decomposed in a rotation V⊤V^\\topV⊤, followed by some scaling by Σ\\SigmaΣ, followed by another rotation UUU.\n\nVisualizing SVD\n\nConsider the following simple shear matrix:\n\nA:=(1101)A := \\left(\n\\begin{array}{cc}\n1  &amp; 1 \\\\\n0 &amp; 1\n\\end{array}\n\\right)A:=(10​11​)\n\nWe can use the Open Source manim python library to visualize AAA as well as its decomposition A=UΣV⊤A = U \\Sigma V^\\topA=UΣV⊤:\n\n# file: \"svd_2d.py\"\nfrom manimlib.imports import *\n\n\nclass SVD_2D(LinearTransformationScene):\n    CONFIG = {\n        \"include_background_plane\": True,\n        \"include_foreground_plane\": True,\n        \"foreground_plane_kwargs\": {\n            \"x_radius\": FRAME_WIDTH,\n            \"y_radius\": FRAME_HEIGHT,\n            \"secondary_line_ratio\": 0\n        },\n        \"background_plane_kwargs\": {\n            \"color\": GREY,\n            \"secondary_color\": DARK_GREY,\n            \"axes_color\": GREY,\n            \"stroke_width\": 2,\n        },\n        \"show_coordinates\": True,\n        \"show_basis_vectors\": True,\n        \"basis_vector_stroke_width\": 6,\n        \"i_hat_color\": X_COLOR,\n        \"j_hat_color\": Y_COLOR,\n        \"leave_ghost_vectors\": False\n    }\n\n    def construct(self):\n\n        circle = Circle()\n        circle.move_to(RIGHT + UP)\n\n        self.add_transformable_mobject(circle)\n\n        text_applying = TextMobject(\"Applying $ A $ (overall transformation)\")\n        text_applying.to_edge(UP + LEFT)\n\n        text_applying_inverse = TextMobject(\"Applying $ A^{-1} $ (resetting)\")\n        text_applying_inverse.to_edge(UP + LEFT)\n\n        text_applying_vt = TextMobject(r\"Applying $ V^\\top $ (rotating)\")\n        text_applying_vt.to_edge(UP + LEFT)\n\n        text_applying_s = TextMobject(r\"Applying $ \\Sigma $ (stretching)\")\n        text_applying_s.to_edge(UP + LEFT)\n\n        text_applying_u = TextMobject(\"Applying $ U $ (rotating)\")\n        text_applying_u.to_edge(UP + LEFT)\n        \n        A = np.array([\n            [1, 1],\n            [0, 1]\n        ])\n\n        self.play(Write(text_applying), run_time=0.5)\n        self.apply_matrix(A)\n\n        self.wait()\n        self.wait(0.5)\n\n        self.leave_ghost_vectors = True\n\n        self.play(ReplacementTransform(text_applying, text_applying_inverse), run_time=0.5)\n        self.apply_inverse(A)\n\n        self.wait()\n        self.wait(0.5)\n\n        self.leave_ghost_vectors = False\n\n        U, S, VT = np.linalg.svd(A)\n        \n        self.play(ReplacementTransform(text_applying_inverse, text_applying_vt), run_time=0.5)\n        self.apply_matrix(VT)\n\n        self.wait()\n        self.wait(0.5)\n\n        self.play(ReplacementTransform(text_applying_vt, text_applying_s), run_time=0.5)\n        self.apply_matrix(np.diag(S))\n\n        self.wait()\n        self.wait(0.5)\n\n        self.play(ReplacementTransform(text_applying_s, text_applying_u), run_time=0.5)\n        self.apply_matrix(U)\n\n        self.wait()\n        self.wait(0.5)\n\n\nIn order to compute the SVD decomposition, I used the np.linalg.svd function which returns the tuple with the UUU, Σ\\SigmaΣ and V⊤V^\\topV⊤ matrices as expected. However, for efficiency reasons numpy returnes Σ\\SigmaΣ as a vector (It doesn’t make sense to store zeroes that are off the diagonal). So that’s why I later on use np.diag to convert a vector to a matrix with the vector on the diagonal.\n\nBy running manim with the command manim svd_2d.py SVD_2D -m, we get:\n\n\n\t\n\tUnfortunately, your browser doesn't support the video tag.\n\nAs you can see, the geometric properties of the matrices in the decompositions as described above hold (UUU and V⊤V^\\topV⊤ rotate space, Σ\\SigmaΣ only stretches it).\n\nCompressing images with SVD\n\nAny image can be represented as a matrix of pixels, where each pixel (typically) consists of 3 bytes — for the red, green and blue components of the color, respectively. So, if we want to efficiently store the image, we need to somehow efficiently encode 3 matrices RRR, GGG and BBB for each color component, respectively.\n\nLet’s focus on compressing just one matrix A∈Rm×nA \\in \\mathbb{R}^{m\\times n}A∈Rm×n. As stated above, applying the SVD decomposition gives us:\n\nA=UΣV⊤=(∣∣u1⋯un∣∣)(σ10⋯00⋯00σ2⋯00⋯0⋮⋮⋱⋮⋮⋯⋮00⋯σr0⋯000⋯00⋯0⋮⋮⋮⋮⋮⋱⋮00⋯00⋯0)(v1⊤⋮vn⊤)A = U \\Sigma V^\\top =\n\\begin{pmatrix}\n\\vertbar &amp;  &amp; \\vertbar \\\\\nu_1 &amp;  \\cdots &amp; u_n\\\\\n\\vertbar &amp;  &amp; \\vertbar \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sigma_1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\\n0 &amp; \\sigma_2 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\\n\\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots \\\\\n0 &amp; 0 &amp; \\cdots &amp; \\sigma_r &amp; 0 &amp; \\cdots &amp; 0 \\\\\n0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp;\\cdots &amp; 0 \\\\\n\\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\\n0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\horzbar &amp; v_1^\\top &amp; \\horzbar \\\\\n&amp;  \\vdots &amp; \\\\\n\\horzbar &amp; v_n^\\top &amp; \\horzbar \\\\\n\\end{pmatrix}A=UΣV⊤=⎝⎜⎛​∣u1​∣​⋯​∣un​∣​⎠⎟⎞​⎝⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎛​σ1​0⋮00⋮0​0σ2​⋮00⋮0​⋯⋯⋱⋯⋯⋮⋯​00⋮σr​0⋮0​00⋮00⋮0​⋯⋯⋯⋯⋯⋱⋯​00⋮00⋮0​⎠⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎞​⎝⎜⎜⎛​​v1⊤​⋮vn⊤​​​⎠⎟⎟⎞​\n\nWe can re-write this decomposition in a slightly different way:\n\nA=σ1(∣u1∣)(v1⊤)+⋯+σr(∣ur∣)(vr⊤)=∑i=1rσi(∣ui∣)(vi⊤)\\begin{aligned}\nA &amp;=\n\\sigma_1\n\\begin{pmatrix}\n\\vertbar \\\\\nu_1 \\\\\n\\vertbar\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\horzbar &amp; v_1^\\top &amp; \\horzbar \\\\\n\\end{pmatrix}\n\n  \\cdots +\n\\sigma_r\n\\begin{pmatrix}\n\\vertbar \nu_r \n\\vertbar\n\\end{pmatrix}\n\\begin{pmatrix}\n\\horzbar &amp; v_r^\\top &amp; \\horzbar \n\\end{pmatrix}\n\n&amp;= \\sum_{i=1}^r{\n\\sigma_i\n\\begin{pmatrix}\n\\vertbar \nu_i \n\\vertbar\n\\end{pmatrix}\n\\begin{pmatrix}\n\\horzbar &amp; v_i^\\top &amp; \\horzbar \n\\end{pmatrix}\n}\n\\end{aligned}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;A​=σ1​⎝⎜⎛​∣u1​∣​⎠⎟⎞​(​v1⊤​​​)+⋯+σr​⎝⎜⎛​∣ur​∣​⎠⎟⎞​(​vr⊤​​​)=i=1∑r​σi​⎝⎜⎛​∣ui​∣​⎠⎟⎞​(​vi⊤​​​)​&lt;/span&gt;&lt;/span&gt;\n\n\nWith this notation, we can think of SVD from a slightly different perspective — SVD allows us to take an arbitrary matrix and write it down as a sum of rank-1 matrices.\n\nIf we take a random matrix AAA, it is very unlikely that it will not be full-rank. So for a random image of width www and height hhh, the matrix will almost certainly have rank min⁡{w,h}\\min\\{w,h\\}min{w,h}. We can approximate AAA with the first k&lt;rk &lt; rk&lt;r summands:\n\nA≈A′:=∑i=1kσi(∣ui∣)(vi⊤)A \\approx A&#x27; := \\sum_{i=1}^k{\n\\sigma_i\n\\begin{pmatrix}\n\\vertbar \\\\\nu_i \\\\\n\\vertbar\\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\horzbar &amp; v_i^\\top &amp; \\horzbar \\\\\n\\end{pmatrix}\n}A≈A′:=i=1∑k​σi​⎝⎜⎛​∣ui​∣​⎠⎟⎞​(​vi⊤​​​)\n\nThis approximation is good, because σ1&gt;⋯&gt;σr&gt;0\\sigma_1 &gt; \\dots &gt; \\sigma_r &gt; 0σ1​&gt;⋯&gt;σr​&gt;0 — first singular values contribute more than the following. By the way, the Eckhart-Young theorem states that such an approximation is indeed the best possible one, so, restricted to the model in which we approximate a matrix with a sum of rank-1 matrices, we get the best possible compression algorithm.\n\nWith this approximation idea we are now ready to implement image compression. Let’s start with some image — for example with this photo I recently took in Kaiserslautern, Germany:\n\n\n\nIf the image is saved as img.jpg, we can read it with:\n\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.image as image\n\nA = image.imread(\"img.jpg\")\n\n\nWe can extract the 3 color component matrices as briefly mentioned above as follows:\n\nR = A[:,:,0] / 0xff\nG = A[:,:,1] / 0xff\nB = A[:,:,2] / 0xff\n\n\nNow, we compute the SVD decomposition:\n\nR_U, R_S, R_VT = np.linalg.svd(R)\nG_U, G_S, G_VT = np.linalg.svd(G)\nB_U, B_S, B_VT = np.linalg.svd(B)\n\n\nNow we need to determine kkk, with which we will approximate the 3 matrices. There are different ways of doing it — I will just calculate the maximum rank of the compressed image, divided by the maximum possible rank of the initial matrix (which is the width of the image) and call this ratio the relative rank.\n\nrelative_rank = 0.2\nmax_rank = int(relative_rank * min(R.shape[0], R.shape[1]))\nprint(\"max rank = %d\" % max_rank)  # 144\n\n\nNow, we can implement a function that will only use the first kkk columns of the UUU matrix and the first kkk rows of the V⊤V^\\topV⊤ matrix, as well as σ1,…,σk\\sigma_1, \\dots, \\sigma_kσ1​,…,σk​. By doing this, we essentially emulate what a decompressing algorithm would do.\n\ndef read_as_compressed(U, S, VT, k):\n    A = np.zeros((U.shape[0], VT.shape[1]))\n    for i in range(k):\n        U_i = U[:,[i]]\n        VT_i = np.array([VT[i]])\n        A += S[i] * (U_i @ VT_i)\n    return A\n\n\nActually, it is easier and more efficient to perform the same operation with a lower-rank matrix multiplication.\n\ndef read_as_compressed(U, S, VT, k):\n    return (U[:,:k] @ np.diag(S[:k])) @ VT[:k]\n\n\nSo we can compute the color matrices of the approximated image with:\n\nR_compressed = read_as_compressed(R_U, R_S, R_VT, max_rank)\nG_compressed = read_as_compressed(G_U, G_S, G_VT, max_rank)\nB_compressed = read_as_compressed(B_U, B_S, B_VT, max_rank)\n\ncompressed_float = np.dstack((R_compressed, G_compressed, B_compressed))\ncompressed = (np.minimum(compressed_float, 1.0) * 0xff).astype(np.uint8)\n\n\nWith matplotlib, we can also easily plot both the original as well as the compressed image by simply calling:\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize = (16, 9))\nplt.imshow(A)\n\nplt.figure(figsize = (16, 9))\nplt.imshow(compressed)\n\nimage.imsave(\"compressed.png\", compressed)\n\n\nBy approximating the image with a matrix of maximum rank equal to 20%20\\%20% of the maximum rank of the original image, we get:\n\n\n\nWe can also run the algorithm for other relative rank values:\n\nRelative rank = 100%100\\%100%, Rank: k=720=r=n&lt;mk = 720 = r = n &lt; mk=720=r=n&lt;m and we get the original image:\n\n\n\nRelative rank = 90%90\\%90%, Rank: k=648k = 648k=648:\n\n\n\nRelative rank = 80%80\\%80%, Rank: k=576k = 576k=576:\n\n\n\nRelative rank = 70%70\\%70%, Rank: k=503k = 503k=503:\n\n\n\nRelative rank = 60%60\\%60%, Rank: k=432k = 432k=432:\n\n\n\nRelative rank = 50%50\\%50%, Rank: k=360k = 360k=360:\n\n\n\nRelative rank = 40%40\\%40%, Rank: k=288k = 288k=288:\n\n\n\nRelative rank = 30%30\\%30%, Rank: k=216k = 216k=216:\n\n\n\nRelative rank = 20%20\\%20%, Rank: k=144k = 144k=144:\n\n\n\nRelative rank = 10%10\\%10%, Rank: k=72k = 72k=72:\n\n\n\nRelative rank = 5%5\\%5%, Rank: k=36k = 36k=36:\n\n\n\nRelative rank = 2%2\\%2%, Rank: k=14k = 14k=14:\n\n\n\nRelative rank = 1%1\\%1%, Rank: k=7k = 7k=7:\n\n\n\nRelative rank = 0.4%0.4\\%0.4%, Rank: k=2k = 2k=2:\n\n\n\nRelative rank = 0.2%0.2\\%0.2%, Rank: k=1k = 1k=1:\n\n\n\nRelative rank = 0%0\\%0%, Rank: k=0k = 0k=0:\n\nIn this case we don’t approximate at all and thus get the zero-matrix which means all pixels are black:\n\n\n",
      "categories": ["cs"],
      "tags": ["linalg"],
      
      "collection": "posts",
      "url": "/blog/cs/svd-image-compression/"
    },{
      "image": {"path":"/assets/img/blog/dining-philosophers-tmb.jpg","srcset":{"1920w":"/assets/img/blog/dining-philosophers-tmb.jpg","960w":"/assets/img/blog/dining-philosophers-tmb@0,5x.jpg","480w":"/assets/img/blog/dining-philosophers-tmb@0,25x.jpg","240w":"/assets/img/blog/dining-philosophers-tmb@0,125x.jpg"}},
      "title": "The Dining Philosophers problem and different ways of solving it",
      "date": "2021-01-24 00:00:00 +0100",
      
      "content": "The dining philosophers problem is a well-known problem in computer science, originally formulated by Edsger Dijkstra to illustrate the possibility of deadlocks in programs where multiple threads lock and unlock multiple shared resources, such that a situation in which every thread is waiting for actions from other threads and no thread can thus continue it’s normal execution may occur. We will consider different approaches for solving this problem in the present post.\n\nThe problem\n\nThe dining philosophers problem has different formulations and variations. We will consider one classic definition:\n\n\n  nnn philosophers (philosophers 0,1,…,n−10,1,\\dots,n-10,1,…,n−1) are sitting at a round table.\n  Each philosopher has a plate in front of him. Each plate has a fork to the left and to the right of it. However, if any 2 philosophers sit next to each other, they share 1 fork as illustrated below, for n=8n = 8n=8.\n\n\n\n\nEach philosopher behaves independently from other philosophers, but in accordance with the following scenario:\n\n\n  Think for some time.\n  Take the right fork.\n  Take the left fork.\n  Eat food.\n  Put the left fork back.\n  Put the right fork back.\n  Repeat the whole process again, i.e. go to step 1.\n\n\nIf a philosopher wants to take a fork, but this fork is currently used by the neighbor, the philosopher waits until the neighbor puts the fork back before getting it. If two neighbors try to take one fork at the same time, only one of them succeeds and the other one waits.\n\nFrom a programmer’s perspective, each philosopher is a thread performing these actions in an infinite loop.\n\n// file: \"Philosopher.java\"\npublic class Philosopher extends Thread {\n\n    private final int id;\n    private final Fork leftFork;\n    private final Fork rightFork;\n\n    public Philosopher(int id, Fork leftFork, Fork rightFork) {\n        this.id = id;\n        this.leftFork = leftFork;\n        this.rightFork = rightFork;\n    }\n\n    @Override\n    public void run() {\n        for (;;) {\n            System.out.println(\"Philosopher \" + id + \" is thinking...\");\n            rightFork.take(id);\n            System.out.println(\"Philosopher \" + id + \" took the right fork \" + rightFork.id);\n            leftFork.take(id);\n            System.out.println(\"Philosopher \" + id + \" took the left fork \" + leftFork.id);\n            System.out.println(\"Philosopher \" + id + \" is eating...\");\n            leftFork.put(id);\n            System.out.println(\"Philosopher \" + id + \" has put down the left fork \" + leftFork.id);\n            rightFork.put(id);\n            System.out.println(\"Philosopher \" + id + \" has put down the right fork \" + rightFork.id);\n        }\n    }\n\n}\n\n\nForks are essentially resources shared under mutual exclusion. In computer science, such data structures are called monitors. In Java, they can be implemented easily with synchronized methods. As mentioned above, a fork can only be used by one philosopher at a time, so we can use the wait() method to wait in case the fork is not available, and notify() some waiting philosopher (thread) if the fork is not being used anymore:\n\n// file: \"Fork.java\"\npublic class Fork {\n\n    public final int id;\n    private boolean forkIsOnTheTable = true;\n    private int philosopherUsingThisFork;\n\n    public Fork(int id) {\n        this.id = id;\n    }\n\n    public synchronized void take(int philosopher) {\n\n        while (!forkIsOnTheTable) {\n            try {\n                wait();\n            }\n            catch (InterruptedException ignored) {}\n        }\n\n        philosopherUsingThisFork = philosopher;\n\n        forkIsOnTheTable = false;\n\n    }\n\n    public synchronized void put(int philosopher) {\n\n        if (!forkIsOnTheTable &amp;&amp; philosopherUsingThisFork == philosopher) {\n            forkIsOnTheTable = true;\n            notify();\n        }\n\n    }\n\n}\n\n\nFinally, the round table described and illustrated above can be simulated as follows:\n\n// file: \"DiningPhilosophers.java\"\npublic class DiningPhilosophers {\n\n    public static Philosopher[] createPhilosophers(int n) {\n\n        Fork[] forks = new Fork[n];\n\n        for (int i = 0; i &lt; n; i++) {\n            forks[i] = new Fork(i);\n        }\n\n        Philosopher[] philosophers = new Philosopher[n];\n\n        for (int i = 0; i &lt; n; i++) {\n\n            Fork leftFork = forks[i];\n            Fork rightFork = forks[(i + 1) % n];\n\n            philosophers[i] = new Philosopher(i, leftFork, rightFork);\n\n        }\n\n        return philosophers;\n\n    }\n\n    public static void main(String[] args) {\n\n        Philosopher[] philosophers = createPhilosophers(8);\n\n        for (Philosopher philosopher : philosophers) {\n            philosopher.start();\n        }\n\n    }\n\n}\n\n\nBy running the listed program, we can observe different concurrent scenarios of philosophers thinking, competing for forks and eating. For example, I got the following output, after which the program neither terminated nor printed anything else:\n\nPhilosopher 0 is thinking...\nPhilosopher 1 is thinking...\nPhilosopher 2 is thinking...\nPhilosopher 3 is thinking...\nPhilosopher 0 took the right fork 1\nPhilosopher 0 took the left fork 0\nPhilosopher 0 is eating...\nPhilosopher 0 has put down the left fork 0\nPhilosopher 0 has put down the right fork 1\nPhilosopher 1 took the right fork 2\nPhilosopher 1 took the left fork 1\nPhilosopher 1 is eating...\nPhilosopher 1 has put down the left fork 1\nPhilosopher 1 has put down the right fork 2\nPhilosopher 1 is thinking...\nPhilosopher 0 is thinking...\nPhilosopher 2 took the right fork 3\nPhilosopher 4 is thinking...\nPhilosopher 5 is thinking...\nPhilosopher 3 took the right fork 4\nPhilosopher 1 took the right fork 2\nPhilosopher 0 took the right fork 1\nPhilosopher 0 took the left fork 0\nPhilosopher 0 is eating...\nPhilosopher 0 has put down the left fork 0\nPhilosopher 1 took the left fork 1\nPhilosopher 1 is eating...\nPhilosopher 1 has put down the left fork 1\nPhilosopher 1 has put down the right fork 2\nPhilosopher 1 is thinking...\nPhilosopher 2 took the left fork 2\nPhilosopher 2 is eating...\nPhilosopher 2 has put down the left fork 2\nPhilosopher 2 has put down the right fork 3\nPhilosopher 2 is thinking...\nPhilosopher 0 has put down the right fork 1\nPhilosopher 0 is thinking...\nPhilosopher 4 took the right fork 5\nPhilosopher 6 is thinking...\nPhilosopher 7 is thinking...\nPhilosopher 5 took the right fork 6\nPhilosopher 2 took the right fork 3\nPhilosopher 1 took the right fork 2\nPhilosopher 0 took the right fork 1\nPhilosopher 6 took the right fork 7\nPhilosopher 7 took the right fork 0\n\n\nBy reading this output upwards, we can see that the last thing each philosopher did was to take the right fork. After that, all philosophers need to take the left fork, so all of them wait for each other to return the right fork after eating, but no one can eat, so the program comes to a deadlock state.\n\nBy the way, in most cases if you run this program, it will probably hang after a lot more operations, but we can provoke a deadlock (the situation described above) by adding Thread.yield() before leftFork.get(id); in the run() method of the Philosopher class. This will recommend the JVM to reschedule threads after the current philosopher has taken only one fork.\n\nThe simplest possible scenario leading to a deadlock can be illustrated with the following diagram:\n\n\n\nIn this diagram, inclined arrows should be understood as lock-actions where the order in which threads perform them doesn’t matter. On the other hand, the horizontal line means that the threads should be “synchronized” at this point.\n\nAvoiding the deadlock with a semaphore\n\nThe program comes to a deadlock if there are nnn threads, such that the last operation each thread performed is locking a fork. We can use an n−1n-1n−1-permit semaphore to ensure that this never happens:\n\n// file: \"Philosopher.java\"\nimport java.util.concurrent.Semaphore;\n\npublic class Philosopher extends Thread {\n\n    private static final Semaphore semaphore = new Semaphore(7);\n\n    private final int id;\n    private final Fork leftFork;\n    private final Fork rightFork;\n\n    public Philosopher(int id, Fork leftFork, Fork rightFork) {\n        this.id = id;\n        this.leftFork = leftFork;\n        this.rightFork = rightFork;\n    }\n\n    @Override\n    public void run() {\n        for (;;) {\n            System.out.println(\"Philosopher \" + id + \" is thinking...\");\n            semaphore.acquireUninterruptibly();\n            rightFork.take(id);\n            System.out.println(\"Philosopher \" + id + \" took the right fork \" + rightFork.id);\n            leftFork.take(id);\n            semaphore.release();\n            System.out.println(\"Philosopher \" + id + \" took the left fork \" + leftFork.id);\n            System.out.println(\"Philosopher \" + id + \" is eating...\");\n            leftFork.put(id);\n            System.out.println(\"Philosopher \" + id + \" has put down the left fork \" + leftFork.id);\n            rightFork.put(id);\n            System.out.println(\"Philosopher \" + id + \" has put down the right fork \" + rightFork.id);\n        }\n    }\n\n}\n\n\nIn other words, the idea of this solution is to make sure only n−1n - 1n−1 threads at most are “stuck” in the dangerous zone - between lines rightFork.get(id); and leftFork.get(id);. With this change to the Philosopher class, deadlock’s aren’t possible anymore.\n\nOne philosopher can just be left-handed\n\nWe can systematically analyze the deadlock occurring in the program if we consider 4 necessary conditions for a deadlock, according to Edward Coffman:\n\n\n  Mutual exclusion — at least one held resource must be non-sharable.\n  Hold and wait — there exists at least one process holding a resource and waiting for another one.\n  No preemption — resources cannot be preempted. For example, it is impossible to force a thread to release a lock safely.\n  Circular wait — Processes wait for each other in a cycle.\n\n\nTurning to the present problem, we see directly that the first 3 conditions hold.\n\nWe can analyze whether the circular-condition holds by constructing the so-called precedence-graph. For each resource (in this case, for each fork), we create a node. If it is possible, that process PiP_iPi​ in some state locks resource aaa and waits for resource bbb, then node (a,b)(a, b)(a,b) should be added to the graph:\n\n\n\nClearly, there is a cycle in this graph, so a state in which all threads wait for each other is possible (Circular-wait condition holds).\n\nIn order to make the graph acyclic, it is sufficient to just flip the direction of one arbitrary arrow. In terms of the problem, this means changing the behavior of one philosopher to first take the left fork, and then the right one. It doesn’t matter in which order forks are put back on the table. So, this change will prevent deadlocks:\n\n// file: \"Philosopher.java\"\npublic class Philosopher extends Thread {\n\n    private final int id;\n    private final Fork leftFork;\n    private final Fork rightFork;\n\n    public Philosopher(int id, Fork leftFork, Fork rightFork) {\n        this.id = id;\n        this.leftFork = leftFork;\n        this.rightFork = rightFork;\n    }\n\n    @Override\n    public void run() {\n        for (;;) {\n            System.out.println(\"Philosopher \" + id + \" is thinking...\");\n            if (id == 0) {\n                leftFork.take(id);\n                System.out.println(\"Philosopher \" + id + \" took the left fork \" + leftFork.id);\n                rightFork.take(id);\n                System.out.println(\"Philosopher \" + id + \" took the right fork \" + rightFork.id);\n            }\n            else {\n                rightFork.take(id);\n                System.out.println(\"Philosopher \" + id + \" took the right fork \" + rightFork.id);\n                leftFork.take(id);\n                System.out.println(\"Philosopher \" + id + \" took the left fork \" + leftFork.id);\n            }\n            System.out.println(\"Philosopher \" + id + \" is eating...\");\n            leftFork.put(id);\n            System.out.println(\"Philosopher \" + id + \" has put down the left fork \" + leftFork.id);\n            rightFork.put(id);\n            System.out.println(\"Philosopher \" + id + \" has put down the right fork \" + rightFork.id);\n        }\n    }\n\n}\n\n\nThis code changes the behavior of philosopher 0 — now this philosopher first takes the left fork, leading us to the following acyclic precedence graph:\n\n\n\nEven more efficient solution\n\nWhen possible, it is a good idea to get rid of long paths in the precedence graph, because this will improve the overall concurrency of the system. In this case, we can make even philosophers first take the left fork and odd ones — the right fork first. This idea leads us to the following precedence graph:\n\n\n\nand the little change in code:\n\n// file: \"Philosopher.java\"\npublic class Philosopher extends Thread {\n\n    private final int id;\n    private final Fork leftFork;\n    private final Fork rightFork;\n\n    public Philosopher(int id, Fork leftFork, Fork rightFork) {\n        this.id = id;\n        this.leftFork = leftFork;\n        this.rightFork = rightFork;\n    }\n\n    @Override\n    public void run() {\n        for (;;) {\n            System.out.println(\"Philosopher \" + id + \" is thinking...\");\n            if (id % 2 == 0) {\n                leftFork.take(id);\n                System.out.println(\"Philosopher \" + id + \" took the left fork \" + leftFork.id);\n                rightFork.take(id);\n                System.out.println(\"Philosopher \" + id + \" took the right fork \" + rightFork.id);\n            }\n            else {\n                rightFork.take(id);\n                System.out.println(\"Philosopher \" + id + \" took the right fork \" + rightFork.id);\n                leftFork.take(id);\n                System.out.println(\"Philosopher \" + id + \" took the left fork \" + leftFork.id);\n            }\n            System.out.println(\"Philosopher \" + id + \" is eating...\");\n            leftFork.put(id);\n            System.out.println(\"Philosopher \" + id + \" has put down the left fork \" + leftFork.id);\n            rightFork.put(id);\n            System.out.println(\"Philosopher \" + id + \" has put down the right fork \" + rightFork.id);\n        }\n    }\n\n}\n\n",
      "categories": ["cs"],
      "tags": ["concurrency"],
      
      "collection": "posts",
      "url": "/blog/cs/dining-philosophers-problem/"
    },{
      "image": {"path":"/assets/img/blog/gray-code-logic-minimization-tmb.jpg","srcset":{"1920w":"/assets/img/blog/gray-code-logic-minimization-tmb.jpg","960w":"/assets/img/blog/gray-code-logic-minimization-tmb@0,5x.jpg","480w":"/assets/img/blog/gray-code-logic-minimization-tmb@0,25x.jpg","240w":"/assets/img/blog/gray-code-logic-minimization-tmb@0,125x.jpg"}},
      "title": "How propositional logic minimization and gray codes are connected",
      "date": "2021-01-31 00:00:00 +0100",
      
      "content": "In this post I will introduce an interesting connection between gray codes and the problem of minimizing propositional logic formulas in disjunctive normal forms (DNF’s). Both of these concepts are related to the Hamming distance between two binary sequences, which is the amount of bits that need to be flipped in one sequence in order to obtain the other one.\n\nBoolean functions and n-dimensional cubes\n\nAny boolean function with nnn variables can be visualized as some subset of vertices in an nnn-dimensional cube. For example, for n=3n = 3n=3, the cube looks like this:\n\n\n\nIn such cubes every edge connects vertices that correspond to variable assignments differing in exactly one variable. In other words, any disjoint n−1n - 1n−1-dimensional sub-cubes of an nnn-dimensional cube, viewed as boolean cubes (a. k. a. partial variable assignments), are adjacent.\n\nVariable assignments that have the same number of ones can be grouped together:\n\n\n\nIt is easy to see that assignments from the blue segment have hamming distance 2 when compared with the assignments from the red segment. Analogously, this is also true for the green and brown segments. It also holds, that assignments in one segment have hamming distance 2 because of the way we grouped them. This leads us to the idea that boolean cubes are bipartite graphs if we draw edges between variable assignments that have hamming distance 1:\n\n\n\nWith the above described intuition we can now formulate and prove the following statements:\n\nTheorem 1\n\nLet g(x)=an−1…a0g(x) = a_{n-1}\\dots a_{0}g(x)=an−1​…a0​ be the gray code for x:=xn−1…x0x := x_{n-1}\\dots x_{0}x:=xn−1​…x0​ and g(y)=bn−1…b0g(y) = b_{n-1}\\dots b_{0}g(y)=bn−1​…b0​ be the gray code for y:=yn−1…y0y := y_{n-1}\\dots y_{0}y:=yn−1​…y0​. Assuming x≠yx \\neq yx​=y, it holds that:\n\n\n  xxx and yyy are both even ⇒\\Rightarrow⇒ ΔH(g(x),g(y))≥2\\Delta_{\\mathrm{H}}(g(x), g(y)) \\ge 2ΔH​(g(x),g(y))≥2.\n  xxx and yyy are both odd ⇒\\Rightarrow⇒ ΔH(g(x),g(y))≥2\\Delta_{\\mathrm{H}}(g(x), g(y)) \\ge 2ΔH​(g(x),g(y))≥2.\n\n\nProof: by induction on BiB_iBi​ for all i∈N+i \\in \\mathbb{N}^{+}i∈N+ (I’ve defined BiB_iBi​ in this post and proved that this definition is equivalent to another definition of gray codes).\n\nInduction base: (i=1i = 1i=1): There is no way of choosing different xxx and yyy such that at least one assumption above is true, so the statements hold trivially.\n\nInduction base: (i=2i = 2i=2):\n\nΔH(B2(0),B2(2))=ΔH(00,11)=2≥2ΔH(B2(1),B2(3))=ΔH(01,10)=2≥2\\Delta_{\\mathrm{H}}(B_2(0), B_2(2)) = \\Delta_{\\mathrm{H}}(00, 11) = 2 \\ge 2 \\\\\n\\Delta_{\\mathrm{H}}(B_2(1), B_2(3)) = \\Delta_{\\mathrm{H}}(01, 10) = 2 \\ge 2ΔH​(B2​(0),B2​(2))=ΔH​(00,11)=2≥2ΔH​(B2​(1),B2​(3))=ΔH​(01,10)=2≥2\n\nInduction step: (i−1→ii - 1 \\rightarrow ii−1→i):\n\n\n  If both Bi(x)B_i(x)Bi​(x) and Bi(y)B_i(y)Bi​(y) are in the first half of BiB_iBi​, then the statements hold directly by induction hypothesis.\n  If both Bi(x)B_i(x)Bi​(x) and Bi(y)B_i(y)Bi​(y) are in the second half of BiB_iBi​ (formally: 2i−1≤x,y&lt;2i2^{i-1} \\le x, y &lt; 2^i2i−1≤x,y&lt;2i), then, by definition of BiB_iBi​, we have:\n\n\nBi(x)=1⋅Bi−1(2i−1−1−(x−2i−1))=1⋅Bi−1(2i−1−x)Bi(y)=1⋅Bi−1(2i−1−1−(y−2i−1))=1⋅Bi−1(2i−1−y)B_i(x) = 1 \\cdot B_{i-1}(2^{i-1} - 1 - (x - 2^{i-1})) = 1 \\cdot B_{i-1}(2^i - 1 - x) \\\\\nB_i(y) = 1 \\cdot B_{i-1}(2^{i-1} - 1 - (y - 2^{i-1})) = 1 \\cdot B_{i-1}(2^i - 1 - y)Bi​(x)=1⋅Bi−1​(2i−1−1−(x−2i−1))=1⋅Bi−1​(2i−1−x)Bi​(y)=1⋅Bi−1​(2i−1−1−(y−2i−1))=1⋅Bi−1​(2i−1−y)\n\n\n  So, in this case, due to x≠yx \\neq yx​=y it follows that Bi−1(2i−1−x)≠Bi−1(2i−1−y)B_{i-1}(2^i - 1 - x) \\neq B_{i-1}(2^i - 1 - y)Bi−1​(2i−1−x)​=Bi−1​(2i−1−y) and the statements hold by induction hypothesis.\n  Otherwise, without losing generality, we can assume that Bi(x)B_i(x)Bi​(x) is in the first half of BiB_iBi​ and Bi(y)B_i(y)Bi​(y) — in the second half of BiB_iBi​.\n  Formally, this means 0≤x&lt;2i−10 \\le x &lt; 2^{i-1}0≤x&lt;2i−1 and 2i−1≤y&lt;2i2^{i-1} \\le y &lt; 2^i2i−1≤y&lt;2i.\n  By the definition of BiB_iBi​, it holds that Bi(x)=0⋅Bi−1(x)B_i(x) = 0 \\cdot B_{i-1}(x)Bi​(x)=0⋅Bi−1​(x).\n  By the symmetry theorem I proved in this post:\n\n\nBi(y)=110…0⏟i⊕Bi−1(y−2i−1)B_i(y) = \\underbrace{110 \\dots 0}_{i} \\oplus B_{i-1}(y - 2^{i-1})Bi​(y)=i110…0​​⊕Bi−1​(y−2i−1)\n\n\n  \n    If Bi−1(y−2i−1)=Bi−1(x)B_{i-1}(y - 2^{i-1}) = B_{i-1}(x)Bi−1​(y−2i−1)=Bi−1​(x), then both statements are true, because the first bit of Bi−1(y−2i−1)B_{i-1}(y - 2^{i-1})Bi−1​(y−2i−1) needs to be flipped (see formula above) and the leading 1-bit of Bi(x)B_i(x)Bi​(x) should be prepended, meaning that the hamming distance cannot be less than 2.\n  \n  \n    Otherwise, because subtracting 2i−12^{i-1}2i−1 doesn’t affect whether yyy is even or odd, we can use the induction hypothesis for xxx and y−2i−1y - 2^{i-1}y−2i−1 which states that:\n  \n\n\nΔH(Bi−1(x),Bi−1(y−2i−1))≥2\\Delta_{\\mathrm{H}}(B_{i-1}(x), B_{i-1}(y - 2^{i-1})) \\ge 2ΔH​(Bi−1​(x),Bi−1​(y−2i−1))≥2\n\n\n  Unfortunately, the first bit of Bi−1(y−2i−1)B_{i-1}(y - 2^{i-1})Bi−1​(y−2i−1) gets flipped, so for yyy, from the induction hypothesis we can only conclude that:\n\n\nΔH(0⋅Bi−1(x),010…0⏟i⊕Bi−1(y−2i−1))≥1\\Delta_{\\mathrm{H}}(0 \\cdot B_{i-1}(x), \\underbrace{010 \\dots 0}_{i} \\oplus B_{i-1}(y - 2^{i-1})) \\ge 1ΔH​(0⋅Bi−1​(x),i010…0​​⊕Bi−1​(y−2i−1))≥1\n\n\n  \n    However, by definition of Bi(y)B_i(y)Bi​(y) we know that a leading bit gets prepended, which increases the overall hamming distance as Bi(x)=0⋅Bi−1(x)B_i(x) = 0 \\cdot B_{i-1}(x)Bi​(x)=0⋅Bi−1​(x).\n  \n  \n    We therefore conclude that:\n  \n\n\nΔH(0⋅Bi−1(x),010…0⏟i⊕Bi−1(y−2i−1))≥1⇒ΔH(0⋅Bi−1(x),110…0⏟i⊕Bi−1(y−2i−1))≥2⇔ΔH(Bi(x),Bi(y))≥2\\begin{aligned}\n&amp;\\Delta_{\\mathrm{H}}(0 \\cdot B_{i-1}(x), \\underbrace{010 \\dots 0}_{i} \\oplus B_{i-1}(y - 2^{i-1})) \\ge 1 \\\\\n&amp;\\Rightarrow \\Delta_{\\mathrm{H}}(0 \\cdot B_{i-1}(x), \\underbrace{110 \\dots 0}_{i} \\oplus B_{i-1}(y - 2^{i-1})) \\ge 2 \\\\\n&amp;\\Leftrightarrow \\Delta_{\\mathrm{H}}(B_i(x), B_i(y)) \\ge 2\n\\end{aligned}​ΔH​(0⋅Bi−1​(x),i010…0​​⊕Bi−1​(y−2i−1))≥1⇒ΔH​(0⋅Bi−1​(x),i110…0​​⊕Bi−1​(y−2i−1))≥2⇔ΔH​(Bi​(x),Bi​(y))≥2​\n\nCorollary 1\n\nnnn-dimensional boolean cubes are bipartite graphs — we can partition variable assignments based on whether they are gray codes of even numbers or whether they are gray codes of odd numbers. Theorem 1 guarantees that no edges in the nnn-dimensional cube connect vertices inside one group.\n\nHamming distance for boolean cubes\n\nIn order to prove the lemma below, we need to define hamming distance for boolean cubes. Intuitively, 2 cubes have hamming distance 1 iff there exist two assignments (vertices) in the nnn-dimensional cube, such that the hamming distance between them is one, or, in other words, the shortest path between the corresponding vertices is 1.\n\nMore generally speaking, two cubes have hamming distance kkk if the shortest path between some assignment of the first cube cube to some assignment from the second one is kkk. For example, the hamming distance between the following boolean cubes inside an nnn-dimensional cube is 2, because the shortest path (one of them is marked red) has length 2 (ΔH(001,100)=2\\Delta_{\\mathrm{H}}(001, 100) = 2ΔH​(001,100)=2).\n\n\n\nFormally, let φ:V→{0,1,∗}\\varphi : V \\rightarrow \\{0, 1, *\\}φ:V→{0,1,∗} and ψ:V→{0,1,∗}\\psi : V \\rightarrow \\{0, 1, *\\}ψ:V→{0,1,∗} be boolean cubes. We define:\n\nΔH(φ,ψ):=∣{v∈V:φ(v)≠∗,ψ(v)≠∗,φ(v)=1−ψ(v)}∣\\Delta_{\\mathrm{H}}(\\varphi, \\psi) :=\n|\\{v \\in V : \\varphi(v) \\neq *, \\psi(v) \\neq *, \\varphi(v) = 1 - \\psi(v)\\}|ΔH​(φ,ψ):=∣{v∈V:φ(v)​=∗,ψ(v)​=∗,φ(v)=1−ψ(v)}∣\n\nWe will first need the following statements for the proof of the main theorem (theorem 2) below:\n\nLemma 1\n\nA minterm α:V→{0,1}\\alpha : V \\rightarrow \\{0, 1\\}α:V→{0,1} is covered by some other implicant β:V→{0,1,∗}\\beta : V \\rightarrow \\{0, 1, *\\}β:V→{0,1,∗}, β≠α\\beta \\neq \\alphaβ​=α iff there exists some other minterm γ:V→{0,1}\\gamma : V \\rightarrow \\{0, 1\\}γ:V→{0,1}, γ≠α\\gamma \\neq \\alphaγ​=α, such that ΔH(α,γ)=1\\Delta_{\\mathrm{H}}(\\alpha, \\gamma) = 1ΔH​(α,γ)=1.\n\nProof: (⇒\\Rightarrow⇒): β\\betaβ covers α\\alphaα means β(x)≠∗⇒β(x)=α(x)\\beta(x) \\neq * \\Rightarrow \\beta(x) = \\alpha(x)β(x)​=∗⇒β(x)=α(x) for all x∈Vx \\in Vx∈V. As β≠α\\beta \\neq \\alphaβ​=α, there exists at least one variable z∈Vz \\in Vz∈V such that β(z)=∗\\beta(z) = *β(z)=∗. We can thus define γ\\gammaγ as follows:\n\n\n  If β(x)≠∗\\beta(x) \\neq *β(x)​=∗, define γ(x):=α(x)=β(x)\\gamma(x) := \\alpha(x) = \\beta(x)γ(x):=α(x)=β(x).\n  If β(x)=∗\\beta(x) = *β(x)=∗ and x≠zx \\neq zx​=z, define γ(x):=α(x)\\gamma(x) := \\alpha(x)γ(x):=α(x).\n  Finally, define γ(z):=1−α(z)\\gamma(z) := 1 - \\alpha(z)γ(z):=1−α(z).\n\n\nBy construction, it holds that ΔH(α,γ)=1\\Delta_{\\mathrm{H}}(\\alpha, \\gamma) = 1ΔH​(α,γ)=1 and γ≠α\\gamma \\neq \\alphaγ​=α.\n\n(⇐\\Leftarrow⇐): If there exists a γ≠α\\gamma \\neq \\alphaγ​=α with ΔH(α,γ)=1\\Delta_{\\mathrm{H}}(\\alpha, \\gamma) = 1ΔH​(α,γ)=1, then, by definition of ΔH\\Delta_{\\mathrm{H}}ΔH​, there exists exactly one variable z∈Vz \\in Vz∈V such that α(z)=1−γ(z)\\alpha(z) = 1 - \\gamma(z)α(z)=1−γ(z) and for all other variables x∈Vx \\in Vx∈V, x≠zx \\neq zx​=z it holds that α(x)=γ(z)\\alpha(x) = \\gamma(z)α(x)=γ(z). We define β\\betaβ accordingly:\n\n\n  If x≠zx \\neq zx​=z, set β(x):=α(x)=γ(z)\\beta(x) := \\alpha(x) = \\gamma(z)β(x):=α(x)=γ(z).\n  Set β(z):=∗\\beta(z) := *β(z):=∗.\n\n\nBy construction, β≠α\\beta \\neq \\alphaβ​=α covers α\\alphaα.\n\nCorollary 2\n\nA minterm α:V→{0,1}\\alpha : V \\rightarrow \\{0, 1\\}α:V→{0,1} is a prime implicant iff for all other minterms γ:V→{0,1}\\gamma : V \\rightarrow \\{0, 1\\}γ:V→{0,1}, γ≠α\\gamma \\neq \\alphaγ​=α the hamming distance ΔH(α,γ)≥2\\Delta_{\\mathrm{H}}(\\alpha, \\gamma) \\ge 2ΔH​(α,γ)≥2.\n\nProof: By definition, a cube is a prime implicant iff it is not covered by any other implicant. Accordingly, a minterm α:V→{0,1}\\alpha : V \\rightarrow \\{0, 1\\}α:V→{0,1} (each minterm is also a cube) is a prime implicant iff it is not covered by any other implicant. Not being covered by any other implicant means, by the lemma 1 above, that for all other minterms γ:V→{0,1}\\gamma : V \\rightarrow \\{0, 1\\}γ:V→{0,1}, γ≠α\\gamma \\neq \\alphaγ​=α the hamming distance ΔH(α,γ)≥2\\Delta_{\\mathrm{H}}(\\alpha, \\gamma) \\ge 2ΔH​(α,γ)≥2.\n\nCorollary 3\n\nThere exist at most 2n−12^{n-1}2n−1 models that are prime implicants.\n\nProof: Corollary 2, taken in conjunction with the fact that every vertex in the nnn-dimensional cube has nnn adjacent vertices, implies that it is impossible to choose k&gt;2n−1k &gt; 2^{n-1}k&gt;2n−1 models (out of 2n2^n2n variable assignments) such that no models among them have hamming distance 1. Accordingly, there cannot exist k&gt;2n−1k &gt; 2^{n-1}k&gt;2n−1 (different) models that are prime implicants.\n\nCorollary 4\n\nThere are at most 2n−12^{n-1}2n−1 (distinct) essential prime implicants.\n\nProof:\n\n\n  Suppose we managed to pick k&gt;2n−1k &gt; 2^{n-1}k&gt;2n−1 essential prime implicants — let the set PPP contain them — ∣P∣=k\\st P \\st = k∣P∣=k.\n  Every p∈Pp \\in Pp∈P is essential. Intuitively, this means that it must contribute at least one model that other essential prime implicants in PPP cannot contribute.\n  Formally, it holds that for every p∈Pp \\in Pp∈P there exists a model mpm_pmp​, such that there is no other q∈Pq \\in Pq∈P, q≠pq \\neq pq​=p, that contains this model (M(q)M(q)M(q) denotes the set of models qqq covers):\n\n\n∀p,q∈P:p≠q:mp∉M(q)\\forall p, q \\in P : p \\neq q : m_p \\notin M(q)∀p,q∈P:p​=q:mp​∈/​M(q)\n\n\n  It follows that models {mp:p∈P}\\{m_p : p \\in P\\}{mp​:p∈P} must have hamming distance 1 among each other.\n  This means, by corollary 2, that all these models by themselves must be prime implicants.\n  We get a contradiction, because corollary 3 states that it is impossible to choose k&gt;2n−1k &gt; 2^{n-1}k&gt;2n−1 models that are prime implicants.\n\n\nTheorem 2\n\nFor any n∈N+n \\in \\mathbb{N}^+n∈N+, there exist exactly 2 boolean functions φ\\varphiφ and ψ\\psiψ with nnn variables, such that the smallest possible DNF’s describing these functions consist of 2n−12^{n-1}2n−1 terms and each term has exactly nnn literals. All other boolean functions with nnn variables have smaller minimal DNF’s.\n\nMoreover, φ\\varphiφ and ψ\\psiψ have the following properties:\n\nφ(xn−1,…,x0)=1⇔∃k∈N0:xn−1…x0=g(2⋅k)ψ(xn−1,…,x0)=1⇔∃k∈N0:xn−1…x0=g(2⋅k+1)φ≡¬ψφ(xn−1,…,x0)≡xn−1⊕⋯⊕x0ψ(xn−1,…,x0)≡xn−1↔⋯↔x0\\begin{aligned}\n\\varphi(x_{n-1}, \\dots, x_0) = 1 &amp;\\Leftrightarrow\n\\exists k \\in \\mathbb{N}_0 : x_{n-1}\\dots x_0 = g(2 \\cdot k) \\\\\n\\psi(x_{n-1}, \\dots, x_0) = 1 &amp;\\Leftrightarrow\n\\exists k \\in \\mathbb{N}_0 : x_{n-1}\\dots x_0 = g(2 \\cdot k + 1) \\\\\n\\varphi &amp;\\equiv \\neg \\psi \\\\\n\\varphi(x_{n-1}, \\dots, x_0) &amp;\\equiv x_{n-1} \\oplus \\dots \\oplus x_0 \\\\\n\\psi(x_{n-1}, \\dots, x_0) &amp;\\equiv x_{n-1} \\leftrightarrow \\dots \\leftrightarrow x_0\n\\end{aligned}φ(xn−1​,…,x0​)=1ψ(xn−1​,…,x0​)=1φφ(xn−1​,…,x0​)ψ(xn−1​,…,x0​)​⇔∃k∈N0​:xn−1​…x0​=g(2⋅k)⇔∃k∈N0​:xn−1​…x0​=g(2⋅k+1)≡¬ψ≡xn−1​⊕⋯⊕x0​≡xn−1​↔⋯↔x0​​\n\nProof: Each boolean function is a set of models. Clearly, this set is a subset of the set of vertices of an nnn-dimensional cube, built as described at the beginning of this post.\n\nBy the theorem 1 above, stating that gray codes for even numbers have hamming distance at least 2 among each other, combined with corollary 2, it follows that all 2n−12^{n-1}2n−1 models of φ\\varphiφ are prime implicants. Analogously, all 2n−12^{n-1}2n−1 models of ψ\\psiψ are prime implicants. Corollary 3 states, that there exist at most 2n−12^{n-1}2n−1 models that are prime implicants, so we conclude that φ\\varphiφ and ψ\\psiψ reach this maximum.\n\nWe now argue why it also follows that there are no other ways of choosing 2n−12^{n-1}2n−1 models that are prime implicants, apart from the ways models for φ\\varphiφ and ψ\\psiψ — M(φ)M(\\varphi)M(φ) and M(ψ)M(\\psi)M(ψ) are chosen:\n\n\n  Assume we found 2n−12^{n-1}2n−1 models which are prime implicants. Let AAA be the set of these models.\n  Also, assume A≠M(φ)A \\neq M(\\varphi)A​=M(φ) and A≠M(ψ)A \\neq M(\\psi)A​=M(ψ).\n  This means that A∩M(φ)≠∅A \\cap M(\\varphi) \\neq \\varnothingA∩M(φ)​=∅ and A∩M(ψ)≠∅A \\cap M(\\psi) \\neq \\varnothingA∩M(ψ)​=∅, or, in other words, there exist variable models i,j∈Ai,j \\in Ai,j∈A, i≠ji \\neq ji​=j, such that i∈M(φ)i \\in M(\\varphi)i∈M(φ) and j∈M(ψ)j \\in M(\\psi)j∈M(ψ).\n  As i∈M(φ)i \\in M(\\varphi)i∈M(φ), because a vertex in an nnn-dimensional cube has nnn adjacent vertices, there exists Nψ⊆M(ψ)N_\\psi \\subseteq M(\\psi)Nψ​⊆M(ψ) with ∣Nψ∣=n\\st N_\\psi \\st = n∣Nψ​∣=n and Nψ∩A=∅N_\\psi \\cap A = \\varnothingNψ​∩A=∅.\n  If it is not he case, that Nψ∩A=∅N_\\psi \\cap A = \\varnothingNψ​∩A=∅ (for example, if j∈Nψj \\in N_\\psij∈Nψ​), then we directly get a contradiction.\n  Analogously, there exists Nφ⊆M(ψ)N_\\varphi \\subseteq M(\\psi)Nφ​⊆M(ψ) with ∣Nφ∣=n\\st N_\\varphi \\st = n∣Nφ​∣=n and Nφ∩A=∅N_\\varphi \\cap A = \\varnothingNφ​∩A=∅.\n  Therefore, it holds that A⊆M\\(Nφ∪Nψ)A \\subseteq M \\backslash (N_\\varphi \\cup N_\\psi)A⊆M\\(Nφ​∪Nψ​), where MMM is the set of all models. Note that NφN_\\varphiNφ​ and NψN_\\psiNψ​ are disjoint. In other words, AAA contains models from the remaining 2n−2⋅n2^n - 2 \\cdot n2n−2⋅n vertices of the cube.\n  Suppose we we know all 2n−1−22^{n-1} - 22n−1−2 vertices apart from iii and jjj that are in AAA (these nodes are “chosen” out of 2n−2⋅n−22^n - 2 \\cdot n - 22n−2⋅n−2 remaining vertices).\n  We now estimate the maximum number of edges that are not inside AAA, i.e. that either connect nodes outside AAA with each other or connect nodes inside AAA with nodes outside of AAA:\n\n\n(2n−2⋅n−2n−1+n)⋅n=(2n−2n−1−n)⋅n=(2n−1−n)⋅n\\begin{aligned}\n(2^n - 2 \\cdot n - 2^{n-1} + n) \\cdot n\n&amp;= (2^n - 2^{n-1} - n) \\cdot n \\\\\n&amp;= (2^{n-1} - n) \\cdot n\n\\end{aligned}(2n−2⋅n−2n−1+n)⋅n​=(2n−2n−1−n)⋅n=(2n−1−n)⋅n​\n\n\n  There are 2n−1⋅n2^{n-1} \\cdot n2n−1⋅n edges total. So, at least the following amount of edges must connect nodes inside AAA:\n\n\n2n−1⋅n−(2n−1−n)⋅n=(2n−1−(2n−1−n))⋅n=(2n−1−2n−1+n)⋅n=n2\\begin{aligned}\n2^{n-1} \\cdot n - (2^{n-1} - n) \\cdot n\n&amp;= (2^{n-1} - (2^{n-1} - n)) \\cdot n \\\\\n&amp;= (2^{n-1} - 2^{n-1} + n) \\cdot n \\\\\n&amp;= n^2\n\\end{aligned}2n−1⋅n−(2n−1−n)⋅n​=(2n−1−(2n−1−n))⋅n=(2n−1−2n−1+n)⋅n=n2​\n\n\n  We therefore get a contradiction.\n\n\nThe above reasoning can be visualized for n=5n = 5n=5 as follows (we consider the worst-case AAA set in this visualization):\n\n\n\nIt follows from the above that φ\\varphiφ and ψ\\psiψ are exactly the boolean functions which have the most models that are prime implicants — all other boolean functions have less.\n\nNow, we will need the Quine’s Theorem, which reads as follows:\n\n\n  Each minimal disjunction of cubes is a disjunction of prime implicants.\n\n\nIn the present context, because of corollary 4 stating that there are at most 2n−12^{n-1}2n−1 essential prime implicants, this means that every minimal DNF has at most 2n−12^{n-1}2n−1 terms (each term satisfies the whole formula iff the variable assignment is described by the corresponding prime implicant).\n\nAll (essential) prime implicants of φ\\varphiφ and ψ\\psiψ are models, so each term in the minimal DNF is guaranteed to have nnn literals, making it the maximal DNF possible:\n\n\n  If some other boolean function has less than 2n−12^{n-1}2n−1 essential prime implicants, it will have less than 2n−12^{n-1}2n−1 terms in the minimal DNF.\n  If some other boolean function has 2n−12^{n-1}2n−1 essential prime implicants, then there will be at least one cube of order at least 1, making the corresponding term of the minimal DNF have less than nnn literals.\n\n\nThe properties of the φ\\varphiφ and ψ\\psiψ functions mentioned in the statement of this theorem hold by construction of the models — gray codes of even numbers have an even number of ones and gray codes of odd numbers have an odd number of ones. This is not hard to prove — but it is also not necessary at this point because of the above argument about the uniqueness of the sets of models. This was also the idea I mentioned at the beginning of the post. It also follows that φ≡¬ψ\\varphi \\equiv \\neg \\psiφ≡¬ψ, because both φ\\varphiφ and ψ\\psiψ have 2n−12^{n-1}2n−1 disjoint models out of 2n2^n2n possible variable assignments.\n\nThis completes the proof.\n\nExample\n\nConsider all boolean functions with n:=3n := 3n:=3 variables. There are 2(2n)=28=2562^{(2^n)} = 2^8 = 2562(2n)=28=256 such functions. These are the minimal DNF’s of all these 256 functions (I computed them with the Quine-McCluskey algorithm), the first 8 columns of the table represent the right-hand side of the truth table. The order of the assignments in the truth table is the order in which we count in binary. Please note there may be different minimal DNF’s that correspond to a particular boolean function, if that is the case then the corresponding table entry contains one of them.\n\n\n  \n    \n      R1\n      R2\n      R3\n      R4\n      R5\n      R6\n      R7\n      R8\n      Minimal DNF\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      a ∧ b ∧ c\n    \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      a ∧ b ∧ ¬c\n    \n    \n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      a ∧ b\n    \n    \n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      a ∧ ¬b ∧ c\n    \n    \n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      a ∧ c\n    \n    \n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      a ∧ ¬b ∧ c ∨ a ∧ b ∧ ¬c\n    \n    \n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      a ∧ c ∨ a ∧ b\n    \n    \n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      a ∧ ¬b ∧ ¬c\n    \n    \n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      a ∧ ¬b ∧ ¬c ∨ a ∧ b ∧ c\n    \n    \n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      a ∧ ¬c\n    \n    \n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      a ∧ ¬c ∨ a ∧ b\n    \n    \n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      a ∧ ¬b\n    \n    \n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      a ∧ ¬b ∨ a ∧ c\n    \n    \n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      a ∧ ¬b ∨ a ∧ ¬c\n    \n    \n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      1\n      a\n    \n    \n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ¬a ∧ b ∧ c\n    \n    \n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      b ∧ c\n    \n    \n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      ¬a ∧ b ∧ c ∨ a ∧ b ∧ ¬c\n    \n    \n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      1\n      b ∧ c ∨ a ∧ b\n    \n    \n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      ¬a ∧ b ∧ c ∨ a ∧ ¬b ∧ c\n    \n    \n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      b ∧ c ∨ a ∧ c\n    \n    \n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      ¬a ∧ b ∧ c ∨ a ∧ ¬b ∧ c ∨ a ∧ b ∧ ¬c\n    \n    \n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      1\n      b ∧ c ∨ a ∧ c ∨ a ∧ b\n    \n    \n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      a ∧ ¬b ∧ ¬c ∨ ¬a ∧ b ∧ c\n    \n    \n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      a ∧ ¬b ∧ ¬c ∨ b ∧ c\n    \n    \n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      ¬a ∧ b ∧ c ∨ a ∧ ¬c\n    \n    \n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n      a ∧ ¬c ∨ b ∧ c\n    \n    \n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      0\n      ¬a ∧ b ∧ c ∨ a ∧ ¬b\n    \n    \n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      a ∧ ¬b ∨ b ∧ c\n    \n    \n      0\n      0\n      0\n      1\n      1\n      1\n      1\n      0\n      ¬a ∧ b ∧ c ∨ a ∧ ¬b ∨ a ∧ ¬c\n    \n    \n      0\n      0\n      0\n      1\n      1\n      1\n      1\n      1\n      b ∧ c ∨ a\n    \n    \n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      ¬a ∧ b ∧ ¬c\n    \n    \n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      ¬a ∧ b ∧ ¬c ∨ a ∧ b ∧ c\n    \n    \n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      b ∧ ¬c\n    \n    \n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      1\n      b ∧ ¬c ∨ a ∧ b\n    \n    \n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ¬a ∧ b ∧ ¬c ∨ a ∧ ¬b ∧ c\n    \n    \n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      ¬a ∧ b ∧ ¬c ∨ a ∧ c\n    \n    \n      0\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      a ∧ ¬b ∧ c ∨ b ∧ ¬c\n    \n    \n      0\n      0\n      1\n      0\n      0\n      1\n      1\n      1\n      b ∧ ¬c ∨ a ∧ c\n    \n    \n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      ¬a ∧ b ∧ ¬c ∨ a ∧ ¬b ∧ ¬c\n    \n    \n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      ¬a ∧ b ∧ ¬c ∨ a ∧ ¬b ∧ ¬c ∨ a ∧ b ∧ c\n    \n    \n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      b ∧ ¬c ∨ a ∧ ¬c\n    \n    \n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      b ∧ ¬c ∨ a ∧ ¬c ∨ a ∧ b\n    \n    \n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      ¬a ∧ b ∧ ¬c ∨ a ∧ ¬b\n    \n    \n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      ¬a ∧ b ∧ ¬c ∨ a ∧ ¬b ∨ a ∧ c\n    \n    \n      0\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      b ∧ ¬c ∨ a ∧ ¬b\n    \n    \n      0\n      0\n      1\n      0\n      1\n      1\n      1\n      1\n      b ∧ ¬c ∨ a\n    \n    \n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      ¬a ∧ b\n    \n    \n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      ¬a ∧ b ∨ b ∧ c\n    \n    \n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      ¬a ∧ b ∨ b ∧ ¬c\n    \n    \n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      1\n      b\n    \n    \n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      a ∧ ¬b ∧ c ∨ ¬a ∧ b\n    \n    \n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n      ¬a ∧ b ∨ a ∧ c\n    \n    \n      0\n      0\n      1\n      1\n      0\n      1\n      1\n      0\n      a ∧ ¬b ∧ c ∨ ¬a ∧ b ∨ b ∧ ¬c\n    \n    \n      0\n      0\n      1\n      1\n      0\n      1\n      1\n      1\n      a ∧ c ∨ b\n    \n    \n      0\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      a ∧ ¬b ∧ ¬c ∨ ¬a ∧ b\n    \n    \n      0\n      0\n      1\n      1\n      1\n      0\n      0\n      1\n      a ∧ ¬b ∧ ¬c ∨ ¬a ∧ b ∨ b ∧ c\n    \n    \n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n      ¬a ∧ b ∨ a ∧ ¬c\n    \n    \n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      1\n      a ∧ ¬c ∨ b\n    \n    \n      0\n      0\n      1\n      1\n      1\n      1\n      0\n      0\n      ¬a ∧ b ∨ a ∧ ¬b\n    \n    \n      0\n      0\n      1\n      1\n      1\n      1\n      0\n      1\n      ¬a ∧ b ∨ a ∧ ¬b ∨ a ∧ c\n    \n    \n      0\n      0\n      1\n      1\n      1\n      1\n      1\n      0\n      ¬a ∧ b ∨ a ∧ ¬b ∨ a ∧ ¬c\n    \n    \n      0\n      0\n      1\n      1\n      1\n      1\n      1\n      1\n      b ∨ a\n    \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      ¬a ∧ ¬b ∧ c\n    \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      ¬a ∧ ¬b ∧ c ∨ a ∧ b ∧ c\n    \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      ¬a ∧ ¬b ∧ c ∨ a ∧ b ∧ ¬c\n    \n    \n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      ¬a ∧ ¬b ∧ c ∨ a ∧ b\n    \n    \n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ¬b ∧ c\n    \n    \n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      ¬b ∧ c ∨ a ∧ c\n    \n    \n      0\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      a ∧ b ∧ ¬c ∨ ¬b ∧ c\n    \n    \n      0\n      1\n      0\n      0\n      0\n      1\n      1\n      1\n      ¬b ∧ c ∨ a ∧ b\n    \n    \n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      ¬a ∧ ¬b ∧ c ∨ a ∧ ¬b ∧ ¬c\n    \n    \n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      ¬a ∧ ¬b ∧ c ∨ a ∧ ¬b ∧ ¬c ∨ a ∧ b ∧ c\n    \n    \n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      ¬a ∧ ¬b ∧ c ∨ a ∧ ¬c\n    \n    \n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      ¬a ∧ ¬b ∧ c ∨ a ∧ ¬c ∨ a ∧ b\n    \n    \n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      ¬b ∧ c ∨ a ∧ ¬b\n    \n    \n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      1\n      ¬b ∧ c ∨ a ∧ ¬b ∨ a ∧ c\n    \n    \n      0\n      1\n      0\n      0\n      1\n      1\n      1\n      0\n      ¬b ∧ c ∨ a ∧ ¬c\n    \n    \n      0\n      1\n      0\n      0\n      1\n      1\n      1\n      1\n      ¬b ∧ c ∨ a\n    \n    \n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      ¬a ∧ c\n    \n    \n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      ¬a ∧ c ∨ b ∧ c\n    \n    \n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      a ∧ b ∧ ¬c ∨ ¬a ∧ c\n    \n    \n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      1\n      ¬a ∧ c ∨ a ∧ b\n    \n    \n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      ¬a ∧ c ∨ ¬b ∧ c\n    \n    \n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n      c\n    \n    \n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n      a ∧ b ∧ ¬c ∨ ¬a ∧ c ∨ ¬b ∧ c\n    \n    \n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      a ∧ b ∨ c\n    \n    \n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      a ∧ ¬b ∧ ¬c ∨ ¬a ∧ c\n    \n    \n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      a ∧ ¬b ∧ ¬c ∨ ¬a ∧ c ∨ b ∧ c\n    \n    \n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      0\n      ¬a ∧ c ∨ a ∧ ¬c\n    \n    \n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n      ¬a ∧ c ∨ a ∧ ¬c ∨ a ∧ b\n    \n    \n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      ¬a ∧ c ∨ a ∧ ¬b\n    \n    \n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      1\n      a ∧ ¬b ∨ c\n    \n    \n      0\n      1\n      0\n      1\n      1\n      1\n      1\n      0\n      ¬a ∧ c ∨ a ∧ ¬b ∨ a ∧ ¬c\n    \n    \n      0\n      1\n      0\n      1\n      1\n      1\n      1\n      1\n      c ∨ a\n    \n    \n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      ¬a ∧ ¬b ∧ c ∨ ¬a ∧ b ∧ ¬c\n    \n    \n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      ¬a ∧ ¬b ∧ c ∨ ¬a ∧ b ∧ ¬c ∨ a ∧ b ∧ c\n    \n    \n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      ¬a ∧ ¬b ∧ c ∨ b ∧ ¬c\n    \n    \n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      1\n      ¬a ∧ ¬b ∧ c ∨ b ∧ ¬c ∨ a ∧ b\n    \n    \n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      0\n      ¬a ∧ b ∧ ¬c ∨ ¬b ∧ c\n    \n    \n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      ¬a ∧ b ∧ ¬c ∨ ¬b ∧ c ∨ a ∧ c\n    \n    \n      0\n      1\n      1\n      0\n      0\n      1\n      1\n      0\n      ¬b ∧ c ∨ b ∧ ¬c\n    \n    \n      0\n      1\n      1\n      0\n      0\n      1\n      1\n      1\n      ¬b ∧ c ∨ b ∧ ¬c ∨ a ∧ b\n    \n    \n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      ¬a ∧ ¬b ∧ c ∨ ¬a ∧ b ∧ ¬c ∨ a ∧ ¬b ∧ ¬c\n    \n    \n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      ¬a ∧ ¬b ∧ c ∨ ¬a ∧ b ∧ ¬c ∨ a ∧ ¬b ∧ ¬c ∨ a ∧ b ∧ c\n    \n    \n      0\n      1\n      1\n      0\n      1\n      0\n      1\n      0\n      ¬a ∧ ¬b ∧ c ∨ b ∧ ¬c ∨ a ∧ ¬c\n    \n    \n      0\n      1\n      1\n      0\n      1\n      0\n      1\n      1\n      ¬a ∧ ¬b ∧ c ∨ b ∧ ¬c ∨ a ∧ ¬c ∨ a ∧ b\n    \n    \n      0\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      ¬a ∧ b ∧ ¬c ∨ ¬b ∧ c ∨ a ∧ ¬b\n    \n    \n      0\n      1\n      1\n      0\n      1\n      1\n      0\n      1\n      ¬a ∧ b ∧ ¬c ∨ ¬b ∧ c ∨ a ∧ ¬b ∨ a ∧ c\n    \n    \n      0\n      1\n      1\n      0\n      1\n      1\n      1\n      0\n      ¬b ∧ c ∨ b ∧ ¬c ∨ a ∧ ¬c\n    \n    \n      0\n      1\n      1\n      0\n      1\n      1\n      1\n      1\n      ¬b ∧ c ∨ b ∧ ¬c ∨ a\n    \n    \n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      ¬a ∧ c ∨ ¬a ∧ b\n    \n    \n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      1\n      ¬a ∧ c ∨ ¬a ∧ b ∨ b ∧ c\n    \n    \n      0\n      1\n      1\n      1\n      0\n      0\n      1\n      0\n      ¬a ∧ c ∨ b ∧ ¬c\n    \n    \n      0\n      1\n      1\n      1\n      0\n      0\n      1\n      1\n      ¬a ∧ c ∨ b\n    \n    \n      0\n      1\n      1\n      1\n      0\n      1\n      0\n      0\n      ¬b ∧ c ∨ ¬a ∧ b\n    \n    \n      0\n      1\n      1\n      1\n      0\n      1\n      0\n      1\n      ¬a ∧ b ∨ c\n    \n    \n      0\n      1\n      1\n      1\n      0\n      1\n      1\n      0\n      ¬b ∧ c ∨ ¬a ∧ b ∨ b ∧ ¬c\n    \n    \n      0\n      1\n      1\n      1\n      0\n      1\n      1\n      1\n      c ∨ b\n    \n    \n      0\n      1\n      1\n      1\n      1\n      0\n      0\n      0\n      a ∧ ¬b ∧ ¬c ∨ ¬a ∧ c ∨ ¬a ∧ b\n    \n    \n      0\n      1\n      1\n      1\n      1\n      0\n      0\n      1\n      a ∧ ¬b ∧ ¬c ∨ ¬a ∧ c ∨ ¬a ∧ b ∨ b ∧ c\n    \n    \n      0\n      1\n      1\n      1\n      1\n      0\n      1\n      0\n      ¬a ∧ c ∨ b ∧ ¬c ∨ a ∧ ¬c\n    \n    \n      0\n      1\n      1\n      1\n      1\n      0\n      1\n      1\n      ¬a ∧ c ∨ a ∧ ¬c ∨ b\n    \n    \n      0\n      1\n      1\n      1\n      1\n      1\n      0\n      0\n      ¬b ∧ c ∨ ¬a ∧ b ∨ a ∧ ¬b\n    \n    \n      0\n      1\n      1\n      1\n      1\n      1\n      0\n      1\n      ¬a ∧ b ∨ a ∧ ¬b ∨ c\n    \n    \n      0\n      1\n      1\n      1\n      1\n      1\n      1\n      0\n      ¬a ∧ c ∨ b ∧ ¬c ∨ a ∧ ¬b\n    \n    \n      0\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      c ∨ b ∨ a\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ¬a ∧ ¬b ∧ ¬c\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      1\n      ¬a ∧ ¬b ∧ ¬c ∨ a ∧ b ∧ c\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      ¬a ∧ ¬b ∧ ¬c ∨ a ∧ b ∧ ¬c\n    \n    \n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      1\n      ¬a ∧ ¬b ∧ ¬c ∨ a ∧ b\n    \n    \n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ¬a ∧ ¬b ∧ ¬c ∨ a ∧ ¬b ∧ c\n    \n    \n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      ¬a ∧ ¬b ∧ ¬c ∨ a ∧ c\n    \n    \n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      ¬a ∧ ¬b ∧ ¬c ∨ a ∧ ¬b ∧ c ∨ a ∧ b ∧ ¬c\n    \n    \n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      1\n      ¬a ∧ ¬b ∧ ¬c ∨ a ∧ c ∨ a ∧ b\n    \n    \n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      0\n      ¬b ∧ ¬c\n    \n    \n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      a ∧ b ∧ c ∨ ¬b ∧ ¬c\n    \n    \n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      ¬b ∧ ¬c ∨ a ∧ ¬c\n    \n    \n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n      ¬b ∧ ¬c ∨ a ∧ b\n    \n    \n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      ¬b ∧ ¬c ∨ a ∧ ¬b\n    \n    \n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      ¬b ∧ ¬c ∨ a ∧ c\n    \n    \n      1\n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      ¬b ∧ ¬c ∨ a ∧ ¬b ∨ a ∧ ¬c\n    \n    \n      1\n      0\n      0\n      0\n      1\n      1\n      1\n      1\n      ¬b ∧ ¬c ∨ a\n    \n    \n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ¬a ∧ ¬b ∧ ¬c ∨ ¬a ∧ b ∧ c\n    \n    \n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      ¬a ∧ ¬b ∧ ¬c ∨ b ∧ c\n    \n    \n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      ¬a ∧ ¬b ∧ ¬c ∨ ¬a ∧ b ∧ c ∨ a ∧ b ∧ ¬c\n    \n    \n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      1\n      ¬a ∧ ¬b ∧ ¬c ∨ b ∧ c ∨ a ∧ b\n    \n    \n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      ¬a ∧ ¬b ∧ ¬c ∨ ¬a ∧ b ∧ c ∨ a ∧ ¬b ∧ c\n    \n    \n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      ¬a ∧ ¬b ∧ ¬c ∨ b ∧ c ∨ a ∧ c\n    \n    \n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      ¬a ∧ ¬b ∧ ¬c ∨ ¬a ∧ b ∧ c ∨ a ∧ ¬b ∧ c ∨ a ∧ b ∧ ¬c\n    \n    \n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      1\n      ¬a ∧ ¬b ∧ ¬c ∨ b ∧ c ∨ a ∧ c ∨ a ∧ b\n    \n    \n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      ¬a ∧ b ∧ c ∨ ¬b ∧ ¬c\n    \n    \n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      ¬b ∧ ¬c ∨ b ∧ c\n    \n    \n      1\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      ¬a ∧ b ∧ c ∨ ¬b ∧ ¬c ∨ a ∧ ¬c\n    \n    \n      1\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n      ¬b ∧ ¬c ∨ b ∧ c ∨ a ∧ b\n    \n    \n      1\n      0\n      0\n      1\n      1\n      1\n      0\n      0\n      ¬a ∧ b ∧ c ∨ ¬b ∧ ¬c ∨ a ∧ ¬b\n    \n    \n      1\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      ¬b ∧ ¬c ∨ b ∧ c ∨ a ∧ c\n    \n    \n      1\n      0\n      0\n      1\n      1\n      1\n      1\n      0\n      ¬a ∧ b ∧ c ∨ ¬b ∧ ¬c ∨ a ∧ ¬b ∨ a ∧ ¬c\n    \n    \n      1\n      0\n      0\n      1\n      1\n      1\n      1\n      1\n      ¬b ∧ ¬c ∨ b ∧ c ∨ a\n    \n    \n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      ¬a ∧ ¬c\n    \n    \n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      a ∧ b ∧ c ∨ ¬a ∧ ¬c\n    \n    \n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      ¬a ∧ ¬c ∨ b ∧ ¬c\n    \n    \n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      1\n      ¬a ∧ ¬c ∨ a ∧ b\n    \n    \n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      a ∧ ¬b ∧ c ∨ ¬a ∧ ¬c\n    \n    \n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      ¬a ∧ ¬c ∨ a ∧ c\n    \n    \n      1\n      0\n      1\n      0\n      0\n      1\n      1\n      0\n      a ∧ ¬b ∧ c ∨ ¬a ∧ ¬c ∨ b ∧ ¬c\n    \n    \n      1\n      0\n      1\n      0\n      0\n      1\n      1\n      1\n      ¬a ∧ ¬c ∨ a ∧ c ∨ a ∧ b\n    \n    \n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      ¬a ∧ ¬c ∨ ¬b ∧ ¬c\n    \n    \n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      a ∧ b ∧ c ∨ ¬a ∧ ¬c ∨ ¬b ∧ ¬c\n    \n    \n      1\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      ¬c\n    \n    \n      1\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      a ∧ b ∨ ¬c\n    \n    \n      1\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      ¬a ∧ ¬c ∨ a ∧ ¬b\n    \n    \n      1\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      ¬a ∧ ¬c ∨ a ∧ ¬b ∨ a ∧ c\n    \n    \n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      0\n      a ∧ ¬b ∨ ¬c\n    \n    \n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      1\n      ¬c ∨ a\n    \n    \n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      0\n      ¬a ∧ ¬c ∨ ¬a ∧ b\n    \n    \n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      ¬a ∧ ¬c ∨ b ∧ c\n    \n    \n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      ¬a ∧ ¬c ∨ ¬a ∧ b ∨ b ∧ ¬c\n    \n    \n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      1\n      ¬a ∧ ¬c ∨ b\n    \n    \n      1\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      a ∧ ¬b ∧ c ∨ ¬a ∧ ¬c ∨ ¬a ∧ b\n    \n    \n      1\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n      ¬a ∧ ¬c ∨ b ∧ c ∨ a ∧ c\n    \n    \n      1\n      0\n      1\n      1\n      0\n      1\n      1\n      0\n      a ∧ ¬b ∧ c ∨ ¬a ∧ ¬c ∨ ¬a ∧ b ∨ b ∧ ¬c\n    \n    \n      1\n      0\n      1\n      1\n      0\n      1\n      1\n      1\n      ¬a ∧ ¬c ∨ a ∧ c ∨ b\n    \n    \n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      0\n      ¬b ∧ ¬c ∨ ¬a ∧ b\n    \n    \n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      1\n      ¬b ∧ ¬c ∨ ¬a ∧ b ∨ b ∧ c\n    \n    \n      1\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n      ¬a ∧ b ∨ ¬c\n    \n    \n      1\n      0\n      1\n      1\n      1\n      0\n      1\n      1\n      ¬c ∨ b\n    \n    \n      1\n      0\n      1\n      1\n      1\n      1\n      0\n      0\n      ¬b ∧ ¬c ∨ ¬a ∧ b ∨ a ∧ ¬b\n    \n    \n      1\n      0\n      1\n      1\n      1\n      1\n      0\n      1\n      ¬a ∧ ¬c ∨ a ∧ ¬b ∨ b ∧ c\n    \n    \n      1\n      0\n      1\n      1\n      1\n      1\n      1\n      0\n      ¬a ∧ b ∨ a ∧ ¬b ∨ ¬c\n    \n    \n      1\n      0\n      1\n      1\n      1\n      1\n      1\n      1\n      ¬c ∨ b ∨ a\n    \n    \n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      ¬a ∧ ¬b\n    \n    \n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      a ∧ b ∧ c ∨ ¬a ∧ ¬b\n    \n    \n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      a ∧ b ∧ ¬c ∨ ¬a ∧ ¬b\n    \n    \n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      ¬a ∧ ¬b ∨ a ∧ b\n    \n    \n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ¬a ∧ ¬b ∨ ¬b ∧ c\n    \n    \n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      ¬a ∧ ¬b ∨ a ∧ c\n    \n    \n      1\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      a ∧ b ∧ ¬c ∨ ¬a ∧ ¬b ∨ ¬b ∧ c\n    \n    \n      1\n      1\n      0\n      0\n      0\n      1\n      1\n      1\n      ¬a ∧ ¬b ∨ a ∧ c ∨ a ∧ b\n    \n    \n      1\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      ¬a ∧ ¬b ∨ ¬b ∧ ¬c\n    \n    \n      1\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      a ∧ b ∧ c ∨ ¬a ∧ ¬b ∨ ¬b ∧ ¬c\n    \n    \n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      ¬a ∧ ¬b ∨ a ∧ ¬c\n    \n    \n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      ¬a ∧ ¬b ∨ a ∧ ¬c ∨ a ∧ b\n    \n    \n      1\n      1\n      0\n      0\n      1\n      1\n      0\n      0\n      ¬b\n    \n    \n      1\n      1\n      0\n      0\n      1\n      1\n      0\n      1\n      a ∧ c ∨ ¬b\n    \n    \n      1\n      1\n      0\n      0\n      1\n      1\n      1\n      0\n      a ∧ ¬c ∨ ¬b\n    \n    \n      1\n      1\n      0\n      0\n      1\n      1\n      1\n      1\n      ¬b ∨ a\n    \n    \n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      ¬a ∧ ¬b ∨ ¬a ∧ c\n    \n    \n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      ¬a ∧ ¬b ∨ b ∧ c\n    \n    \n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      a ∧ b ∧ ¬c ∨ ¬a ∧ ¬b ∨ ¬a ∧ c\n    \n    \n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      1\n      ¬a ∧ ¬b ∨ b ∧ c ∨ a ∧ b\n    \n    \n      1\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      ¬a ∧ ¬b ∨ ¬a ∧ c ∨ ¬b ∧ c\n    \n    \n      1\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n      ¬a ∧ ¬b ∨ c\n    \n    \n      1\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n      a ∧ b ∧ ¬c ∨ ¬a ∧ ¬b ∨ ¬a ∧ c ∨ ¬b ∧ c\n    \n    \n      1\n      1\n      0\n      1\n      0\n      1\n      1\n      1\n      ¬a ∧ ¬b ∨ a ∧ b ∨ c\n    \n    \n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      ¬b ∧ ¬c ∨ ¬a ∧ c\n    \n    \n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      ¬b ∧ ¬c ∨ ¬a ∧ c ∨ b ∧ c\n    \n    \n      1\n      1\n      0\n      1\n      1\n      0\n      1\n      0\n      ¬b ∧ ¬c ∨ ¬a ∧ c ∨ a ∧ ¬c\n    \n    \n      1\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n      ¬a ∧ ¬b ∨ a ∧ ¬c ∨ b ∧ c\n    \n    \n      1\n      1\n      0\n      1\n      1\n      1\n      0\n      0\n      ¬a ∧ c ∨ ¬b\n    \n    \n      1\n      1\n      0\n      1\n      1\n      1\n      0\n      1\n      ¬b ∨ c\n    \n    \n      1\n      1\n      0\n      1\n      1\n      1\n      1\n      0\n      ¬a ∧ c ∨ a ∧ ¬c ∨ ¬b\n    \n    \n      1\n      1\n      0\n      1\n      1\n      1\n      1\n      1\n      ¬b ∨ c ∨ a\n    \n    \n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      0\n      ¬a ∧ ¬b ∨ ¬a ∧ ¬c\n    \n    \n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      1\n      a ∧ b ∧ c ∨ ¬a ∧ ¬b ∨ ¬a ∧ ¬c\n    \n    \n      1\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      ¬a ∧ ¬b ∨ b ∧ ¬c\n    \n    \n      1\n      1\n      1\n      0\n      0\n      0\n      1\n      1\n      ¬a ∧ ¬b ∨ b ∧ ¬c ∨ a ∧ b\n    \n    \n      1\n      1\n      1\n      0\n      0\n      1\n      0\n      0\n      ¬a ∧ ¬c ∨ ¬b ∧ c\n    \n    \n      1\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      ¬a ∧ ¬c ∨ ¬b ∧ c ∨ a ∧ c\n    \n    \n      1\n      1\n      1\n      0\n      0\n      1\n      1\n      0\n      ¬a ∧ ¬c ∨ ¬b ∧ c ∨ b ∧ ¬c\n    \n    \n      1\n      1\n      1\n      0\n      0\n      1\n      1\n      1\n      ¬a ∧ ¬b ∨ b ∧ ¬c ∨ a ∧ c\n    \n    \n      1\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      ¬a ∧ ¬b ∨ ¬a ∧ ¬c ∨ ¬b ∧ ¬c\n    \n    \n      1\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      a ∧ b ∧ c ∨ ¬a ∧ ¬b ∨ ¬a ∧ ¬c ∨ ¬b ∧ ¬c\n    \n    \n      1\n      1\n      1\n      0\n      1\n      0\n      1\n      0\n      ¬a ∧ ¬b ∨ ¬c\n    \n    \n      1\n      1\n      1\n      0\n      1\n      0\n      1\n      1\n      ¬a ∧ ¬b ∨ a ∧ b ∨ ¬c\n    \n    \n      1\n      1\n      1\n      0\n      1\n      1\n      0\n      0\n      ¬a ∧ ¬c ∨ ¬b\n    \n    \n      1\n      1\n      1\n      0\n      1\n      1\n      0\n      1\n      ¬a ∧ ¬c ∨ a ∧ c ∨ ¬b\n    \n    \n      1\n      1\n      1\n      0\n      1\n      1\n      1\n      0\n      ¬b ∨ ¬c\n    \n    \n      1\n      1\n      1\n      0\n      1\n      1\n      1\n      1\n      ¬b ∨ ¬c ∨ a\n    \n    \n      1\n      1\n      1\n      1\n      0\n      0\n      0\n      0\n      ¬a\n    \n    \n      1\n      1\n      1\n      1\n      0\n      0\n      0\n      1\n      b ∧ c ∨ ¬a\n    \n    \n      1\n      1\n      1\n      1\n      0\n      0\n      1\n      0\n      b ∧ ¬c ∨ ¬a\n    \n    \n      1\n      1\n      1\n      1\n      0\n      0\n      1\n      1\n      ¬a ∨ b\n    \n    \n      1\n      1\n      1\n      1\n      0\n      1\n      0\n      0\n      ¬b ∧ c ∨ ¬a\n    \n    \n      1\n      1\n      1\n      1\n      0\n      1\n      0\n      1\n      ¬a ∨ c\n    \n    \n      1\n      1\n      1\n      1\n      0\n      1\n      1\n      0\n      ¬b ∧ c ∨ b ∧ ¬c ∨ ¬a\n    \n    \n      1\n      1\n      1\n      1\n      0\n      1\n      1\n      1\n      ¬a ∨ c ∨ b\n    \n    \n      1\n      1\n      1\n      1\n      1\n      0\n      0\n      0\n      ¬b ∧ ¬c ∨ ¬a\n    \n    \n      1\n      1\n      1\n      1\n      1\n      0\n      0\n      1\n      ¬b ∧ ¬c ∨ b ∧ c ∨ ¬a\n    \n    \n      1\n      1\n      1\n      1\n      1\n      0\n      1\n      0\n      ¬a ∨ ¬c\n    \n    \n      1\n      1\n      1\n      1\n      1\n      0\n      1\n      1\n      ¬a ∨ ¬c ∨ b\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1\n      0\n      0\n      ¬a ∨ ¬b\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1\n      0\n      1\n      ¬a ∨ ¬b ∨ c\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      0\n      ¬a ∨ ¬b ∨ ¬c\n    \n    \n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n      1\n    \n  \n\n\nWe see that the there exist exactly two longest minimal DNF’s that correspond to the functions that we described in theorem 2 above:\n\nφ=(¬a∧¬b∧¬c)∨(¬a∧b∧c)∨(a∧¬b∧c)∨(a∧b∧¬c)≡a⊕b⊕cψ=(¬a∧¬b∧c)∨(¬a∧b∧¬c)∨(a∧¬b∧¬c)∨(a∧b∧c)≡a↔b↔cφ≡¬ψ\\begin{aligned}\n\\varphi &amp;= (\\neg a \\wedge \\neg b \\wedge \\neg c) \\vee\n(\\neg a \\wedge b \\wedge c) \\vee\n(a \\wedge \\neg b \\wedge c) \\vee\n(a \\wedge b \\wedge \\neg c)\n\\equiv a \\oplus b \\oplus c \\\\\n\\psi &amp;= (\\neg a \\wedge \\neg b \\wedge c) \\vee\n(\\neg a \\wedge b \\wedge \\neg c) \\vee\n(a \\wedge \\neg b \\wedge \\neg c) \\vee\n(a \\wedge b \\wedge c)\n\\equiv a \\leftrightarrow b \\leftrightarrow c \\\\\n\\varphi &amp;\\equiv \\neg \\psi\n\\end{aligned}φψφ​=(¬a∧¬b∧¬c)∨(¬a∧b∧c)∨(a∧¬b∧c)∨(a∧b∧¬c)≡a⊕b⊕c=(¬a∧¬b∧c)∨(¬a∧b∧¬c)∨(a∧¬b∧¬c)∨(a∧b∧c)≡a↔b↔c≡¬ψ​\n",
      "categories": ["cs"],
      "tags": ["logic"],
      
      "collection": "posts",
      "url": "/blog/cs/logic-minimization-gray-codes/"
    },{
      "image": {"path":"/assets/img/blog/gram-schmidt-process-tmb.jpg","srcset":{"1920w":"/assets/img/blog/gram-schmidt-process-tmb.jpg","960w":"/assets/img/blog/gram-schmidt-process-tmb@0,5x.jpg","480w":"/assets/img/blog/gram-schmidt-process-tmb@0,25x.jpg","240w":"/assets/img/blog/gram-schmidt-process-tmb@0,125x.jpg"}},
      "title": "Implementing and visualizing Gram-Schmidt orthogonalization",
      "date": "2021-02-19 00:00:00 +0100",
      
      "content": "In linear algebra, orthogonal bases have many beautiful properties. For example, matrices consisting of orthogonal column vectors (a. k. a. orthogonal matrices) can be easily inverted by just transposing the matrix. Also, it is easier for example to project vectors on subspaces spanned by vectors that are orthogonal to each other. The Gram-Schmidt process is an important algorithm that allows us to convert an arbitrary basis to an orthogonal one spanning the same subspace. In this post, we will implement and visualize this algorithm in 3D with a popular Open-Source library manim.\n\nGram-Schmidt process\n\nConsider some arbitrary ddd-dimensional subspace\n\nA=⟨a1,…,ad⟩A = \\langle a_1, \\dots, a_d \\rangleA=⟨a1​,…,ad​⟩\n\nspanned by linear independent vectors a1,…,ada_1, \\dots, a_da1​,…,ad​. We want to find the following basis spanning the same subspace:\n\nA=⟨q1,…,qd⟩A = \\langle q_1, \\dots, q_d \\rangleA=⟨q1​,…,qd​⟩\n\nThe Gram-Schmidt process is a typical dynamic programming algorithm, because the core idea behind it is to make ⟨q1,…,qi⟩\\langle q_1, \\dots, q_i \\rangle⟨q1​,…,qi​⟩ an orthogonal basis assuming ⟨q1,…,qi−1⟩\\langle q_1, \\dots, q_{i-1} \\rangle⟨q1​,…,qi−1​⟩ is already such a basis. The construction works as follows:\n\nStep 1: For q1q_1q1​, we can just take a1a_1a1​ and normalize it:\n\nq1:=a1∥a1∥q_1 := \\frac{a_1}{\\lVert a_1 \\rVert}q1​:=∥a1​∥a1​​\n\nStep 2: q2q_2q2​ must be orthogonal to ⟨q1⟩\\langle q_1 \\rangle⟨q1​⟩, so we need to find out the component of a2a_2a2​ that is orthogonal to ⟨q1⟩\\langle q_1 \\rangle⟨q1​⟩. We can do this by subtracting the projection of a2a_2a2​ onto ⟨q1⟩\\langle q_1 \\rangle⟨q1​⟩ from a2a_2a2​:\n\nq2′:=a2−(q1⊤a2)q1q1⊤q1=a2−(q1⊤a2)q1q&#x27;_2 := a_2 - \\frac{(q_1^\\top a_2) q_1}{q_1^\\top q_1} = a_2 - (q_1^\\top a_2) q_1q2′​:=a2​−q1⊤​q1​(q1⊤​a2​)q1​​=a2​−(q1⊤​a2​)q1​\n\nThe second equality holds because we normalized the q1q_1q1​ vector and the denominator is thus equal to one.\n\nThis step can be visualized as follows:\n\n\n\nFinally, we need to normalize the resulting vector:\n\nq2:=q2′∥q2′∥q_2 := \\frac{q&#x27;_2}{\\lVert q&#x27;_2 \\rVert}q2​:=∥q2′​∥q2′​​\n\nStep 3: q3q_3q3​ must be orthogonal to ⟨q1,q2⟩\\langle q_1, q_2 \\rangle⟨q1​,q2​⟩, which is already an orthogonal basis, so q3q_3q3​ must be orthogonal to both ⟨q1⟩\\langle q_1 \\rangle⟨q1​⟩ and ⟨q2⟩\\langle q_2 \\rangle⟨q2​⟩ which means that we need to subtract the projections of a3a_3a3​ onto both ⟨q1⟩\\langle q_1 \\rangle⟨q1​⟩ and ⟨q2⟩\\langle q_2 \\rangle⟨q2​⟩ from a3a_3a3​:\n\nq3′:=a3−(q1⊤a3)q1−(q2⊤a2)q2q&#x27;_3 := a_3 - (q_1^\\top a_3) q_1 - (q_2^\\top a_2) q_2q3′​:=a3​−(q1⊤​a3​)q1​−(q2⊤​a2​)q2​\n\nAs in the previous step, we normalize the vector:\n\nq3:=q3′∥q3′∥q_3 := \\frac{q&#x27;_3}{\\lVert q&#x27;_3 \\rVert}q3​:=∥q3′​∥q3′​​\n\nMore generally, we can write the formulas for step 1≤i≤d1 \\le i \\le d1≤i≤d as follows:\n\nStep i: qiq_iqi​ must be orthogonal to ⟨q1,…,qi−1⟩\\langle q_1, \\dots, q_{i-1} \\rangle⟨q1​,…,qi−1​⟩. Because in the previous steps we made ⟨q1,…,qi−1⟩\\langle q_1, \\dots, q_{i-1} \\rangle⟨q1​,…,qi−1​⟩ an orthogonal basis, it follows that qiq_iqi​ must be orthogonal to each vector from {qj:j&lt;i}\\{q_j : j &lt; i\\}{qj​:j&lt;i}. We therefore subtract projections of aia_iai​ onto qjq_jqj​ for all j&lt;ij &lt; ij&lt;i from aia_iai​:\n\nqi′:=ai−(q1⊤ai)q1−(q2⊤ai)q2−⋯−(qi−1⊤ai)qi−1q&#x27;_i := a_i - (q_1^\\top a_i) q_1 - (q_2^\\top a_i) q_2 - \\dots - (q_{i-1}^\\top a_i) q_{i-1}qi′​:=ai​−(q1⊤​ai​)q1​−(q2⊤​ai​)q2​−⋯−(qi−1⊤​ai​)qi−1​\n\nNow we only need to normalize the new basis vector:\n\nqi:=qi′∥qi′∥q_i := \\frac{q&#x27;_i}{\\lVert q&#x27;_i \\rVert}qi​:=∥qi′​∥qi′​​\n\nImplementation &amp; Visualization\n\nWe can implement the Gram-Schmidt orthogonalization algorithm in Python the following way:\n\nimport numpy as np\n\n\ndef gram_schmidt(A):\n    \n    (n, m) = A.shape\n    \n    for i in range(m):\n        \n        q = A[:, i] # i-th column of A\n        \n        for j in range(i):\n            q = q - np.dot(A[:, j], A[:, i]) * A[:, j]\n        \n        if np.array_equal(q, np.zeros(q.shape)):\n            raise np.linalg.LinAlgError(\"The column vectors are not linearly independent\")\n        \n        # normalize q\n        q = q / np.sqrt(np.dot(q, q))\n        \n        # write the vector back in the matrix\n        A[:, i] = q\n\n\nThe input basis vectors are given as columns of the matrix, which then get’s modified in-place by the algorithm. It is important to verify that qi′q&#x27;_iqi′​ is not zero, because if qi′=0q&#x27;_i = 0qi′​=0, then it means that ai∈⟨q1,…,qi−1⟩=⟨a1,…,ai−1⟩a_i \\in \\langle q_1, \\dots, q_{i-1} \\rangle = \\langle a_1, \\dots, a_{i-1} \\rangleai​∈⟨q1​,…,qi−1​⟩=⟨a1​,…,ai−1​⟩ and ⟨a1,…,ad⟩\\langle a_1, \\dots, a_d \\rangle⟨a1​,…,ad​⟩ is therefore not a basis (vectors are not linearly independent).\n\nIn order to visualize the algorithm, I added a few yield-statements to the above implementation, because they allow us to elegantly separate the algorithm from the code visualizing it. For the visualization itself, I will use the popular manim library:\n\n# file: \"gram-schmidt.py\"\nfrom manimlib.imports import *\nfrom enum import Enum\n\n\nclass Action(Enum):\n    UPDATE_MATRIX_REMOVE_Q = 1\n    ADD_PROJECTION = 2\n    REMOVE_PROJECTIONS_SET_Q = 3\n    NORMALIZE_Q = 4\n\n\ndef gram_schmidt(A):\n    \n    (n, m) = A.shape\n    \n    for i in range(m):\n        \n        q = A[:, i] # i-th column of A\n        \n        for j in range(i):\n            yield (Action.ADD_PROJECTION, np.dot(A[:, j], A[:, i]) * A[:, j])\n            q = q - np.dot(A[:, j], A[:, i]) * A[:, j]\n        \n        if np.array_equal(q, np.zeros(q.shape)):\n            raise np.linalg.LinAlgError(\"The column vectors are not linearly independent\")\n        \n        yield (Action.REMOVE_PROJECTIONS_SET_Q, q)\n        \n        # normalize q\n        q = q / np.sqrt(np.dot(q, q))\n\n        yield (Action.NORMALIZE_Q, q)\n        \n        # write the vector back in the matrix\n        A[:, i] = q\n\n        yield (Action.UPDATE_MATRIX_REMOVE_Q, None)\n\n\nclass GramSchmidt(ThreeDScene):\n\n    CONFIG = {\n        \"x_axis_label\": \"$x$\",\n        \"y_axis_label\": \"$y$\",\n        \"basis_i_color\": GREEN,\n        \"basis_j_color\": RED,\n        \"basis_k_color\": GOLD,\n        \"q_color\": PURPLE,\n        \"q_shifted_color\": PINK,\n        \"projection_color\": BLUE\n    }\n\n    def create_matrix(self, np_matrix):\n\n        m = Matrix(np_matrix)\n\n        m.scale(0.5)\n        m.set_column_colors(self.basis_i_color, self.basis_j_color, self.basis_k_color)\n\n        m.to_corner(UP + LEFT)\n\n        return m\n\n    def construct(self):\n        \n        M = np.array([\n            [1.0, 1.0, -3.0],\n            [3.0, 2.0, 1.0],\n            [-2.0, 0.5, 2.5]\n        ])\n\n        axes = ThreeDAxes()\n\n        axes.set_color(GRAY)\n\n        axes.add(axes.get_axis_labels())\n\n        self.set_camera_orientation(phi=55 * DEGREES, theta=-45 * DEGREES)\n\n        basis_helper = TextMobject(\"$ a_1 $\", \",\", \"$ a_2 $\", \",\", \"$ a_3 $\")\n        basis_helper[0].set_color(self.basis_i_color)\n        basis_helper[2].set_color(self.basis_j_color)\n        basis_helper[4].set_color(self.basis_k_color)\n\n        q_helper = TextMobject(\"Current $ q_i $ vector\")\n        q_helper[0].set_color(self.q_color)\n\n        q_shifted_helper = TextMobject(\"Shifted $ q_i $ vector\")\n        q_shifted_helper[0].set_color(self.q_shifted_color)\n\n        projection_helper = TextMobject(\"Projection vectors\")\n        projection_helper[0].set_color(self.projection_color)\n\n        helper = VGroup(\n            projection_helper,\n            q_helper,\n            q_shifted_helper,\n            basis_helper\n        )\n\n        helper.arrange(\n            DOWN,\n            aligned_edge = RIGHT,\n            buff=0.1\n        )\n\n        helper.to_corner(UP + RIGHT)\n\n        self.add_fixed_in_frame_mobjects(helper)\n\n        # matrix\n\n        matrix = self.create_matrix(M)\n\n        self.add_fixed_in_frame_mobjects(matrix)\n\n        # axes &amp; camera\n\n        self.add(axes)\n\n        self.begin_ambient_camera_rotation(rate=0.15)\n\n        i_vec = Vector(M[:, 0], color=self.basis_i_color)\n        j_vec = Vector(M[:, 1], color=self.basis_j_color)\n        k_vec = Vector(M[:, 2], color=self.basis_k_color)\n\n        self.play(\n            GrowArrow(i_vec),\n            GrowArrow(j_vec),\n            GrowArrow(k_vec),\n            Write(helper)\n        )\n\n        self.wait()\n\n        projection_vectors = []\n\n        q = None\n\n        for (action, payload) in gram_schmidt(M):\n\n            if action == Action.UPDATE_MATRIX_REMOVE_Q:\n\n                assert not q is None\n\n                M_rounded = np.round(M.copy(), 2)\n                \n                self.remove(matrix)\n\n                matrix = self.create_matrix(M_rounded)\n\n                self.add_fixed_in_frame_mobjects(matrix)\n\n                i_vec_new = Vector(M[:, 0], color=self.basis_i_color)\n                j_vec_new = Vector(M[:, 1], color=self.basis_j_color)\n                k_vec_new = Vector(M[:, 2], color=self.basis_k_color)\n\n                animation_time = 2.0\n\n                self.play(\n                    FadeOut(q, run_time=animation_time * 0.75),\n                    ReplacementTransform(i_vec, i_vec_new, run_time=animation_time),\n                    ReplacementTransform(j_vec, j_vec_new, run_time=animation_time),\n                    ReplacementTransform(k_vec, k_vec_new, run_time=animation_time)\n                )\n\n                self.wait()\n\n                i_vec, j_vec, k_vec = i_vec_new, j_vec_new, k_vec_new\n\n            elif action == Action.ADD_PROJECTION:\n\n                p = Vector(payload, color=self.projection_color)\n\n                projection_vectors.append(p)\n\n                self.play(GrowArrow(p))\n\n                self.wait()\n\n                if len(projection_vectors) == 2:\n\n                    first_projection_end = projection_vectors[0].get_end()\n\n                    p_shifted = Arrow(first_projection_end, first_projection_end + payload, buff=0, color=self.projection_color)\n\n                    projection_vectors[1] = p_shifted\n\n                    self.play(ReplacementTransform(p, p_shifted))\n\n                    self.wait()\n\n            elif action == Action.REMOVE_PROJECTIONS_SET_Q:\n\n                if not projection_vectors:\n\n                    q = Vector(payload, color=self.q_color)\n\n                    self.play(GrowArrow(q))\n\n                    self.wait()\n\n                    continue\n\n                last_projection_end = projection_vectors[len(projection_vectors) - 1].get_end()\n\n                q_shifted = Arrow(last_projection_end, last_projection_end + payload, buff=0, color=self.q_shifted_color)\n\n                self.play(GrowArrow(q_shifted))\n\n                self.wait()\n\n                q = Vector(payload, color=self.q_color)\n\n                self.play(\n                    ReplacementTransform(q_shifted, q),\n                    *[FadeOut(p) for p in projection_vectors]\n                )\n\n                self.wait()\n\n                projection_vectors = []\n\n            elif action == Action.NORMALIZE_Q:\n\n                q_normalized = Vector(payload, color=self.q_color)\n\n                self.play(ReplacementTransform(q, q_normalized))\n\n                self.wait()\n\n                q = q_normalized\n\n            else:\n                assert False\n\n            self.wait(1)\n\n        assert np.allclose(M.T @ M, np.identity(3))\n\n        self.wait(15)\n\n\nBy rendering the animation with manim gram-schmidt.py GramSchmidt -m -p, we get:\n\n\n\t\n\tUnfortunately, your browser doesn't support the video tag.\n\nVector colors in the animation mean the following:\n\n\n  The green, red and gold vectors are initially the a1a_1a1​, a2a_2a2​ and a3a_3a3​ vectors, respectively. During the algorithm they get iteratively replaced by the qiq_iqi​-vectors, so that at the end these vectors represent q1q_1q1​, q2q_2q2​ and q3q_3q3​, respectively (as we can see, they are orthogonal to each other at the end).\n  The blue vectors are the projections of aia_iai​ onto qjq_jqj​, j&lt;ij &lt; ij&lt;i.\n  The pink vector is the shifted qi′q&#x27;_iqi′​ vector at step iii.\n  The purple vector is the qi′q&#x27;_iqi′​ vector at step iii (pink vector moved to the origin). This vector then gets normalized.\n\n",
      "categories": ["cs"],
      "tags": ["linalg"],
      
      "collection": "posts",
      "url": "/blog/cs/gram-schmidt-orthogonalization/"
    },{
      "image": {"path":"/assets/img/blog/hornsat-2sat-np-complete-tmb.jpg","srcset":{"1920w":"/assets/img/blog/hornsat-2sat-np-complete-tmb.jpg","960w":"/assets/img/blog/hornsat-2sat-np-complete-tmb@0,5x.jpg","480w":"/assets/img/blog/hornsat-2sat-np-complete-tmb@0,25x.jpg","240w":"/assets/img/blog/hornsat-2sat-np-complete-tmb@0,125x.jpg"}},
      "title": "Satisfiability of formulas with both Horn and 2-SAT clauses is NP-Complete",
      "date": "2021-03-02 00:00:00 +0100",
      
      "content": "The SAT problem is the first problem proven to be NP\\mathcal{NP}NP-Complete. However, many instances of this problem with certain properties can be solved efficiently in polynomial time. Well-known examples of such easy instances are 2-sat and horn formulas. All clauses in a 2-sat formula have at most 2 literals and horn-formulas consist only of clauses that contain at most one positive literal. In other words, a 2-sat clause is a conjunction of two implications and any horn clause can be rewritten as a conjunction of positive literals that implies either false or a negative literal. It may seem that satisfiability of formulas containing both 2-sat as well as horn clauses can be decided in polynomial time, however, because of the NP\\mathcal{NP}NP-completeness that we will prove shortly, this is not the case unless P=NP\\mathcal{P} = \\mathcal{NP}P=NP. The problem also remains NP\\mathcal{NP}NP-complete for Dual-horn clauses instead of normal horn clauses.\n\nThe problem\n\nThe problem mentioned above can be formally formulated as follows:\n\nName: Horn2SAT\n\nInput: A formula φ\\varphiφ in conjunctive normal form (CNF), that consists only of 2-sat or horn clauses.\n\nQuestion: Is φ\\varphiφ satisfiable?\n\nExample of a formula φ\\varphiφ that contains only 2-sat or horn clauses:\n\n(¬a∨¬b)∧(b∨c)∧(¬c∨a)∧(¬b∨d)∧(c∨d)∧(a∨c)∧(¬a∨¬c∨¬d∨b)(\\neg a \\vee \\neg b) \\wedge (b \\vee c) \\wedge (\\neg c \\vee a) \\wedge (\\neg b \\vee d) \\wedge (c \\vee d) \\wedge (a \\vee c) \\wedge (\\neg a \\vee \\neg c \\vee \\neg d \\vee b)(¬a∨¬b)∧(b∨c)∧(¬c∨a)∧(¬b∨d)∧(c∨d)∧(a∨c)∧(¬a∨¬c∨¬d∨b)\n\nThe first 6 clauses are 2-sat clauses and the last one is a horn clause, this formula is satisfiable.\n\nProof of NP-Completeness\n\nThis problem is a special case of the SAT problem and is therefore also in NP\\mathcal{NP}NP (the variable assignment can be used as a certificate and it can be verified in polynomial time).\n\nWe will now construct a reduction from the 3-SAT problem. Consider an arbitrary 3-sat clause. If it contains at most 2 literals, that it is directly a 2-sat clause. If there is at most one positive literal, then it is already a horn clause. Therefore, we need to find equisatisfiable sets of clauses for the following situations:\n\n\n  Two positive literals: a∨b∨¬ca \\vee b \\vee \\neg ca∨b∨¬c\n  Three positive literals: a∨b∨ca \\vee b \\vee ca∨b∨c\n\n\nBy using the idea of the Tseitin transformation for the first case, it follows that a∨b∨¬ca \\vee b \\vee \\neg ca∨b∨¬c is satisfiable iff (x↔b∨¬c)∧(a∨x)(x \\leftrightarrow b \\vee \\neg c) \\wedge (a \\vee x)(x↔b∨¬c)∧(a∨x) is satisfiable, this can be rewritten as:\n\n(x↔b∨¬c)∧(a∨x)≡(x∨¬b)∧(x∨c)⏟2-sat clauses∧(¬x∨b∨¬c)⏟Horn clause∧(a∨x)⏟2-sat clause(x \\leftrightarrow b \\vee \\neg c) \\wedge (a \\vee x) \\equiv\n\\underbrace{(x \\vee \\neg b) \\wedge (x \\vee c)}_\\text{2-sat clauses}\n\\wedge \\underbrace{(\\neg x \\vee b \\vee \\neg c)}_\\text{Horn clause}\n\\wedge \\underbrace{(a \\vee x)}_\\text{2-sat clause}(x↔b∨¬c)∧(a∨x)≡2-sat clauses(x∨¬b)∧(x∨c)​​∧Horn clause(¬x∨b∨¬c)​​∧2-sat clause(a∨x)​​\n\nAs regards the second case, we can use the result from the first case to produce a horn-clause:\n\na∨b∨c≡sat(x↔a∨b)∧(x∨c)≡((x∨¬a)∧(x∨¬b)∧(¬x∨a∨b))∧(x∨c)≡(x∨¬a)∧(x∨¬b)∧(a∨b∨¬x)∧(x∨c)≡sat(x∨¬a)∧(x∨¬b)∧(y↔b∨¬x)∧(a∨y)∧(x∨c)≡(x∨¬a)∧(x∨¬b)∧(y∨¬b)∧(y∨x)⏟2-sat clauses∧(¬y∨b∨¬x)⏟Horn clause∧(a∨y)∧(x∨c)⏟2-sat clauses\\begin{aligned}\na \\vee b \\vee c\n&amp;\\overset{\\text{sat}}{\\equiv} (x \\leftrightarrow a \\vee b) \\wedge (x \\vee c) \\\\\n&amp;\\equiv ((x \\vee \\neg a) \\wedge (x \\vee \\neg b) \\wedge (\\neg x \\vee a \\vee b)) \\wedge (x \\vee c) \\\\\n&amp;\\equiv (x \\vee \\neg a) \\wedge (x \\vee \\neg b) \\wedge (a \\vee b \\vee \\neg x) \\wedge (x \\vee c) \\\\\n&amp;\\overset{\\text{sat}}{\\equiv} (x \\vee \\neg a) \\wedge (x \\vee \\neg b) \\wedge (y \\leftrightarrow b \\vee \\neg x) \\wedge (a \\vee y) \\wedge (x \\vee c) \\\\\n&amp;\\equiv \\underbrace{(x \\vee \\neg a) \\wedge (x \\vee \\neg b) \\wedge (y \\vee \\neg b) \\wedge (y \\vee x)}_\\text{2-sat clauses} \\wedge \\underbrace{(\\neg y \\vee b \\vee \\neg x)}_\\text{Horn clause} \\wedge \\underbrace{(a \\vee y) \\wedge (x \\vee c)}_\\text{2-sat clauses}\n\\end{aligned}a∨b∨c​≡sat(x↔a∨b)∧(x∨c)≡((x∨¬a)∧(x∨¬b)∧(¬x∨a∨b))∧(x∨c)≡(x∨¬a)∧(x∨¬b)∧(a∨b∨¬x)∧(x∨c)≡sat(x∨¬a)∧(x∨¬b)∧(y↔b∨¬x)∧(a∨y)∧(x∨c)≡2-sat clauses(x∨¬a)∧(x∨¬b)∧(y∨¬b)∧(y∨x)​​∧Horn clause(¬y∨b∨¬x)​​∧2-sat clauses(a∨y)∧(x∨c)​​​\n\nThis is clearly a polynomial-time reduction.\n\nDual Horn-sat and 2-sat clauses\n\nThe above problem remains NP\\mathcal{NP}NP-complete if we consider Dual-horn clauses instead of (normal) Horn clauses (a clause is called Dual-horn, if it has at most one negative literal).\n\nAnalogously to the proof above, in order to prove NP\\mathcal{NP}NP-hardness we have to express 3-sat clauses with 2 and 3 negative literals in terms of 2-sat and dual-horn clauses. If the clause has 2 negative literals, then:\n\n¬a∨¬b∨c≡sat(¬x↔¬b∨c)∧(¬a∨¬x)≡(¬x∨b)∧(¬x∨¬c)⏟2-sat clauses∧(x∨¬b∨c)⏟Dual horn clause∧(¬a∨¬x)⏟2-sat clause\\begin{aligned}\n\\neg a \\vee \\neg b \\vee c\n&amp;\\overset{\\text{sat}}{\\equiv} (\\neg x \\leftrightarrow \\neg b \\vee c) \\wedge (\\neg a \\vee \\neg x) \\\\\n&amp;\\equiv \\underbrace{(\\neg x \\vee b) \\wedge (\\neg x \\vee \\neg c)}_\\text{2-sat clauses}\n\\wedge \\underbrace{(x \\vee \\neg b \\vee c)}_\\text{Dual horn clause}\n\\wedge \\underbrace{(\\neg a \\vee \\neg x)}_\\text{2-sat clause}\n\\end{aligned}¬a∨¬b∨c​≡sat(¬x↔¬b∨c)∧(¬a∨¬x)≡2-sat clauses(¬x∨b)∧(¬x∨¬c)​​∧Dual horn clause(x∨¬b∨c)​​∧2-sat clause(¬a∨¬x)​​​\n\nNormally, in the Tseitin transformation we make the introduced variables equisatisfiable with the corresponding sub-formula. Here, in order to prevent ¬x\\neg x¬x in the horn clause, we make ¬x\\neg x¬x equisatisfiable with ¬b∨c\\neg b \\vee c¬b∨c.\n\nIf the clause has 3 negative literals, then we use the same trick two times:\n\n¬a∨¬b∨¬c≡sat(¬x↔¬a∨¬b)∧(¬x∨¬c)≡((¬x∨a)∧(¬x∨b)∧(x∨¬a∨¬b))∧(¬x∨¬c)≡(¬x∨a)∧(¬x∨b)∧(¬a∨¬b∨x)∧(¬x∨¬c)≡sat(¬x∨a)∧(¬x∨b)∧(¬y↔¬b∨x)∧(¬a∨¬y)∧(¬x∨¬c)≡(¬x∨a)∧(¬x∨b)∧(¬y∨b)∧(¬y∨¬x)⏟2-sat clauses∧(y∨¬b∨x)⏟Dual horn clause∧(¬a∨¬y)∧(¬x∨¬c)⏟2-sat clauses\\begin{aligned}\n\\neg a \\vee \\neg b \\vee \\neg c\n&amp;\\overset{\\text{sat}}{\\equiv} (\\neg x \\leftrightarrow \\neg a \\vee \\neg b) \\wedge (\\neg x \\vee \\neg c) \\\\\n&amp;\\equiv ((\\neg x \\vee a) \\wedge (\\neg x \\vee b) \\wedge (x \\vee \\neg a \\vee \\neg b)) \\wedge (\\neg x \\vee \\neg c) \\\\\n&amp;\\equiv (\\neg x \\vee a) \\wedge (\\neg x \\vee b) \\wedge (\\neg a \\vee \\neg b \\vee x) \\wedge (\\neg x \\vee \\neg c) \\\\\n&amp;\\overset{\\text{sat}}{\\equiv} (\\neg x \\vee a) \\wedge (\\neg x \\vee b) \\wedge (\\neg y \\leftrightarrow \\neg b \\vee x) \\wedge (\\neg a \\vee \\neg y) \\wedge (\\neg x \\vee \\neg c) \\\\\n&amp;\\equiv \\underbrace{(\\neg x \\vee a) \\wedge (\\neg x \\vee b) \\wedge (\\neg y \\vee b) \\wedge (\\neg y \\vee \\neg x)}_\\text{2-sat clauses} \\wedge \\underbrace{(y \\vee \\neg b \\vee x)}_\\text{Dual horn clause} \\wedge \\underbrace{(\\neg a \\vee \\neg y) \\wedge (\\neg x \\vee \\neg c)}_\\text{2-sat clauses}\n\\end{aligned}¬a∨¬b∨¬c​≡sat(¬x↔¬a∨¬b)∧(¬x∨¬c)≡((¬x∨a)∧(¬x∨b)∧(x∨¬a∨¬b))∧(¬x∨¬c)≡(¬x∨a)∧(¬x∨b)∧(¬a∨¬b∨x)∧(¬x∨¬c)≡sat(¬x∨a)∧(¬x∨b)∧(¬y↔¬b∨x)∧(¬a∨¬y)∧(¬x∨¬c)≡2-sat clauses(¬x∨a)∧(¬x∨b)∧(¬y∨b)∧(¬y∨¬x)​​∧Dual horn clause(y∨¬b∨x)​​∧2-sat clauses(¬a∨¬y)∧(¬x∨¬c)​​​\n",
      "categories": ["cs"],
      "tags": ["logic","complexity"],
      
      "collection": "posts",
      "url": "/blog/cs/hornsat-2sat-np-complete/"
    },{
      "image": {"path":"/assets/img/blog/hilbert-frege-f0-proofs-tmb.jpg","srcset":{"1920w":"/assets/img/blog/hilbert-frege-f0-proofs-tmb.jpg","960w":"/assets/img/blog/hilbert-frege-f0-proofs-tmb@0,5x.jpg","480w":"/assets/img/blog/hilbert-frege-f0-proofs-tmb@0,25x.jpg","240w":"/assets/img/blog/hilbert-frege-f0-proofs-tmb@0,125x.jpg"}},
      "title": "Constructing Hilbert-style F0 proofs with a simple graph-based notation",
      "date": "2021-03-09 00:00:00 +0100",
      
      "content": "There exist different kinds of deductive systems in logic. Every deductive system consists of certain axioms and rules of inference. The problem of constructing a proof in every such system is quite difficult — computationally it is NP\\mathcal{NP}NP-Hard, since satisfiability can be reduced to it. So it is a good idea to trade-off between logical axioms and inference rules. Hilbert (Frege)-style proof systems are interesting because most of them use several axioms but only one inference rule — modus ponens, unlike, for example, Gentzen natural deduction. In other words, Hilbert-style proof systems “push” all the complexity of constructing a proof into the axioms — it is hard to syntactically instantiate them, but once done it is easier to combine them as there is only one rule of inference — modus ponens. In this post, I will present a notation for these Hilbert-style proofs I came up with recently, that allows one to construct them in a more systematic way, with less “guessing”.\n\nThe F0 Hilbert-style proof system\n\nFirst, we need to define the proof system in which we will construct proofs. We will consider only formulas over {¬,→}\\{\\neg, \\rightarrow\\}{¬,→} without true and false. By the way, this is a complete operator set, so every boolean function can be expressed only by using negation and implication. Let\n\na→(b→a)Axiom 1(a→(b→c))→((a→b)→(a→c))Axiom 2(¬a→¬b)→(b→a)Axiom 3\\begin{aligned}\na &amp;\\rightarrow (b \\rightarrow a) &amp; \\text{Axiom 1} \\\\\n(a \\rightarrow (b \\rightarrow c)) &amp;\\rightarrow ((a \\rightarrow b) \\rightarrow (a \\rightarrow c)) &amp; \\text{Axiom 2} \\\\\n(\\neg a \\rightarrow \\neg b) &amp;\\rightarrow (b \\rightarrow a) &amp; \\text{Axiom 3}\n\\end{aligned}a(a→(b→c))(¬a→¬b)​→(b→a)→((a→b)→(a→c))→(b→a)​Axiom 1Axiom 2Axiom 3​\n\nbe the set of axioms, that hold for arbitrary propositional formulas aaa, bbb, and ccc.\n\nThe only inference rule we need is modus ponens (MP), which can be defined as follows:\n\nαα→ββ\\begin{array}{c}\n\\alpha \\quad \\alpha \\rightarrow \\beta \\\\\n\\hline\n\\beta\n\\end{array}αα→ββ​​\n\nIt can be proven that this is a sound and complete proof system.\n\nDeduction theorem\n\nIn order to considerably simplify the proof construction in many cases, we need to briefly define the deduction theorem, which states that:\n\nΣ⊢(a→b)⇔Σ∪{a}⊢b\\Sigma \\vdash (a \\rightarrow b) \\Leftrightarrow \\Sigma \\cup \\{a\\} \\vdash bΣ⊢(a→b)⇔Σ∪{a}⊢b\n\nThis theorem essentially provides a bridge between syntactic entailment and provability in the F0 proof system — for example, proving ⊢a→b\\vdash a \\rightarrow b⊢a→b is equivalent to proving {a}⊢b\\{a\\} \\vdash b{a}⊢b. Intuitively, this theorem holds because s→(a→b)↔(s∧a)→bs \\rightarrow (a \\rightarrow b) \\leftrightarrow (s \\wedge a) \\rightarrow bs→(a→b)↔(s∧a)→b is valid and F0 is sound and complete.\n\nThe deduction theorem is not a rule of inference or an axiom — if some F0 proof uses it, then there exists a proof without it (this can be proven by induction).\n\nNotation for proofs\n\nUsually, proofs are defined as finite sequences of propositional formulas, where each element is either an axiom or modus ponens applied to some already proven formulas (that are located before the current formula in the sequence). For example, a typical proof of ⊢a→a\\vdash a \\rightarrow a⊢a→a would be:\n\n\n  a→((a→a)→a)a \\rightarrow ((a \\rightarrow a) \\rightarrow a)a→((a→a)→a) — Axiom 1\n  (a→((a→a)→a))→((a→(a→a))→(a→a))(a \\rightarrow ((a \\rightarrow a) \\rightarrow a)) \\rightarrow ((a \\rightarrow (a \\rightarrow a)) \\rightarrow (a \\rightarrow a))(a→((a→a)→a))→((a→(a→a))→(a→a)) — Axiom 2\n  (a→(a→a))→(a→a)(a \\rightarrow (a \\rightarrow a)) \\rightarrow (a \\rightarrow a)(a→(a→a))→(a→a) — MP(1, 2)\n  a→(a→a)a \\rightarrow (a \\rightarrow a)a→(a→a) — Axiom 1\n  a→aa \\rightarrow aa→a — MP(4, 3)\n\n\nHowever, such proofs are hard to construct by hand, because we need to come up with concrete instances of axioms and it isn’t trivial in many cases which formulas should be substituted for variables. It is therefore much easier to construct the proof in a bottom-up manner, which means that we start with the formula to be proven and then apply axioms, lemmas and modus ponens backwards.\n\nFor example, we can rewrite the proof above with the following graph notation:\n\n\n\nIn this notation:\n\n\n  Nodes mean proven statements.\n  Edge (u,v)∈E(u, v) \\in E(u,v)∈E means that statement uuu is needed for the proof of statement vvv in one step — application of an axiom/lemma/assumption combined with modus ponens or simply modus ponens.\n  Cycles are not allowed, because we cannot refer to the correctness of some statement while proving it (the graph is a DAG).\n\n\nBy the way, this notation is more compact compared to the list-notation because when applying an axiom, lemma or an assumption we don’t need to state it and then add the result of modus ponens. Instead of that, we just replace the left side of the axiom/lemma/assumption with it’s right side.\n\nHow to come up with F0 proofs\n\nThe intuitive “algorithm” for coming up with proofs in the F0 proof system can be summarized as follows:\n\n\n  Apply the deduction theorem as much as possible.\n  Derive as many formulas as possible from the assumptions.\n  If the statement isn’t already proven, start constructing the proof bottom-up and apply axioms, lemmas, and assumptions as well as modus ponens backwards.\n\n\nWhile performing the bottom-up construction of the proof, our goal is essentially to reach an already proven statement or an assumption from the statement to be proven. In order to do so, we need to iteratively search for ancestor nodes of some node vvv that will get us closer to the desired assumptions or lemmas. For the construction of some node uuu such that (u,v)∈E(u, v) \\in E(u,v)∈E holds, we have the following options:\n\n\n  Add an assumption or an already proven statement to the left of vvv (see, for example, the above proof where the first construction step was to add a→(a→a)a \\rightarrow (a \\rightarrow a)a→(a→a) to the left of v=a→av = a \\rightarrow av=a→a). This step is essentially modus ponens applied backwards.\n  Let uuu be the left-hand side of some assumption or already proven statement if vvv is an instance of it’s right-hand side (see, for example, how Axiom 2 was applied above).\n\n\nExamples\n\nIn the following examples, different F0 proofs are visualized with the graph notation described above. It is also a good idea to read these proofs bottom-up, this is how I constructed them.\n\nLemma 1 (Trivial implication)\n\n⊢a→a\\vdash a \\rightarrow a⊢a→a\n\nProof: The statement follows directly with the deduction theorem. Also, we already proved this statement above without using the deduction theorem.\n\nLemma 2 (Double negation elimination)\n\n⊢¬¬a→a\\vdash \\neg \\neg a \\rightarrow a⊢¬¬a→a\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n¬¬a⊢a\\neg \\neg a \\vdash a¬¬a⊢a\n\n\n\nLemma 3 (Introduce double negation)\n\n⊢a→¬¬a\\vdash a \\rightarrow \\neg \\neg a⊢a→¬¬a\n\nProof:\n\n\n\nLemma 4 (Double negation implication)\n\n⊢(a→b)→(¬¬a→¬¬b)\\vdash (a \\rightarrow b) \\rightarrow (\\neg \\neg a \\rightarrow \\neg \\neg b)⊢(a→b)→(¬¬a→¬¬b)\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{a→b,¬¬a}⊢¬¬b\\{a \\rightarrow b, \\neg \\neg a \\} \\vdash \\neg \\neg b{a→b,¬¬a}⊢¬¬b\n\n\n\nLemma 5 (Implication transitivity 1)\n\n⊢(a→b)→((b→c)→(a→c))\\vdash (a \\rightarrow b) \\rightarrow ((b \\rightarrow c) \\rightarrow (a \\rightarrow c))⊢(a→b)→((b→c)→(a→c))\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{a,a→b,b→c}⊢c\\{a, a \\rightarrow b, b \\rightarrow c\\} \\vdash c{a,a→b,b→c}⊢c\n\n\n\nLemma 6 (Simple implication transitivity)\n\n⊢a→((a→b)→b)\\vdash a \\rightarrow ((a \\rightarrow b) \\rightarrow b)⊢a→((a→b)→b)\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{a,a→b}⊢b\\{a, a \\rightarrow b\\} \\vdash b{a,a→b}⊢b\n\n\n\nLemma 7 (Implication transitivity 2)\n\n⊢(a→b)→((c→a)→(c→b))\\vdash (a \\rightarrow b) \\rightarrow ((c \\rightarrow a) \\rightarrow (c \\rightarrow b))⊢(a→b)→((c→a)→(c→b))\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{c,c→a,a→b}⊢b\\{c, c \\rightarrow a, a \\rightarrow b\\} \\vdash b{c,c→a,a→b}⊢b\n\n\n\nLemma 8 (Implication from inconsistency)\n\n⊢¬b→(b→a)\\vdash \\neg b \\rightarrow (b \\rightarrow a)⊢¬b→(b→a)\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{¬b}⊢(b→a)\\{\\neg b\\} \\vdash (b \\rightarrow a){¬b}⊢(b→a)\n\n\n\nLemma 9 (Implication from contradiction)\n\n⊢(a→b)→((a→¬b)→(a→c))\\vdash (a \\rightarrow b) \\rightarrow ((a \\rightarrow \\neg b) \\rightarrow (a \\rightarrow c))⊢(a→b)→((a→¬b)→(a→c))\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{a,a→b,a→¬b}⊢c\\{a, a \\rightarrow b, a \\rightarrow \\neg b\\} \\vdash c{a,a→b,a→¬b}⊢c\n\n\n\nLemma 10\n\n⊢(¬a→a)→a\\vdash (\\neg a \\rightarrow a) \\rightarrow a⊢(¬a→a)→a\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n(¬a→a)⊢a(\\neg a \\rightarrow a) \\vdash a(¬a→a)⊢a\n\n\n\nLemma 11 (Contraposition)\n\n⊢(a→b)→(¬b→¬a)\\vdash (a \\rightarrow b) \\rightarrow (\\neg b \\rightarrow \\neg a)⊢(a→b)→(¬b→¬a)\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{a→b,¬b}⊢¬a\\{a \\rightarrow b, \\neg b\\} \\vdash \\neg a{a→b,¬b}⊢¬a\n\n\n\nLemma 12 (Elimination of assumptions)\n\n⊢(b→a)→((¬b→a)→a)\\vdash (b \\rightarrow a) \\rightarrow ((\\neg b \\rightarrow a) \\rightarrow a)⊢(b→a)→((¬b→a)→a)\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{b→a,¬b→a}⊢a\\{b \\rightarrow a, \\neg b \\rightarrow a\\} \\vdash a{b→a,¬b→a}⊢a\n\n\n\nLemma 13\n\n⊢a→(¬b→¬(a→b))\\vdash a \\rightarrow (\\neg b \\rightarrow \\neg (a \\rightarrow b))⊢a→(¬b→¬(a→b))\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{a,¬b}⊢¬(a→b)\\{a, \\neg b\\} \\vdash \\neg (a \\rightarrow b){a,¬b}⊢¬(a→b)\n\n\n\nLemma 14\n\n⊢(a→¬a)→¬a\\vdash (a \\rightarrow \\neg a) \\rightarrow \\neg a⊢(a→¬a)→¬a\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n(a→¬a)⊢¬a(a \\rightarrow \\neg a) \\vdash \\neg a(a→¬a)⊢¬a\n\n\n\nLemma 15 (Proof by contradiction)\n\n⊢(a→b)→((a→¬b)→¬a)\\vdash (a \\rightarrow b) \\rightarrow ((a \\rightarrow \\neg b) \\rightarrow \\neg a)⊢(a→b)→((a→¬b)→¬a)\n\nProof: By the deduction theorem, the above statement is equivalent to\n\n{a→b,a→¬b}⊢¬a\\{a \\rightarrow b, a \\rightarrow \\neg b\\} \\vdash \\neg a{a→b,a→¬b}⊢¬a\n\n\n",
      "categories": ["cs"],
      "tags": ["logic"],
      
      "collection": "posts",
      "url": "/blog/cs/hilbert-frege-f0-proofs/"
    },{
      "image": {"path":"/assets/img/blog/non-productive-cfg-rules-tmb.jpg","srcset":{"1920w":"/assets/img/blog/non-productive-cfg-rules-tmb.jpg","960w":"/assets/img/blog/non-productive-cfg-rules-tmb@0,5x.jpg","480w":"/assets/img/blog/non-productive-cfg-rules-tmb@0,25x.jpg","240w":"/assets/img/blog/non-productive-cfg-rules-tmb@0,125x.jpg"}},
      "title": "Efficient algorithm for finding non-productive rules in context-free grammars",
      "date": "2021-04-17 00:00:00 +0200",
      
      "content": "In programs that somehow deal with context-free grammars automatically, we may want to check whether some part of the grammar “makes sense” and isn’t totally redundant and useless. In other words, we may want to check whether some non-terminal or production can come up in some arbitrary sentential form. This problem arises often when the program we are implementing accepts user-defined grammars. In this post, we will consider an existing algorithm that solves the problem as well as introduce and implement a new, more efficient algorithm that detects useless parts of an input grammar.\n\nThe problem\n\nConsider a context-free grammar G=(N,T,P,S)G = (\\mathcal{N}, \\mathcal{T}, \\mathcal{P}, S)G=(N,T,P,S), where N\\mathcal{N}N is the set of non-terminals, T\\mathcal{T}T is the set of terminals, P\\mathcal{P}P is the set of productions and S∈NS \\in \\mathcal{N}S∈N is the starting symbol.\n\nLet us call a production A→γA \\rightarrow \\gammaA→γ productive iff γ⇒∗α\\gamma \\Rightarrow^* \\alphaγ⇒∗α holds for some α∈T∗\\alpha \\in \\mathcal{T}^*α∈T∗. In other words, it is always possible to derive at least one word from a productive production. We call a production unproductive or non-productive iff it is not productive. For example, the following grammar with starting symbol SSS\n\nS→AcD∣εA→BbB→CC→aAD→d\\begin{aligned}\nS &amp;\\rightarrow A c D \\mid \\varepsilon \\\\\nA &amp;\\rightarrow B b \\\\\nB &amp;\\rightarrow C \\\\\nC &amp;\\rightarrow a A \\\\\nD &amp;\\rightarrow d\n\\end{aligned}SABCD​→AcD∣ε→Bb→C→aA→d​\n\nhas only 2 productive rules: S→εS \\rightarrow \\varepsilonS→ε and D→dD \\rightarrow dD→d. This is the case because non-terminals AAA, BBB, and CCC derive each other in a cycle.\n\nProductive productions are exactly the productions that we can use when constructing a parse tree for some arbitrary word, since if we reduce a production that itself cannot derive any word, the parse tree will at that point become either infinite or we will not be able to apply any further rules and parsing will therefore fail. Because of this, it is always a good idea to remove unproductive productions from the grammar before trying to parse some word — the language defined by the grammar will not change. But how do we detect such useless and redundant productions?\n\nClosure algorithm\n\nThe problem we just defined is typically solved with a closure algorithm. The idea is inductive:\n\nBase: Any production A→αA \\rightarrow \\alphaA→α where α\\alphaα is a word (sequence of terminals, including α=ε\\alpha = \\varepsilonα=ε) is a productive production, because a word can be derived directly in one derivation step. Also, AAA is a productive non-terminal, which analogously means that we can derive some word from AAA.\n\nStep: For every A→α∈PA \\rightarrow \\alpha \\in \\mathcal{P}A→α∈P, if α\\alphaα consists only of terminals or productive non-terminals, then AAA is also a productive non-terminal and A→αA \\rightarrow \\alphaA→α is a productive rule.\n\nWe can start with P0P_0P0​ and\n\nP0={A→α∈P∣α∈T∗}P1={A→α∈P∣α∈(T∪{B∈N∣B→β∈P0})∗}⋮Pi+1={A→α∈P∣α∈(T∪{B∈N∣B→β∈Pi})∗}\\begin{aligned}\nP_0 &amp;= \\{A \\rightarrow \\alpha \\in \\mathcal{P} \\mid \\alpha \\in \\mathcal{T}^*\\} \\\\\nP_1 &amp;= \\{A \\rightarrow \\alpha \\in \\mathcal{P} \\mid \\alpha \\in (\\mathcal{T} \\cup \\{B \\in \\mathcal{N} \\mid B \\rightarrow \\beta \\in P_0\\})^*\\} \\\\\n\\vdots \\\\\nP_{i + 1} &amp;= \\{A \\rightarrow \\alpha \\in \\mathcal{P} \\mid \\alpha \\in (\\mathcal{T} \\cup \\{B \\in \\mathcal{N} \\mid B \\rightarrow \\beta \\in P_i\\})^*\\}\n\\end{aligned}P0​P1​⋮Pi+1​​={A→α∈P∣α∈T∗}={A→α∈P∣α∈(T∪{B∈N∣B→β∈P0​})∗}={A→α∈P∣α∈(T∪{B∈N∣B→β∈Pi​})∗}​\n\ncompute PiP_iPi​ iteratively for i=0,1,2,…i = 0,1,2,\\dotsi=0,1,2,… until i=j∈Ni = j \\in \\mathbb{N}i=j∈N such that Pj−1=PjP_{j-1} = P_jPj−1​=Pj​. Intuitively, this simply means that we apply the step described above until no more productive rules are found.\n\nThe downside of such an algorithm is its complexity: even if we implement the algorithm in a way that allows us to check in constant time whether a certain non-terminal is productive according to the current step, we still need to iterate over all productions to find the ones with the right side consisting only of terminals or already productive non-terminals. Each step has therefore complexity in Ω(∣P∣)\\Omega(\\st\\mathcal{P}\\st)Ω(∣P∣) which means that the overall worst-case complexity is in Ω(∣P∣⋅∣N∣)\\Omega(\\st \\mathcal{P} \\st \\cdot \\st \\mathcal{N} \\st)Ω(∣P∣⋅∣N∣), because in the following worst-case scenario the algorithm will make Θ(∣N∣)\\Theta(\\st \\mathcal{N} \\st)Θ(∣N∣) iterations:\n\nA1→A2A2→A3⋮Ak→a\\begin{aligned}\nA_1 &amp;\\rightarrow A_2 \\\\\nA_2 &amp;\\rightarrow A_3 \\\\\n&amp;\\vdots \\\\\nA_k &amp;\\rightarrow a\n\\end{aligned}A1​A2​Ak​​→A2​→A3​⋮→a​\n\nDesigning a more efficient algorithm\n\nThe above algorithm is not efficient because it detects only one new unproductive non-terminal in each step in the worst case. In order to optimize it, we need to somehow analyze the structure of the grammar to directly detect how the productivity of some rules affects the productivity of other ones. More precisely, the algorithm needs to be able to efficiently detect non-productive loops as shown in the example at the beginning of this post, non-terminals that cannot be reached from the start symbol as well as how one rule directly affects the productivity of another rule. The core idea is the following:\n\nLemma: For every production P:=A→α1B1…αn−1BkαnP := A \\rightarrow \\alpha_1 B_1 \\dots \\alpha_{n-1} B_k \\alpha_nP:=A→α1​B1​…αn−1​Bk​αn​ where n∈N+,k∈N0n \\in \\mathbb{N}^+, k \\in \\mathbb{N}_0n∈N+,k∈N0​, αi∈T∗\\alpha_i \\in \\mathcal{T}^*αi​∈T∗, Bi∈NB_i \\in \\mathcal{N}Bi​∈N, this production PPP is productive if and only if B1,…,BkB_1, \\dots, B_kB1​,…,Bk​ are all productive. If PPP is productive, then AAA is productive as well.\n\nProof: The correctness of this lemma follows directly from the definition of productivity.\n\nNow, we want to come up with a data structure that captures all grammar rules and makes it easy for an algorithm to apply the above statement — we need to analyze the nature of how exactly the lemma above links productivity of rules. Also, we need to keep in mind that the data structure must be able to express productivity of both productions and non-terminals.\n\nSome non-terminal is productive iff all non-terminals on the right side of at least one corresponding rule are productive. Intuitively, it means that for the left non-terminal, productivity can be expressed as a disjunction of conjunctions. This leads us to the idea that the data-structure could be something like an implication graph where sets of non-terminals are vertices and productions are edges. In such a graph, an edge {B1,…,Bn}→P{A}\\{B_1, \\dots, B_n\\} \\rightarrow^P \\{A\\}{B1​,…,Bn​}→P{A} means that if B1,…,BnB_1, \\dots, B_nB1​,…,Bn​ are productive, then production PPP and non-terminal AAA are also productive.\n\nFor example, consider the following grammar with starting symbol SSS:\n\nS→H∣C∣XE∣XEGbC→DD→ε∣aF∣aSb∣SH→bF∣HF→FaE→ab∣GG→aGX→a∣b∣YY→a∣X\\begin{aligned}\nS &amp;\\rightarrow H \\mid C \\mid X E \\mid X E G b \\\\\nC &amp;\\rightarrow D \\\\\nD &amp;\\rightarrow \\varepsilon \\mid a F \\mid a S b \\mid S \\\\\nH &amp;\\rightarrow b F \\mid H \\\\\nF &amp;\\rightarrow F a \\\\\nE &amp;\\rightarrow a b \\mid G \\\\\nG &amp;\\rightarrow a G \\\\\nX &amp;\\rightarrow a \\mid b \\mid Y \\\\\nY &amp;\\rightarrow a \\mid X\n\\end{aligned}SCDHFEGXY​→H∣C∣XE∣XEGb→D→ε∣aF∣aSb∣S→bF∣H→Fa→ab∣G→aG→a∣b∣Y→a∣X​\n\nBy creating an edge {B1,…,Bk}→P{A}\\{B_1, \\dots, B_k\\} \\rightarrow^P \\{A\\}{B1​,…,Bk​}→P{A} for each P=A→α1B1…αn−1BkαnP = A \\rightarrow \\alpha_1 B_1 \\dots \\alpha_{n-1} B_k \\alpha_nP=A→α1​B1​…αn−1​Bk​αn​, we get the following multigraph that illustrates how the productivity of some non-terminals implies productivity of other non-terminals and productions:\n\n\n\nIn this multigraph, edges leaving ∅\\varnothing∅ are production edges where the production contains only terminals on the right side. By construction of the multigraph, going over a further edge is equivalent to applying the lemma above, hence all edges reachable from the ∅\\varnothing∅ node are productive. However, the converse is not yet true, because we need a way to visit nodes containing two or more non-terminals in case all single non-terminals have already been visited and are therefore productive.\n\nTo later implement such visiting efficiently, we can introduce a new type of edges — count-down edges. I will also call nodes that contain two or more non-terminals compound. The idea is the following: For each compound node, we can additionally store a counter that is initialized with the amount of non-terminals in that node. When a node containing a single non-terminal is visited, the counter for all compound nodes containing that node is decremented and if, after decrementing, it becomes zero, then the compound node can also be visited. In other words, when the counter of some compound node is zero, it means that all two or more non-terminals on the right of some production are productive and thus the production itself is productive. We can implement this count-down behavior by creating a count-down edge Bi→{B1,…,Bn}B_i \\rightarrow \\{B_1, \\dots, B_n\\}Bi​→{B1​,…,Bn​} for all 1≤i≤n1 \\le i \\le n1≤i≤n:\n\nCount-down edges in the multigraph essentially mean that some vertex can be visited only if all nodes required for visiting that node are already visited. Count-down edges work like AND-gates in digital circuits. For example, the following count-down edges simply mean, that node {A,B,C}\\{A, B, C\\}{A,B,C} can be visited only after nodes AAA, BBB and CCC have all been visited:\n\n\n\nBy adding count-down edges (marked dashed) for each compound node to the multigraph for the above example grammar, we get the following final multigraph:\n\n\n\nIn this multigraph, productions reachable from the ∅\\varnothing∅ node are exactly the productive productions and nodes reachable from ∅\\varnothing∅ are exactly the productive nodes. Unproductive rules can be calculated trivially once we’ve calculated productive ones — they correspond to red edges that were not visited during the traversal of the multigraph:\n\n\n\nThe efficient algorithm\n\nThe above description of the algorithm is informal because I focused on the intuition and tried to explain how I came up with this algorithm. Now we can write it down formally and analyze it:\n\nInput: Context-free grammar G=(N,T,P,S)G = (\\mathcal{N}, \\mathcal{T}, \\mathcal{P}, S)G=(N,T,P,S).\n\nOutput: Set D⊆PD \\subseteq \\mathcal{P}D⊆P of non-productive rules.\n\nAlgorithm:\n\n\n  Initialize an empty directed multigraph G=(V,E,s,t)G = (V, E, s, t)G=(V,E,s,t), where:\n    \n      Vertices V⊆2NV \\subseteq 2^{\\mathcal{N}}V⊆2N are sets of non-terminals.\n      Edges E⊆P∪(N×2N)E \\subseteq \\mathcal{P} \\cup (\\mathcal{N} \\times2^{\\mathcal{N}})E⊆P∪(N×2N) are either productions or count-down edges.\n      The source of a production edge where α\\alphaα and β\\betaβ are words is s(A→α1B1…αn−1Bkαn):={B1,…,Bk}s(A \\rightarrow \\alpha_1 B_1 \\dots \\alpha_{n-1} B_k \\alpha_n) := \\{B_1, \\dots, B_k\\}s(A→α1​B1​…αn−1​Bk​αn​):={B1​,…,Bk​}\n      The target of a production edge is t(A→α1B1…αn−1Bkαn):={A}t(A \\rightarrow \\alpha_1 B_1 \\dots \\alpha_{n-1} B_k \\alpha_n) := \\{A\\}t(A→α1​B1​…αn−1​Bk​αn​):={A}.\n      The source of a count-down edge s((n,N)):={n}s((n, N)) := \\{n\\}s((n,N)):={n} is a single non-terminal node.\n      The target of a count-down edge t((n,N)):=Nt((n, N)) := Nt((n,N)):=N is a compound node.\n    \n  \n  For each p:=A→α1B1…αn−1Bkαn∈Pp := A \\rightarrow \\alpha_1 B_1 \\dots \\alpha_{n-1} B_k \\alpha_n \\in \\mathcal{P}p:=A→α1​B1​…αn−1​Bk​αn​∈P where n∈N+n \\in \\mathbb{N}^+n∈N+, k∈N0k \\in \\mathbb{N}_0k∈N0​, αi∈T∗\\alpha_i \\in \\mathcal{T}^*αi​∈T∗, Bi∈NB_i \\in \\mathcal{N}Bi​∈N:\n    \n      Create an edge p∈Ep \\in Ep∈E as well as nodes s(p)s(p)s(p) and t(p)t(p)t(p) if they didn’t already exist.\n      If k≥2k \\ge 2k≥2, then also create a count-down edge e:=(Bi,{B1,…,Bk})∈Ee := (B_i, \\{B_1, \\dots, B_k\\}) \\in Ee:=(Bi​,{B1​,…,Bk​})∈E for all 1≤i≤k1 \\le i \\le k1≤i≤k. If s(e)s(e)s(e) or t(e)t(e)t(e) are missing in the graph, add them.\n    \n  \n  If ∅∉V\\varnothing \\notin V∅∈/​V, terminate the algorithm with D:=PD := \\mathcal{P}D:=P.\n  Let c:{A∈V∣∣A∣≥2}→Gc : \\{A \\in V \\mid \\st A \\st \\ge 2\\} \\rightarrow Gc:{A∈V∣∣A∣≥2}→G be the counter — for all A∈VA \\in VA∈V with ∣A∣≥2\\st A \\st \\ge 2∣A∣≥2 initialize c(A):=∣A∣c(A) := \\st A \\stc(A):=∣A∣.\n  Let P:=∅P := \\varnothingP:=∅ be the set of productive productions, initialized with the empty set.\n  Traverse the graph with depth first search starting at ∅∈V\\varnothing \\in V∅∈V:\n    \n      If a productive outgoing edge e∈Pe \\in \\mathcal{P}e∈P is detected, add eee to PPP and push t(e)t(e)t(e) onto the stack.\n      If a count-down outgoing edge (n,N)∈N×2N(n, N) \\in \\mathcal{N} \\times2^{\\mathcal{N}}(n,N)∈N×2N is detected, set c(N):=c(N)−1c(N) := c(N) - 1c(N):=c(N)−1. If, after this decrement, c(N)=0c(N) = 0c(N)=0, push t((n,N))=Nt((n, N)) = Nt((n,N))=N onto the stack.\n    \n  \n  Terminate the algorithm with D:=P−PD := \\mathcal{P} - PD:=P−P.\n\n\nThis algorithm can also be tweaked a bit:\n\n\n  Breadth first search can be used instead of depth first search.\n  Unproductive non-terminals can be detected instead of unproductive rules — a non-terminal is productive if it was visited during the depth first search traversal. We can thus analogously build an algorithm that determines productive non-terminals.\n\n\nComplexity analysis\n\nFor the complexity analysis, we will assume that the length of the productions is limited by some constant. This implies that every production can create only constant many nodes and edges in the multigraph and thus ∣V∣∈O(∣P∣)\\st V \\st \\in O(\\st \\mathcal{P} \\st)∣V∣∈O(∣P∣) and ∣E∣∈O(∣P∣)\\st E \\st \\in O(\\st \\mathcal{P} \\st)∣E∣∈O(∣P∣) hold.\n\n\n  Step 2 has complexity Θ(∣P∣)\\Theta(\\st \\mathcal{P} \\st)Θ(∣P∣).\n  Steps 3 and 5 run in constant time.\n  Step 4 runs in time in O(∣{A∈V∣∣A∣≥2}∣)⊆O(∣V∣)⊆O(∣P∣)O(\\st \\{A \\in V \\mid \\st A \\st \\ge 2\\} \\st) \\subseteq O(\\st V \\st) \\subseteq O(\\st \\mathcal{P} \\st)O(∣{A∈V∣∣A∣≥2}∣)⊆O(∣V∣)⊆O(∣P∣).\n  Step 6 has complexity O(∣V∣+∣E∣)⊆O(∣P∣+∣P∣)=O(∣P∣)O(\\st V \\st + \\st E \\st) \\subseteq O(\\st \\mathcal{P} \\st + \\st \\mathcal{P} \\st) = O(\\st \\mathcal{P} \\st)O(∣V∣+∣E∣)⊆O(∣P∣+∣P∣)=O(∣P∣).\n  Step 7 takes time in O(∣P∣)O(\\st \\mathcal{P} \\st)O(∣P∣).\n\n\nThe overall time complexity is therefore O(∣P∣)⊆o(∣P∣⋅∣N∣)O(\\st \\mathcal{P} \\st) \\subseteq o(\\st \\mathcal{P} \\st \\cdot \\st \\mathcal{N} \\st)O(∣P∣)⊆o(∣P∣⋅∣N∣) which means that this algorithm is more efficient compared to the closure approach that has worst-case complexity Ω(∣P∣⋅∣N∣)\\Omega(\\st \\mathcal{P} \\st \\cdot \\st \\mathcal{N} \\st)Ω(∣P∣⋅∣N∣).\n\nSpace complexity of this algorithm is also O(∣P∣)O(\\st \\mathcal{P} \\st)O(∣P∣).\n\nImplementation\n\nI’ve implemented this algorithm in Rust, the complete source code can be found in this repository. Here, I will only focus on parts of the code that are most relevant for the algorithm and describe a small trick I used to reduce the amount of data structures the algorithm needs to maintain.\n\nOur goal is to calculate the set of non-productive rules and for that we need the set of productive rules which is essentially the edges visited so far. Normally, in a depth first search, we track which vertices have already been visited, but in this algorithm we can reuse the set of productive rules to determine whether we already used that edge in the traversal. In the get_productive_productions function below, we therefore only check whether the edge has not yet been visited.\n\n// file: \"useless.rs\"\nuse crate::grammar::{Symbol, Grammar, ProductionReference};\nuse std::collections::{HashMap, HashSet, BTreeSet};\nuse std::rc::Rc;\n\nenum Edge {\n    Production(ProductionReference),\n    CountDown(BTreeSet&lt;Rc&lt;Symbol&gt;&gt;)\n}\n\nstruct FindUselessProductions {\n    graph: HashMap&lt;BTreeSet&lt;Rc&lt;Symbol&gt;&gt;, Vec&lt;Edge&gt;&gt;,\n    starting_productions: HashSet&lt;ProductionReference&gt;\n}\n\nfn single_nonterminal_node(symbol: &amp;Rc&lt;Symbol&gt;) -&gt; BTreeSet&lt;Rc&lt;Symbol&gt;&gt; {\n    let mut set = BTreeSet::new();\n    set.insert(Rc::clone(symbol));\n    set\n}\n\nimpl FindUselessProductions {\n\n    fn new() -&gt; Self {\n        Self {\n            graph: HashMap::new(),\n            starting_productions: HashSet::new()\n        }\n    }\n\n    fn handle_production(&amp;mut self, pr: ProductionReference) {\n\n        let ProductionReference(label, body) = pr;\n\n        let right_nonterminals = body\n            .iter()\n            .filter_map(|symbol| if symbol.is_nonterminal() {\n                Some(Rc::clone(symbol))\n            } else {\n                None\n            })\n            .collect::&lt;BTreeSet&lt;_&gt;&gt;();\n\n        // create node for the label of the current production if the node is not yet present\n        self.graph\n            .entry(single_nonterminal_node(&amp;label))\n            .or_insert_with(|| vec![]);\n\n        if right_nonterminals.is_empty() {\n            // this production contains only terminal symbols on its right hand side\n            // it is therefore productive\n\n            self.starting_productions.insert(ProductionReference(label, body));\n\n            return;\n        }\n\n        if right_nonterminals.len() == 1 {\n            let key = single_nonterminal_node(\n                right_nonterminals.iter().next().unwrap()\n            );\n\n            self.graph\n                .entry(key)\n                .or_insert_with(|| vec![])\n                .push(Edge::Production(ProductionReference(label, body)));\n\n            return;\n        }\n\n        debug_assert!(right_nonterminals.len() &gt;= 2);\n\n        for nonterminal in right_nonterminals.iter() {\n            debug_assert!(nonterminal.is_nonterminal());\n\n            self.graph\n                .entry(single_nonterminal_node(nonterminal))\n                .or_insert_with(|| vec![])\n                .push(Edge::CountDown(right_nonterminals.clone()));\n        }\n\n        self.graph\n            .entry(right_nonterminals)\n            .or_insert_with(|| vec![])\n            .push(Edge::Production(ProductionReference(label, body)));\n    }\n\n    fn get_productive_productions(&amp;self) -&gt; HashSet&lt;ProductionReference&gt; {\n\n        let mut stack = self.starting_productions\n            .iter()\n            .map(|pr| single_nonterminal_node(&amp;pr.0))\n            .collect::&lt;Vec&lt;_&gt;&gt;();\n\n        let mut visited = self.starting_productions.clone();\n\n        let mut counters = HashMap::new();\n\n        while let Some(node) = stack.pop() {\n\n            for edge in self.graph.get(&amp;node).unwrap() {\n\n                match edge {\n                    Edge::Production(pr) =&gt; {\n\n                        if !visited.contains(pr) {\n\n                            visited.insert(pr.clone());\n\n                            stack.push(single_nonterminal_node(&amp;pr.0));\n\n                        }\n\n                    }\n                    Edge::CountDown(symbols) =&gt; {\n\n                        // countdown edges can only go from nodes containing\n                        // single non-terminals\n                        debug_assert!(node.len() == 1);\n                        // countdown edges can only lead to nodes containing\n                        // multiple symbols\n                        debug_assert!(symbols.len() &gt;= 2);\n\n                        if !counters.contains_key(&amp;node) {\n\n                            let counter = counters\n                                .entry(symbols.clone())\n                                .or_insert_with(|| symbols.len());\n\n                            debug_assert!(*counter &gt;= 1);\n\n                            *counter -= 1;\n\n                            if *counter == 0 {\n                                // this is the last remaining edge to a compound node\n                                // we therefore go over this edge in our dfs\n\n                                stack.push(symbols.clone());\n                            }\n                        }\n                    }\n                }\n\n            }\n\n            if node.len() == 1 {\n                counters.insert(node.clone(), 0);\n            }\n\n        }\n\n        visited\n    }\n\n    fn get_non_productive_productions(&amp;self, grammar: &amp;Grammar) -&gt; HashSet&lt;ProductionReference&gt; {\n\n        let productive_productions = self.get_productive_productions();\n\n        let all_productions = grammar\n            .all_productions()\n            .collect::&lt;HashSet&lt;ProductionReference&gt;&gt;();\n\n        all_productions\n            .difference(&amp;productive_productions)\n            .map(|e| e.clone())\n            .collect::&lt;HashSet&lt;ProductionReference&gt;&gt;()\n    }\n\n}\n\npub fn find_useless_productions(grammar: &amp;Grammar) -&gt; HashSet&lt;ProductionReference&gt; {\n    let mut useless = FindUselessProductions::new();\n    for pr in grammar.all_productions() {\n        useless.handle_production(pr);\n    }\n    useless.get_non_productive_productions(grammar)\n}\n\n\nHowever, only checking if a productive edge has already been visited is not enough. If some node has an outgoing count-down edge and there exists a cycle out of production edges that leads back to that node, this node will be visited twice and the counter of the compound node will thus be decremented twice which means that a non-productive production may be marked by the algorithm as productive.\n\nTo avoid this bug, in the program above we use the counters hash-map for two purposes at the same time:\n\n\n  To store the current value of the counter for a compound node.\n  To mark a node containing a single non-terminal as visited after all its edges have been considered, in case there is an outgoing count-down edge.\n\n\nA compound node cannot be visited twice because only count-down edges lead to them and we check whether a node with a single non-terminal has already been visited before decrementing the counter and possibly visiting a compound node.\n",
      "categories": ["cs"],
      "tags": [],
      
      "collection": "posts",
      "url": "/blog/cs/non-productive-cfg-rules/"
    },{
      
      "title": "Computer Science",
      "date": "2021-07-06 20:22:02 +0200",
      "description": "This category contains all posts that have something to do with theoretical or practical computer science.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "featured_categories",
      "url": "/blog/cs/"
    },{
      
      "title": "Complexity theory",
      "date": "2021-07-06 20:22:02 +0200",
      
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "featured_tags",
      "url": "/tag/complexity/"
    },{
      
      "title": "Concurrent programming",
      "date": "2021-07-06 20:22:02 +0200",
      
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "featured_tags",
      "url": "/tag/concurrency/"
    },{
      
      "title": "Linear algebra",
      "date": "2021-07-06 20:22:02 +0200",
      
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "featured_tags",
      "url": "/tag/linalg/"
    },{
      
      "title": "Logic",
      "date": "2021-07-06 20:22:02 +0200",
      
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "featured_tags",
      "url": "/tag/logic/"
    },{
      "image": {"path":"/assets/img/projects/crproxy.jpg","srcset":{"1600w":"/assets/img/projects/crproxy.jpg","800w":"/assets/img/projects/crproxy@0,5x.jpg","400w":"/assets/img/projects/crproxy@0,25x.jpg","200w":"/assets/img/projects/crproxy@0,125x.jpg"}},
      "title": "Clash Royale Proxy",
      "date": "2018-07-01 00:00:00 +0200",
      "description": "CrProxy is a NodeJs implementation of a Clash Royale proxy server. It decrypts traffic between Supercell servers and the game.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/crproxy/"
    },{
      "image": "/assets/img/projects/zeropackerjs.svg",
      "title": "ZeroPackerJs",
      "date": "2018-11-20 00:00:00 +0100",
      "description": "ZeroPackerJs is serializing / deserializing library for javascript. Data is represented in binary format.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/zeropacker/"
    },{
      "image": {"path":"/assets/img/projects/sdlgrapher.jpg","srcset":{"1920w":"/assets/img/projects/sdlgrapher.jpg","960w":"/assets/img/projects/sdlgrapher@0,5x.jpg","480w":"/assets/img/projects/sdlgrapher@0,25x.jpg","240w":"/assets/img/projects/sdlgrapher@0,125x.jpg"}},
      "title": "SDL Grapher",
      "date": "2019-02-04 00:00:00 +0100",
      
      "content": "SdlGrapher allows you to plot graphs for mathematical functions with SDL 2.0 in C++.\n\nFeatures\n\n\n  Horizontal / vertical scrolling.\n  Scaling with mouse wheel.\n  No rendering if the math function returns NaN or Infinity.\n  Movable axises. Screen =&gt; Math, Math =&gt; Screen unit converters.\n  Automatically calculate scale and axis position based on interval of the math function.\n  Pixel perfect rendering.\n\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/sdlgrapher/"
    },{
      "image": {"path":"/assets/img/projects/bst.jpg","srcset":{"595w":"/assets/img/projects/bst.jpg","297w":"/assets/img/projects/bst@0,5x.jpg","148w":"/assets/img/projects/bst@0,25x.jpg","74w":"/assets/img/projects/bst@0,125x.jpg"}},
      "title": "BST",
      "date": "2019-06-08 00:00:00 +0200",
      "description": "Optimized binary search tree implementation in C++.\n",
      "content": "Note: Binary search trees are not balanced by definition. This implementation does not guarantee O(log n) complexity when searching or deleting.\n\nFeatures\n\n  Pretty good performance compared to many other implementations on the internet.\n  No recursion except for debugging purposes.\n  Node-based on-delete balancing is supported. If you don’t need it, you can easily disable it.\n  Unique / non-unique element insertion.\n  2 deletion methods are supported:\n    \n      With payload copying (efficient, when it’s size is less than 2 * sizeof(void*)).\n      With pointer rearrangement.\n    \n  \n  Debug tree printing with indentation.\n  Lightweight, only one header file.\n\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/bst/"
    },{
      "image": "/assets/img/projects/chainhashmap.svg",
      "title": "ChainHashMap",
      "date": "2019-06-21 00:00:00 +0200",
      "description": "This is an implementation of a closed-addressed hash map in C++ without STL.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/chainhashmap/"
    },{
      "image": {"path":"/assets/img/projects/lzz.jpg","srcset":{"1920w":"/assets/img/projects/lzz.jpg","960w":"/assets/img/projects/lzz@0,5x.jpg","480w":"/assets/img/projects/lzz@0,25x.jpg","240w":"/assets/img/projects/lzz@0,125x.jpg"}},
      "title": "LZZ",
      "date": "2019-08-26 00:00:00 +0200",
      "description": "LZZ is a URL shortener that allows changing target URL’s after they have been created.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/lzz/"
    },{
      "image": {"path":"/assets/img/projects/cstring.jpg","srcset":{"1920w":"/assets/img/projects/cstring.jpg","960w":"/assets/img/projects/cstring@0,5x.jpg","480w":"/assets/img/projects/cstring@0,25x.jpg","240w":"/assets/img/projects/cstring@0,125x.jpg"}},
      "title": "CString",
      "date": "2019-08-27 00:00:00 +0200",
      "description": "Header-only, expandable and descriptor-caching string implementation in C99.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/cstring/"
    },{
      "image": "/assets/img/projects/zerorobo/gameplay01.png",
      "title": "ZeroRobo",
      "date": "2019-10-25 00:00:00 +0200",
      "description": "This is my RoboCode robot implemented in Java. It moves around special anchor points that allow it to avoid enemy bullets. Only 1vs1 mode is supported.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/zerorobo/"
    },{
      "image": {"path":"/assets/img/projects/vkantispam.jpg","srcset":{"1920w":"/assets/img/projects/vkantispam.jpg","960w":"/assets/img/projects/vkantispam@0,5x.jpg","480w":"/assets/img/projects/vkantispam@0,25x.jpg","240w":"/assets/img/projects/vkantispam@0,125x.jpg"}},
      "title": "VkAntiSpam",
      "date": "2019-11-01 00:00:00 +0100",
      "description": "Intelligent, integrated and self-learning antispam system for filtering spam in VK groups.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/vkantispam/"
    },{
      "image": {"path":"/assets/img/projects/chrem.jpg","srcset":{"1920w":"/assets/img/projects/chrem.jpg","960w":"/assets/img/projects/chrem@0,5x.jpg","480w":"/assets/img/projects/chrem@0,25x.jpg","240w":"/assets/img/projects/chrem@0,125x.jpg"}},
      "title": "chrem",
      "date": "2020-01-18 00:00:00 +0100",
      "description": "Algorithm to solve a linear system of congruences using the Chinese remainder theorem. Works also for non-coprime divisors.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/chrem/"
    },{
      "image": {"path":"/assets/img/projects/pollardrsacracker.jpg","srcset":{"1920w":"/assets/img/projects/pollardrsacracker.jpg","960w":"/assets/img/projects/pollardrsacracker@0,5x.jpg","480w":"/assets/img/projects/pollardrsacracker@0,25x.jpg","240w":"/assets/img/projects/pollardrsacracker@0,125x.jpg"}},
      "title": "PollardRsaCracker",
      "date": "2020-02-21 00:00:00 +0100",
      "description": "RSA cracking algorithm based on Pollard factorization.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/pollardrsacracker/"
    },{
      "image": {"path":"/assets/img/projects/knife.jpg","srcset":{"1280w":"/assets/img/projects/knife.jpg","640w":"/assets/img/projects/knife@0,5x.jpg","320w":"/assets/img/projects/knife@0,25x.jpg","160w":"/assets/img/projects/knife@0,125x.jpg"}},
      "title": "Knife",
      "date": "2020-03-31 00:00:00 +0200",
      "description": "Knife is a tool that reads input grammar in BNF format and converts it to a few Java classes that can parse the given grammar through a simple interface.\n",
      "content": "Knife doesn’t require any external libraries or dependencies. All generation is done ahead-of-time. After generating the parsing classes you can just copy them into your project.\n\nAlso, as other good parser generation tools, knife uses itself to read the input grammar.\n\nFeatures\n\n\n  No runtime dependencies, knife generates pure Java code that can easily be ported to other JVM-based languages.\n  Parsing is done using push-down automata without recursion.\n  Knife uses an explicit API for accepting the token stream. It allows you to easily use knife with any (including your own) lexer. You can pause and resume parsing at any point. Parsing multiple token streams simultaneously is also possible.\n  No complete parse-trees are being built during parsing. Reduction of the tree is done on-the-fly for performance. Optimized AST’s can be built during parsing with minimal overhead.\n  If your grammar is left-recursive without A =&gt;* A derivations (aka without cycles), knife will generate an equivalent grammar without left recursion for you.\n  Syntax error recovery using panic mode approach without any additional performance overhead.\n\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/knife/"
    },{
      "image": {"path":"/assets/img/projects/grammax.jpg","srcset":{"1920w":"/assets/img/projects/grammax.jpg","960w":"/assets/img/projects/grammax@0,5x.jpg","480w":"/assets/img/projects/grammax@0,25x.jpg","240w":"/assets/img/projects/grammax@0,125x.jpg"}},
      "title": "Grammax",
      "date": "2020-05-21 00:00:00 +0200",
      "description": "Grammax takes a grammar in BNF format as an input and converts it to a Java-class that recognizes the language generated by the grammar. Formally speaking, this tool creates a left-to-right, rightmost derivation (LR) parser for a given grammar. That means that grammax parses the given string by constructing a reversed rightmost derivation of it.\n",
      "content": "Grammax doesn’t require any external libraries or dependencies. All generation is done ahead-of-time. After generating the parsing classes you can just copy them into your project.\n\nAlso, as other good parser generation tools, grammax uses itself to read the input grammar.\n\nFeatures\n\n\n  No runtime dependencies, only pure Java code is generated.\n  Parsing is done using a push-down automation without recursion.\n  Grammax uses an explicit API for accepting the token stream. It allows you to easily use the tool with any (including your own) lexer. You can pause and resume parsing at any point. Parsing multiple token streams simultaneously is also possible.\n  Grammax supports simple lr and canonical lr parsing algorithms.\n  Automatic warnings about possible right-recursion cycles that cause a lot of parsing stack memory consumption.\n  Types can be assigned to terminals and non-terminals. The corresponding expressions are casted automatically.\n  The %top statement allows inserting package and import automatically. Therefore grammax can be used in an automated pipeline.\n\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/grammax/"
    },{
      "image": {"path":"/assets/img/projects/garbageset.jpg","srcset":{"1920w":"/assets/img/projects/garbageset.jpg","960w":"/assets/img/projects/garbageset@0,5x.jpg","480w":"/assets/img/projects/garbageset@0,25x.jpg","240w":"/assets/img/projects/garbageset@0,125x.jpg"}},
      "title": "GarbageSet",
      "date": "2020-05-22 00:00:00 +0200",
      "description": "Set data structure with all operation is O(1), including initialization!\n",
      "content": "This is a set data structure implementation in C. It can be initialized in constant time what makes it different compared to typical set implementation.\n\nComplexity\n\nThe following table gives an overview of what the datastructure is capable of. All the complexities are calculated assuming memory allocation is performed in O(1).\n\n\n  \n    \n      Operation\n      Description\n      Complexity\n    \n  \n  \n    \n      garbageset_init\n      Initializes the data structure with the specified capacity.\n      O(1)\n    \n    \n      garbageset_isset\n      Checks if there is an element at a specified index.\n      O(1)\n    \n    \n      garbageset_get\n      Retrieves the element at a specified indexor returns null, if there is no element at that index.\n      O(1)\n    \n    \n      garbageset_write\n      Writes a new element at the specified index or overwrites an old one if it was defined.\n      O(1)\n    \n  \n\n\nSpace complexity\n\nThe space complexity of the data structure is in O(n). However, apart from storing the payload array itself the data structure requires additional space for redundancy checking purposes. This additional space is about 2*n*sizeof(index_t) bytes where n is the capacity and index_t is the type used for indexes. With this user-defined you can easily reduce the memory overhead.\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/garbageset/"
    },{
      "image": {"path":"/assets/img/projects/numpat.jpg","srcset":{"1920w":"/assets/img/projects/numpat.jpg","960w":"/assets/img/projects/numpat@0,5x.jpg","480w":"/assets/img/projects/numpat@0,25x.jpg","240w":"/assets/img/projects/numpat@0,125x.jpg"}},
      "title": "NumPat",
      "date": "2020-08-30 00:00:00 +0200",
      "description": "Research tool for examining integer digit structure in different remainder classes.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/numpat/"
    },{
      "image": {"path":"/assets/img/projects/f0verifier.jpg","srcset":{"1920w":"/assets/img/projects/f0verifier.jpg","960w":"/assets/img/projects/f0verifier@0,5x.jpg","480w":"/assets/img/projects/f0verifier@0,25x.jpg","240w":"/assets/img/projects/f0verifier@0,125x.jpg"}},
      "title": "F0Verifier",
      "date": "2020-12-03 00:00:00 +0100",
      "description": "A program that verifies propositional logic proofs in the F0 proof system\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/f0verifier/"
    },{
      "image": {"path":"/assets/img/projects/flai.jpg","srcset":{"1920w":"/assets/img/projects/flai.jpg","960w":"/assets/img/projects/flai@0,5x.jpg","480w":"/assets/img/projects/flai@0,25x.jpg","240w":"/assets/img/projects/flai@0,125x.jpg"}},
      "title": "FLAI",
      "date": "2020-12-17 00:00:00 +0100",
      "description": "AI that analyzes flat/apartment offers and predicts prices, neural network implemented in pure python &amp; numpy.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/flai/"
    },{
      "image": {"path":"/assets/img/projects/charwise.jpg","srcset":{"1920w":"/assets/img/projects/charwise.jpg","960w":"/assets/img/projects/charwise@0,5x.jpg","480w":"/assets/img/projects/charwise@0,25x.jpg","240w":"/assets/img/projects/charwise@0,125x.jpg"}},
      "title": "charwise",
      "date": "2021-04-06 00:00:00 +0200",
      "description": "This lightweight, dependency-free rust library provides a convenient buffered, peekable character stream API that can read from different resources.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/charwise/"
    },{
      "image": {"path":"/assets/img/projects/useless_productions.jpg","srcset":{"1920w":"/assets/img/projects/useless_productions.jpg","960w":"/assets/img/projects/useless_productions@0,5x.jpg","480w":"/assets/img/projects/useless_productions@0,25x.jpg","240w":"/assets/img/projects/useless_productions@0,125x.jpg"}},
      "title": "useless_productions",
      "date": "2021-04-17 00:00:00 +0200",
      "description": "A Rust implementation of an efficient algorithm that finds non-productive rules in context-free grammars.\n",
      "content": "\n",
      "categories": [],
      "tags": [],
      
      "collection": "projects",
      "url": "/projects/useless-productions/"
    }
  ]
}

